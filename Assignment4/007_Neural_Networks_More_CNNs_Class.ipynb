{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT0S5jTmynFU",
        "outputId": "5b86b68a-64dc-4ade-a6ed-f7bebfb791dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prml/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5982e935d0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vk3ZiOcynFZ"
      },
      "source": [
        "# Classifying objects\n",
        "\n",
        "In the following, we will use the old-school CIFAR-10 dataset that contains low-resolution pictures of objects of 10 categories. This - and other - dataset is available as part of the `torchvision` package, which you should install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1-IfMh7TynFc"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f5220d8de32b4201bdcecb0a9b700a54",
            "8cca6e3c839e48f395ad7e72f60cc77e",
            "312782531b4b4394a5c16a2e317348de",
            "bd45b523924544e7ac1f5491f4253d06",
            "1634931e860540dcbe8f7a62a1eaccb6",
            "e1767402b20241c987dab26907ab1adb",
            "102292e4ae1d47a6a020e9954ffcb82e",
            "b265cad13b724f50b99be913e2d767a8",
            "5504cd6f70024482ab73f370276e017f",
            "4ca64e0b99b74dddbb9badc5e0e1d76e",
            "4073f565ff8f48bfa25785e7b1a48550"
          ]
        },
        "id": "p8L2ueaEynFd",
        "outputId": "15cc033e-d726-4284-ae3f-a4a2efa64617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "170499072it [00:38, 4449523.01it/s]                                \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms, utils\n",
        "# where to put the data\n",
        "data_path = './'\n",
        "\n",
        "# this constructs a CIFAR10 dataset, selects the training part of it, \n",
        "# downloads it if necessary, and adds additional transforms that we \n",
        "# will need to convert each image to a pytorch tensor AND to provide\n",
        "# a nice conversion of the RGB images into greyscale\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zkt4rexynFe"
      },
      "source": [
        "This has now downloaded the images if not already done so - you will notice that this is quite a \"hefty\" dataset already at 170MB. Now let's download our validation set or test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuDKREFwynFe",
        "outputId": "a56e9f61-9757-42f0-cbf4-2b652493f8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMIsB4KPynFf"
      },
      "source": [
        "## Two-class problem on the GPU\n",
        "\n",
        "One of the reasons for the success of CNNs has been the realization that computation can be done efficiently on GPUs - originally designed to help with calculation of 3D graphics. \n",
        "\n",
        "So, let's push everything onto the GPU now. Let's redefine the same two-class problem as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n46EQQp1ynFg"
      },
      "outputs": [],
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4eCgRwsynFi"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QAahFVJYynFj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0C3UtsjynFl"
      },
      "source": [
        "Now, let's define a device that consists of CUDA (Compute Unified Device Architecture - a standard for general-purpose computing on GPU devices first introduced by NVIDIA in 2007) if supported, or the CPU otherwise.\n",
        "\n",
        "It is considered good standard to put code similar to this at the beginning of any script so that your code will run no matter if a GPU is present or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9lxilbyynFl",
        "outputId": "0a3b2cf0-68d7-43e9-b3ed-9fe8c36a72a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ]
        }
      ],
      "source": [
        "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "print(f\"Training on device {device}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_ypwhUynFm"
      },
      "source": [
        "We now have to modify our training loop slightly - the only difference is that we will need to tell Pytorch explicitly where the data will need to be processed.\n",
        "\n",
        "**Note that if data or model reside in different places, Pytorch will produce a run-time error!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "36L-ut0AynFn"
      },
      "outputs": [],
      "source": [
        "import datetime \n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            # put the data onto the correct pytorch calculation device\n",
        "            imgs = imgs.to(device = device)\n",
        "            labels =labels.to(device = device)\n",
        "            # put a batch through the model\n",
        "            outputs = model(imgs)\n",
        "            # determine the loss\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            # zero the gradients and determine backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # do one step of optimization\n",
        "            optimizer.step()\n",
        "            # keep track of the loss\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxeq3XGDynFo"
      },
      "source": [
        "Now, let's train this (small) convolutional neural network with this training_loop class for 100 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaRSpj0gynFp",
        "outputId": "c8625a3b-0036-44c2-9afc-b7d6f2ca4d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 18:19:46.937467 Epoch 1, Training loss 0.5481962994405418\n",
            "2022-11-14 18:19:50.005368 Epoch 10, Training loss 0.32753151607741215\n",
            "2022-11-14 18:19:53.006500 Epoch 20, Training loss 0.2894373558888769\n",
            "2022-11-14 18:19:56.063038 Epoch 30, Training loss 0.26512936849123353\n",
            "2022-11-14 18:19:58.850674 Epoch 40, Training loss 0.25034802136527506\n",
            "2022-11-14 18:20:01.641148 Epoch 50, Training loss 0.23126863711958479\n",
            "2022-11-14 18:20:04.398422 Epoch 60, Training loss 0.21490916927718812\n",
            "2022-11-14 18:20:07.182649 Epoch 70, Training loss 0.20163768064823878\n",
            "2022-11-14 18:20:10.597743 Epoch 80, Training loss 0.18958105882451792\n",
            "2022-11-14 18:20:13.406425 Epoch 90, Training loss 0.17352295306268012\n",
            "2022-11-14 18:20:16.142739 Epoch 100, Training loss 0.15799839400751575\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda import device_of\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# get the model and put it onto the proper device\n",
        "model = Net().to(device =device)\n",
        "# standard optimizer \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
        "# classification loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# go for 100 epochs\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-rlHCJgynFq"
      },
      "source": [
        "Compared to our previous version this takes less time - the exact speed-up depends of course on your GPU/CPU combination. \n",
        "\n",
        "The validation also needs to change accordingly - here, we will also need to put the data onto the device. Note that the call to the `predicted` variable below also produces a device-fixed output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWW6v22gynFq",
        "outputId": "d47c4880-d137-4e07-90d6-ccd9553fee9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.94\n",
            "Accuracy val: 0.88\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device= device)\n",
        "                labels = labels.to(device = device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxpNk_ZCynFr"
      },
      "source": [
        "## Optimizing CNNs\n",
        "\n",
        "In the following, we will discuss a few basic ways how to advance the architecture of CNNs.\n",
        "\n",
        "### Width\n",
        "\n",
        "One of the easiest ways to enhance the capacity of a CNN is of course to change the \"width\" of the network. This means that you will add more filters to a layer. Changing the width can be done, for example, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TEctf8g3ynFs"
      },
      "outputs": [],
      "source": [
        "class NetWidth(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 //2, kernel_size=3,padding=1)\n",
        "        self.fc1 = nn.Linear(8*8*n_ch1//2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8*8*self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrRR5JuFynFt"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "As we increase the number of parameters, however, we will quickly see that it become easy to overfit the NN on pretty much any data. The increased capacity of a wider network is something that we would like to keep nonetheless, so how can we do this?\n",
        "\n",
        "One solution for this is the so-called **dropout** - a technique proposed by Srivastava and Hinton in 2014. \n",
        "\n",
        "The idea for this is actually very simple and implemented like a layer in Pytorch: in every iteration of the training, you zero out a random fraction of outputs of the preceding layer to reduce their influence. \n",
        "\n",
        "What this does is to train slightly different \"models\" in each iteration that try to solve your task, preventing the individual filters (neurons) from talking to each other too much and forming overfitting connections. \n",
        "\n",
        "This technique is not dependent on CNNs actually and can also be used with fully-connected neural networks, btw. \n",
        "\n",
        "In the case of Pytorch and CNNs we can specify 2D dropouts or 3D dropouts that zero entire channel outputs.\n",
        "\n",
        "In order to make the network function properly, we have to be aware, however, whether we are training (dropout should be active) or evaluating (dropout should not be active or have probability zero). In Pytorch, you can control this via the parameters `model.train()` and `model.eval()` for a `nn.Model` subclass like ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a-87KCBnynFu"
      },
      "outputs": [],
      "source": [
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        # add the dropout layer\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3, padding=1)\n",
        "        # add the dropout layer\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        # add call to dropout\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        # add call to dropout\n",
        "        out = self.conv2_dropout(out)\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_s3CyYhynFv"
      },
      "source": [
        "### Batch normalization\n",
        "\n",
        "Another idea, published in 2015 by Ioffe and Szegedy, is Batch Normalization - a technique that rescales the inputs to the activation functions of the networks so that each (mini)batch has a desired, nicely-behaving distribution. \n",
        "\n",
        "Why would we want to do this? Remember that activation functions have saturation points that may prevent efficient learning - we therefore would like to present the activation function with a range of values that make full use of the optimization gradients.\n",
        "\n",
        "In practice, this normalization is done by shifting and scaling an intermediate input using mean and standard deviation across the samples of a given (mini)batch.\n",
        "\n",
        "This results in a regularization of sorts as any individual sample and its following activations are viewed by the full model as shifted and scaled.\n",
        "\n",
        "As such, the normalization was proposed by the authors to obviate the need for dropout as the regularization through shifting and scaling was supposed to help also with overfitting.\n",
        "\n",
        "The place for the batch norm layer in Pytorch is directly before the activation function. Again, as values for the layers are automatically updated with each call, we have to be aware, however, whether we are training (batch norm should be active) or evaluating (batch norm should not be active). In Pytorch, you can control this via the parameters `model.train()` and `model.eval()` for a `nn.Model` subclass like ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qkFY3K3YynFx"
      },
      "outputs": [],
      "source": [
        "class NetBatchNormalization(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        # note that it is also possible to use \"bias=False\" here, as the \n",
        "        # batch normalization layer \"cancels\" the effect of any bias - see below\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1, bias = \"False\")\n",
        "        # add the batch normalization layer\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features = n_ch1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3, padding=1, bias = \"False\")\n",
        "        # add the batch normalization layer\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features = n_ch1 // 2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        # add call to batch normalization layer **before activation func**\n",
        "        out = self.conv1_batchnorm(self.conv1(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        # add call to batch normalization layer **before activation func**\n",
        "        out = self.conv2_batchnorm(self.conv2(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQqEe8ByynFy"
      },
      "source": [
        "### Depth\n",
        "\n",
        "Maybe you have wondered why we did not talk about going deeper until now - after all, it's called \"deep learning\"... \n",
        "\n",
        "What are the advantages of going deep again? Depth allows you to discover hierarchies of features - although a shallow network is in principle - via the Universal Approximation Theorem - capable of solving any task for you, splitting the network into layers enables it to discover hierarchical structures in your data. \n",
        "\n",
        "Example: a shallow network will be able to tell apart birds from airplanes, but a \"deep\" network may be able to tell you that birds and airplanes consist of different parts (wings and bodies in both cases, eyes and legs for birds, windows and perhaps landing gears for airplanes, etc.), hence allowing you to describe the data with more structure. \n",
        "\n",
        "So why don't we simply go deep? The reason for that was already hinted at several times previously: each additional layer means that gradients in backpropagation will need to be multiplied, and if you are in the tails of the activation functions that will lead to saturation and to either exploding or vanishing gradients, which will make learning unstable or not moving forward.\n",
        "\n",
        "We have already mentioned batch normalization as one possible way to overcome this problem, and, indeed, this will help us to construct deeper networks.\n",
        "\n",
        "Another approach was presented in 2015 by He et al., with their publication of residual networks (ResNets), which uses the trick of adding a \"skip connection\" to a layer.\n",
        "\n",
        "#### Skip connections\n",
        "\n",
        "A skip connection is simply the addition of the input to the output of a layer. Like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Bq8Ec19nynFz"
      },
      "outputs": [],
      "source": [
        "class NetSkip(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3, padding=1)\n",
        "        # go deeper!\n",
        "        self.conv3 = nn.Conv2d(n_ch1 //2, n_ch1//2, kernel_size = 3, padding = 1)\n",
        "        # we have now half the size from before\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        # save the input to layer3\n",
        "        ln_layer3 = out\n",
        "        # and add it to the output\n",
        "        out = F.max_pool2d(torch.relu(self.conv3d(out)) + ln_layer3, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_ch1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN5qbkZSynF0"
      },
      "source": [
        "Good, so now we have added another layer, and we have also added its input to its output. Because we have done this, the skip connection is now part of the computational graph and hence part of the loss gradient path.\n",
        "\n",
        "Since they are more directly connected to the loss (note, they are outside the activation functions!), their addition to the network means that the optimizer can decide to bypass a layer. The effect of this is that gradients across multiple layers are less likely to vanish.\n",
        "\n",
        "#### Blocks of layers\n",
        "\n",
        "If we really want to go deep, we need a better way to initialize our networks, as the \"manual\" way of adding layers that we have done so far is not going to be feasible.\n",
        "\n",
        "We do this by first defining a `ResNetBlock` class, which includes one block of convolutions, activations, and skip connections. In addition, we will add back batch normalization and also add a special type of weight initialization that aids optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wnArNEu5ynF0"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_ch):\n",
        "        super(ResBlock, self).__init__()\n",
        "        # here we now explicitly get rid of the biases as we use BN\n",
        "        self.conv = nn.Conv2d(n_ch, n_ch, kernel_size = 3, padding =1, bias = False)\n",
        "        # batch normalization \n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_ch)\n",
        "        # BN is initialized to have 0.5 \"variance\" and 0 mean\n",
        "        # 이거 안하면 -1 과 1 사이로 initialization 됨\n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "        # this is a special initialization of the convolution weights \n",
        "        # that was found to help with optimization\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        # simple forward function that includes skip connection\n",
        "        out = self.conv(x)\n",
        "        out = self.batch_norm(out)\n",
        "        out = torch.relu(out)\n",
        "        return out + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlP5OTqynF1"
      },
      "source": [
        "Now that we have one block, we will use this to create a deep architecture like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PrR2J5RjynF1"
      },
      "outputs": [],
      "source": [
        "class NetResDeep(nn.Module):\n",
        "    # the constructor now has a parameter n_blocks that determines how many\n",
        "    # ResBlocks will be used\n",
        "    def __init__(self, n_ch1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n_ch1 = n_ch1\n",
        "        # initial convolution for embedding\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        # add the ResBlocks to the network\n",
        "        # nn.Sequential makes sure that we can add them together nicely\n",
        "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_ch = n_ch1)]))\n",
        "        # note that the \"net\" effect of the blocks does not reduce the dimensionality\n",
        "        # of the features - hence, we again have 8x8 \"pixels\" resolution here\n",
        "        self.fc1 = nn.Linear(8*8*n_ch1, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = self.resblocks(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vOv_7Xo4ynF2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 18:20:17.600212 Epoch 1, Training loss 0.496273444146867\n",
            "2022-11-14 18:20:26.215406 Epoch 10, Training loss 0.27691914549299107\n",
            "2022-11-14 18:20:36.160893 Epoch 20, Training loss 0.22144736230942855\n",
            "2022-11-14 18:20:45.569596 Epoch 30, Training loss 0.17552546829364862\n",
            "2022-11-14 18:20:55.888347 Epoch 40, Training loss 0.1373107213004949\n",
            "2022-11-14 18:21:05.879839 Epoch 50, Training loss 0.10863749730359216\n",
            "2022-11-14 18:21:15.314981 Epoch 60, Training loss 0.08405274740022839\n",
            "2022-11-14 18:21:25.369174 Epoch 70, Training loss 0.06332396752657785\n",
            "2022-11-14 18:21:35.770035 Epoch 80, Training loss 0.04269365870814984\n",
            "2022-11-14 18:21:46.139439 Epoch 90, Training loss 0.03392608419867458\n",
            "2022-11-14 18:21:57.250519 Epoch 100, Training loss 0.02649187296035753\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# get the model and put it onto the proper device\n",
        "model = NetResDeep().to(device=device)\n",
        "# standard optimizer \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3) \n",
        "# classification loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# go for 100 epochs\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_C7-dBaJynF2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy train: 1.00\n",
            "Accuracy val: 0.88\n"
          ]
        }
      ],
      "source": [
        "validate(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMUv_EFlynF2"
      },
      "source": [
        "As we can see, however, although the last - and possibly most advanced design - is easily capable of achieving 100% training accuracy, its validation accuracy is still roughly the same. \n",
        "\n",
        "This is in part due to the fact that we've tried to go deep on images that only have 32x32 pixels - discovering hierarchies in such a low-dimensional input space may therefore be limited. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WULGIHwIynF3"
      },
      "source": [
        "## 1x1 convolutions\n",
        "\n",
        "Another architecture element that is often used in CNNs is the 1x1 convolution layer. It is often called a projection or embedding layer as well.\n",
        "\n",
        "Now, mathematically, a 1x1 convolution basically takes each input and weights it with a single number, producing another number. So, there is no influence of neighboring elements in this operation - it is purely **local**.\n",
        "\n",
        "Note that if we treat a 1x1 convolution as a layer, however, its output can be fed into an activation function, which in turn means that the full layer can perform complex, non-linear operations on inputs.\n",
        "\n",
        "In addition, note, that this layer can perform its operations along the channel dimension, generating again a single number as output across all input channels. This means that in a deep architecture, the 1x1 operation will basically \"summarize\" (other words that are often used are \"project\" / \"embed\" / \"pool\" in this context) the full set of channels into one feature map, preserving its width x height dimensions. \n",
        "\n",
        "The following is an example of a projection, in which the dimensionality is preserved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_XrbIs4_ynF3"
      },
      "outputs": [],
      "source": [
        "class NetWidthProject(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3,padding=1)\n",
        "        # keeps the number of filters\n",
        "        self.conv3 = nn.Conv2d(n_ch1 // 2, n_ch1 //2, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        # Conv1d -> Keep the number of filter\n",
        "        out = torch.tanh(self.conv3(out))\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9kMO0uXwynF4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NetWidthProject(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(38658, [864, 32, 4608, 16, 256, 16, 32768, 32, 64, 2])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NetWidthProject().to(device=device)\n",
        "print(model)\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7I6gZqQynF5"
      },
      "source": [
        "The following is an example of how to use the 1x1 layer for embedding or dimensionality reduction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EBVLRKUUynF6"
      },
      "outputs": [],
      "source": [
        "class NetWidthEmbed(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3,padding=1)\n",
        "        # reduces the number of filters by 2\n",
        "        # Reduce dimenionality ; Embed or summarize to lower dimension\n",
        "        self.conv3 = nn.Conv2d(n_ch1 // 2, n_ch1 //4, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 4, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = torch.tanh(self.conv3(out))\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 4)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DoP1crm8ynF6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NetWidthEmbed(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=512, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(22138, [864, 32, 4608, 16, 128, 8, 16384, 32, 64, 2])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NetWidthEmbed().to(device=device)\n",
        "print(model)\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qmVRpIqXynF7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 18:21:58.537577 Epoch 1, Training loss 0.58195888521565\n",
            "2022-11-14 18:22:02.188575 Epoch 10, Training loss 0.34625388501556054\n",
            "2022-11-14 18:22:06.020402 Epoch 20, Training loss 0.2947164955697242\n",
            "2022-11-14 18:22:10.149256 Epoch 30, Training loss 0.2688618850935796\n",
            "2022-11-14 18:22:14.459880 Epoch 40, Training loss 0.24737764624463524\n",
            "2022-11-14 18:22:18.592151 Epoch 50, Training loss 0.2310817678263233\n",
            "2022-11-14 18:22:23.010495 Epoch 60, Training loss 0.21424930742022338\n",
            "2022-11-14 18:22:27.159885 Epoch 70, Training loss 0.1969047422716572\n",
            "2022-11-14 18:22:30.593173 Epoch 80, Training loss 0.18144229201564363\n",
            "2022-11-14 18:22:34.135474 Epoch 90, Training loss 0.16661728847368507\n",
            "2022-11-14 18:22:37.808325 Epoch 100, Training loss 0.15340321321206488\n",
            "2022-11-14 18:22:41.714665 Epoch 110, Training loss 0.13679726993676963\n",
            "2022-11-14 18:22:46.056748 Epoch 120, Training loss 0.12310045815197526\n",
            "2022-11-14 18:22:49.792912 Epoch 130, Training loss 0.11143399705030736\n",
            "2022-11-14 18:22:53.452153 Epoch 140, Training loss 0.09742835280337152\n",
            "2022-11-14 18:22:56.977737 Epoch 150, Training loss 0.0854427948190718\n",
            "2022-11-14 18:23:00.627153 Epoch 160, Training loss 0.07209441713561678\n",
            "2022-11-14 18:23:04.162981 Epoch 170, Training loss 0.06082105612275517\n",
            "2022-11-14 18:23:07.887414 Epoch 180, Training loss 0.054672398634112565\n",
            "2022-11-14 18:23:11.788623 Epoch 190, Training loss 0.04474510520008529\n",
            "2022-11-14 18:23:15.517254 Epoch 200, Training loss 0.03694070628873861\n",
            "2022-11-14 18:23:19.058922 Epoch 210, Training loss 0.029952417850007962\n",
            "2022-11-14 18:23:22.501549 Epoch 220, Training loss 0.025612550998213374\n",
            "2022-11-14 18:23:25.892003 Epoch 230, Training loss 0.022945833840306587\n",
            "2022-11-14 18:23:29.282242 Epoch 240, Training loss 0.01740618609485163\n",
            "2022-11-14 18:23:32.686755 Epoch 250, Training loss 0.015654491775247987\n",
            "2022-11-14 18:23:36.487324 Epoch 260, Training loss 0.012946628072209343\n",
            "2022-11-14 18:23:40.128305 Epoch 270, Training loss 0.01115189894594869\n",
            "2022-11-14 18:23:43.633299 Epoch 280, Training loss 0.009436602681240506\n",
            "2022-11-14 18:23:47.069882 Epoch 290, Training loss 0.00827748758053247\n",
            "2022-11-14 18:23:50.486205 Epoch 300, Training loss 0.0069062249689766314\n",
            "2022-11-14 18:23:53.870044 Epoch 310, Training loss 0.006119707049090106\n",
            "2022-11-14 18:23:57.171139 Epoch 320, Training loss 0.005486681116789366\n",
            "2022-11-14 18:24:00.632181 Epoch 330, Training loss 0.004587528788170238\n",
            "2022-11-14 18:24:04.513793 Epoch 340, Training loss 0.004010410809586904\n",
            "2022-11-14 18:24:08.093396 Epoch 350, Training loss 0.003700201392950858\n",
            "2022-11-14 18:24:11.397027 Epoch 360, Training loss 0.003394063616840311\n",
            "2022-11-14 18:24:15.029882 Epoch 370, Training loss 0.0034001313923312363\n",
            "2022-11-14 18:24:18.780704 Epoch 380, Training loss 0.002747830394269887\n",
            "2022-11-14 18:24:22.308150 Epoch 390, Training loss 0.0024562876776845497\n",
            "2022-11-14 18:24:25.791718 Epoch 400, Training loss 0.0022958222356592277\n",
            "2022-11-14 18:24:29.174392 Epoch 410, Training loss 0.0021071860638660657\n",
            "2022-11-14 18:24:32.613982 Epoch 420, Training loss 0.0020034221304242447\n",
            "2022-11-14 18:24:36.380775 Epoch 430, Training loss 0.001910064032799594\n",
            "2022-11-14 18:24:39.867898 Epoch 440, Training loss 0.0017307474580815856\n",
            "2022-11-14 18:24:43.273167 Epoch 450, Training loss 0.0016126318050216242\n",
            "2022-11-14 18:24:46.861758 Epoch 460, Training loss 0.0015435767229157054\n",
            "2022-11-14 18:24:50.931522 Epoch 470, Training loss 0.0014313920480747866\n",
            "2022-11-14 18:24:54.799348 Epoch 480, Training loss 0.0013636006770883894\n",
            "2022-11-14 18:24:58.355739 Epoch 490, Training loss 0.001293254967129739\n",
            "2022-11-14 18:25:01.853621 Epoch 500, Training loss 0.0012374555093872474\n",
            "2022-11-14 18:25:05.322421 Epoch 510, Training loss 0.0011751168953150267\n",
            "2022-11-14 18:25:08.772215 Epoch 520, Training loss 0.0011159774814331962\n",
            "2022-11-14 18:25:12.036082 Epoch 530, Training loss 0.0010541447300751617\n",
            "2022-11-14 18:25:15.635777 Epoch 540, Training loss 0.001009639494418045\n",
            "2022-11-14 18:25:19.591469 Epoch 550, Training loss 0.0009674447552289125\n",
            "2022-11-14 18:25:23.252628 Epoch 560, Training loss 0.0009347749169571859\n",
            "2022-11-14 18:25:26.840154 Epoch 570, Training loss 0.0008987919633220406\n",
            "2022-11-14 18:25:30.649361 Epoch 580, Training loss 0.0008597613010513982\n",
            "2022-11-14 18:25:34.356863 Epoch 590, Training loss 0.0008334760156691454\n",
            "2022-11-14 18:25:37.982987 Epoch 600, Training loss 0.0007996699854525423\n",
            "2022-11-14 18:25:41.555271 Epoch 610, Training loss 0.0007676661778671465\n",
            "2022-11-14 18:25:45.062779 Epoch 620, Training loss 0.0007409332876681694\n",
            "2022-11-14 18:25:48.395075 Epoch 630, Training loss 0.0007223400247845336\n",
            "2022-11-14 18:25:51.741427 Epoch 640, Training loss 0.0006945001581130597\n",
            "2022-11-14 18:25:55.644692 Epoch 650, Training loss 0.0006732609314669018\n",
            "2022-11-14 18:25:59.593942 Epoch 660, Training loss 0.0006514471433026075\n",
            "2022-11-14 18:26:03.182004 Epoch 670, Training loss 0.000629078148316242\n",
            "2022-11-14 18:26:06.720487 Epoch 680, Training loss 0.0006146828879774281\n",
            "2022-11-14 18:26:10.393800 Epoch 690, Training loss 0.0005974451634791164\n",
            "2022-11-14 18:26:13.804826 Epoch 700, Training loss 0.0005793250964844754\n",
            "2022-11-14 18:26:17.148791 Epoch 710, Training loss 0.0005616558585753428\n",
            "2022-11-14 18:26:20.695816 Epoch 720, Training loss 0.0005446942196862333\n",
            "2022-11-14 18:26:24.769070 Epoch 730, Training loss 0.0005264880374232663\n",
            "2022-11-14 18:26:28.563593 Epoch 740, Training loss 0.000520927213769329\n",
            "2022-11-14 18:26:32.085559 Epoch 750, Training loss 0.0005057489777141055\n",
            "2022-11-14 18:26:35.557934 Epoch 760, Training loss 0.0004891903260329762\n",
            "2022-11-14 18:26:38.947671 Epoch 770, Training loss 0.00047944146344715516\n",
            "2022-11-14 18:26:42.357697 Epoch 780, Training loss 0.0004748518102851275\n",
            "2022-11-14 18:26:45.651859 Epoch 790, Training loss 0.00045796211351600483\n",
            "2022-11-14 18:26:49.354641 Epoch 800, Training loss 0.00044755685388979616\n",
            "2022-11-14 18:26:53.221361 Epoch 810, Training loss 0.00043287940250656237\n",
            "2022-11-14 18:26:56.823373 Epoch 820, Training loss 0.0004230464940629662\n",
            "2022-11-14 18:27:00.333462 Epoch 830, Training loss 0.00041804224626972356\n",
            "2022-11-14 18:27:03.728132 Epoch 840, Training loss 0.0004071457359523091\n",
            "2022-11-14 18:27:07.110860 Epoch 850, Training loss 0.00039838560479919273\n",
            "2022-11-14 18:27:10.450473 Epoch 860, Training loss 0.000389819305898417\n",
            "2022-11-14 18:27:13.913820 Epoch 870, Training loss 0.00038243621356220275\n",
            "2022-11-14 18:27:17.715370 Epoch 880, Training loss 0.00037594829559189734\n",
            "2022-11-14 18:27:21.641566 Epoch 890, Training loss 0.00036752761112600953\n",
            "2022-11-14 18:27:25.200254 Epoch 900, Training loss 0.0003585749800086383\n",
            "2022-11-14 18:27:28.667601 Epoch 910, Training loss 0.00035337083853423823\n",
            "2022-11-14 18:27:32.043076 Epoch 920, Training loss 0.00034333768020888974\n",
            "2022-11-14 18:27:35.394996 Epoch 930, Training loss 0.0003390150577499855\n",
            "2022-11-14 18:27:38.819199 Epoch 940, Training loss 0.0003312435843662958\n",
            "2022-11-14 18:27:42.281535 Epoch 950, Training loss 0.0003240397344117207\n",
            "2022-11-14 18:27:46.052747 Epoch 960, Training loss 0.00032163714336759297\n",
            "2022-11-14 18:27:49.856684 Epoch 970, Training loss 0.0003143181285363397\n",
            "2022-11-14 18:27:53.364635 Epoch 980, Training loss 0.0003095216475433453\n",
            "2022-11-14 18:27:56.842765 Epoch 990, Training loss 0.0003033590331194614\n",
            "2022-11-14 18:28:00.291318 Epoch 1000, Training loss 0.0002975797694147517\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "# get the model and put it onto the proper device\n",
        "model = NetWidthEmbed().to(device=device)\n",
        "# standard optimizer \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
        "# classification loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# go for 100 epochs\n",
        "training_loop(\n",
        "    n_epochs = 1000,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NjYrfCIynF7"
      },
      "source": [
        "Just in the same way is it can be used for dimensionality reduction, the 1x1 convolution can also be used for dimensionality increase! This can be useful for upscaling images or feature maps.\n",
        "\n",
        "The idea of using these filters was first proposed in 2013 and put to full use in the 2014 Inception architecture.\n",
        "\n",
        "In the original ResNet architecture, for example, these have also been used to introduce 3x3 \"bottleneck\" layers, in which a 1x1 convolution first decreased the dimensionality and then increased it again after the actual 3x3 convolutions like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YfS7PMZsynF8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'n_ch1' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prml/workspace/NN/Assignment4/007_Neural_Networks_More_CNNs_Class.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/prml/workspace/NN/Assignment4/007_Neural_Networks_More_CNNs_Class.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# reduces the number of filters\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/prml/workspace/NN/Assignment4/007_Neural_Networks_More_CNNs_Class.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvNNm1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(n_ch1, n_ch1 \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/prml/workspace/NN/Assignment4/007_Neural_Networks_More_CNNs_Class.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# applies 3x3 convolution \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/prml/workspace/NN/Assignment4/007_Neural_Networks_More_CNNs_Class.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvNN \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConv2d(\u001b[39m3\u001b[39m, n_ch1 \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_ch1' is not defined"
          ]
        }
      ],
      "source": [
        "# reduces the number of filters\n",
        "self.convNNm1 = nn.Conv2d(n_ch1, n_ch1 // 4, kernel_size=1)\n",
        "# applies 3x3 convolution \n",
        "self.convNN = nn.Conv2d(3, n_ch1 // 4, kernel_size=3, padding=1)\n",
        "# upsamples again\n",
        "self.convNNp1 = nn.Conv2d(n_ch1 // 4, n_ch1, kernel_size=1,padding=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "102292e4ae1d47a6a020e9954ffcb82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1634931e860540dcbe8f7a62a1eaccb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "312782531b4b4394a5c16a2e317348de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b265cad13b724f50b99be913e2d767a8",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5504cd6f70024482ab73f370276e017f",
            "value": 170498071
          }
        },
        "4073f565ff8f48bfa25785e7b1a48550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca64e0b99b74dddbb9badc5e0e1d76e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5504cd6f70024482ab73f370276e017f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cca6e3c839e48f395ad7e72f60cc77e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1767402b20241c987dab26907ab1adb",
            "placeholder": "​",
            "style": "IPY_MODEL_102292e4ae1d47a6a020e9954ffcb82e",
            "value": "100%"
          }
        },
        "b265cad13b724f50b99be913e2d767a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd45b523924544e7ac1f5491f4253d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca64e0b99b74dddbb9badc5e0e1d76e",
            "placeholder": "​",
            "style": "IPY_MODEL_4073f565ff8f48bfa25785e7b1a48550",
            "value": " 170498071/170498071 [00:14&lt;00:00, 13363288.04it/s]"
          }
        },
        "e1767402b20241c987dab26907ab1adb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5220d8de32b4201bdcecb0a9b700a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cca6e3c839e48f395ad7e72f60cc77e",
              "IPY_MODEL_312782531b4b4394a5c16a2e317348de",
              "IPY_MODEL_bd45b523924544e7ac1f5491f4253d06"
            ],
            "layout": "IPY_MODEL_1634931e860540dcbe8f7a62a1eaccb6"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
