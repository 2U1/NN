{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import colorsys\n",
    "import sys\n",
    "import time\n",
    "from IPython import display\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimization - Neural Networks - More networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning in a better way - gradient descent!\n",
    "\n",
    "Let's return to our very simple model of neurons, for which we said that we can write their output as the activation function applied to the linearly weighted input:\n",
    "$$\n",
    "\\vec{y}=f(\\vec{w}^{\\top}\\vec{x})\n",
    "$$\n",
    "\n",
    "\n",
    "Now, we need to find a way to **learn** the weights $\\vec{w}$ somehow. \n",
    "\n",
    "In the previous perceptron examples, we did this by updating the weights with the class-weighted sample - $\\vec{w}=\\vec{w}+y_k\\vec{x}_k$, but how would we do this in a general case?\n",
    "\n",
    "Let's try to do **gradient descent**!!\n",
    "\n",
    "Let's again go to our standard way of doing things and minimize the least squares error of some target values $\\vec{t}$ versus the prediction. So, let's write our loss function:\n",
    "\n",
    "$$\n",
    "l(\\vec{w})=\\frac{1}{2} \\sum \\|\\vec{y}(\\vec{w})-\\vec{t}\\|^2\n",
    "$$\n",
    "\n",
    "Now we remember that the output is some kind of function of the weighted sum of all inputs $\\vec{x}$, i.e., $\\vec{y}=f(\\vec{w}^{\\top}\\vec{x})$, so we can take the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l(\\vec{w})}{\\partial \\vec{w}}= \\left ( f(\\vec{w}^{\\top}\\vec{x})-\\vec{y}\\right ) \\frac{\\partial f(\\vec{w}^{\\top}\\vec{x})}{\\partial \\vec{w}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\left ( f(\\vec{w}^{\\top}\\vec{x})-\\vec{y}\\right )\\vec{x} \\frac{\\partial f(\\vec{w}^{\\top}\\vec{x})}{\\partial(\\vec{w}^{\\top}\\vec{x})}\n",
    "$$\n",
    "\n",
    "So, once we've decided on an activation function, we can determine this derivative and then try to optimize the whole loss function via gradient descent.\n",
    "\n",
    "#### Linear activation function: $f=x$\n",
    "\n",
    "If the activation function $f$ is simply the identity function $f(\\vec{x}) = \\vec{x}$, then this derivative becomes very simple, as its derivative is $\\vec{1}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l(\\vec{w})}{\\partial \\vec{w}}=\\left ( \\vec{w}^{\\top}\\vec{x}-\\vec{y}\\right )\\vec{x} \n",
    "$$\n",
    "\n",
    "So, now our update for the gradient can be calculated, and it will be:\n",
    "\n",
    "$$\n",
    "-\\lambda\\frac{\\partial l(\\vec{w})}{\\partial \\vec{w}}=-\\lambda \\left ( \\vec{w}^{\\top}\\vec{x}-\\vec{y}\\right )\\vec{x} \n",
    "$$\n",
    "\n",
    "#### Logistic activation function: $f=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "If the activation function $f$ is the logistic function $f=\\frac{1}{1+e^{-\\vec{w}^{\\top}\\vec{x}}}$, its derivative is related to the function itself, and so the whole term becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l(\\vec{w})}{\\partial \\vec{w}}=\\left ( f(\\vec{w}^{\\top}\\vec{x})-\\vec{y}\\right )\\vec{x}f(\\vec{w}^{\\top}\\vec{x})(1-f(\\vec{w}^{\\top}\\vec{x})) \n",
    "$$\n",
    "\n",
    "#### ReLU activation function: $f=\\text{ReLU}=\\max(0,x)$\n",
    "\n",
    "This function is not differentiable at $x=0$, so we will arbitrarily say that\n",
    "\n",
    "$$\n",
    "f'(x)=\\left \\{ \\begin{array}{cc}1&: x>0\\\\0&:x\\leq 0 \\end{array} \\right .\n",
    "$$\n",
    "\n",
    "where, again, we made the choice to make the derivative 0 at $x=0$ in order to \"solve\" the discontinuity issue.\n",
    "\n",
    "#### Note\n",
    "Note also that the internal process of weighting the inputs $\\vec{x}$ with the weights of the neurons $\\vec{w}$ will include the bias implicitly with a 1 added to the first dimension of the inputs, such that \n",
    "\n",
    "$$\n",
    "\\vec{w}^{\\top}\\vec{x}=w_0+w_1x_1+w_2x_2+...+w_nx_n\n",
    "$$\n",
    "\n",
    "for $n$ neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer networks: Backpropagation\n",
    "Now, this approach is all fine, if we only have one layer of neurons - but what about multiple, connected neurons that live in different layers like the connections we see in the brain??\n",
    "\n",
    "Formally, we will start with a fully-connected neural network, in which each neuron is connected to all other neurons in the next down-stream layer.\n",
    "\n",
    "We have an input layer consisting of the vectorized inputs, and then several so-called \"hidden\" layers, containing neurons with an activation function $f$ that receive linearly-weighted inputs from their previous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fully_connected](004_NN_FCN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error in the output of an arbitrary neuron in a neural network will depend on all previous weights, so if we want to do gradient descent, we need to somehow know how to derive this.\n",
    "\n",
    "Again, we assume that each neuron $j$ will provide an output $y_j$, which is defined as the weighted sum of inputs pushed through the activation function:\n",
    "\n",
    "$$\n",
    "y_j = f(\\vec{w}\\vec{x})=f(\\sum w_{kj}x_k)\n",
    "$$\n",
    "\n",
    "So, the derivative of the total error with respect to any weight $w_{ij}$ is done using the full sets of dependents:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_{ij}}=\\frac{\\partial l}{\\partial y_{j}}\\frac{\\partial y_j}{\\partial w_{ij}}=\\frac{\\partial l}{\\partial y_{j}}\\frac{\\partial y_j}{\\partial (\\sum w_{kj}y_k)}\\frac{\\partial (\\sum w_{kj}y_k)}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "Let's look at the three terms. Let's start with the second term, which is the derivative of the error function. Let's assume it's the logistic function, which has the neat and simple derivative from above\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_j}{\\partial (\\sum w_{kj}y_k)}=f(\\sum w_{kj}y_k)(1-f(\\sum w_{kj}y_k))\n",
    "$$\n",
    "\n",
    "For the third term we observe that in the sum only one term contains the actual derivative weight $w_{ij}$, so that becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial (\\sum w_{kj}y_k)}{\\partial w_{ij}}=y_i\n",
    "$$\n",
    "\n",
    "Now, for the first term, we distinguish two cases: if the neuron is in the final layer, then $y_j=y_{end}$, which means:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial y_{j}}=\\frac{\\partial l}{\\partial y_{end}}= ...\n",
    "$$\n",
    "\n",
    "For \"inner\" neurons $j$ in other layers, however, this term needs to be evaluated with respect to earlier layers as well, since the error term depends on the outputs of all neurons connected to the present neuron. Let's put all these $a$ connected neurons into one set $N=\\{n_{1j},\\dots,n_{aj}\\}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l(y_{j})}{\\partial y_{j}}=\n",
    "\\sum_{n\\in N}\\left ( \\frac{\\partial l}{\\partial (\\sum w_{nj}y_n)}\\frac{\\partial (\\sum w_{nj}y_n)}{\\partial y_j} \\right )=\\sum_{n\\in N}\\left ( \\frac{\\partial l}{\\partial y_n}\\frac{\\partial y_n}{\\partial (\\sum w_{nj}y_n)}\\frac{\\partial (\\sum w_{nj}y_n)}{\\partial y_j} \\right )=\\sum_{n\\in N}\\left ( \\frac{\\partial l}{\\partial y_n}\\frac{\\partial y_n}{\\partial (\\sum w_{nj}y_n)}w_{jn} \\right )\n",
    "$$\n",
    "\n",
    "This means that we can calculate the derivative, once we know all of the derivatives with respect to the downstream connected neurons. This is why this is called backpropagation, since in order to run the algorithm, you start from the output layer, and then proceed backwards through the network!\n",
    "\n",
    "In total, we get for a logistic activation function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_{ij}}=y_i\\alpha_j\n",
    "$$\n",
    "\n",
    "with \n",
    "$$\n",
    "\\alpha_j:=\\frac{\\partial y_j}{\\partial (\\sum w_{kj}y_k)}\\frac{\\partial (\\sum w_{kj}y_k)}{\\partial w_{ij}}=\\left \\{ \\begin{array}{ll}... &: \\text{if }j\\text{ is an output layer neuron}\\\\... &: \\text{if }j\\text{ is an inner/hidden layer neuron} \\end{array}\\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example implementation\n",
    "\n",
    "Here's an example implementation that learns a simple one hidden-layer network with a few neurons in order to approximate a $f(x,y)=x^2+y^2+1$.\n",
    "\n",
    "We generate 400 input/output value pairs and then try to learn the whole thing using our update rules from above.\n",
    "\n",
    "Let's first define activation functions and their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return ...\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return ...\n",
    "\n",
    "def tanh(X):\n",
    "    return ...\n",
    "\n",
    "def tanh_derivative(X):\n",
    "    return ...\n",
    "\n",
    "def logistic(X):\n",
    "    return ...\n",
    "\n",
    "def logistic_derivative(X):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "xs=np.linspace(-4,4,100)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xs,relu(xs))\n",
    "plt.plot(xs,relu_derivative(xs))\n",
    "plt.grid()\n",
    "plt.title('Relu and Derivative')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xs,tanh(xs))\n",
    "plt.plot(xs,tanh_derivative(xs))\n",
    "plt.grid()\n",
    "plt.title('tanh and Derivative')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xs,logistic(xs))\n",
    "plt.plot(xs,logistic_derivative(xs))\n",
    "plt.grid()\n",
    "plt.title('Logistic Function (Sigmoid) and Derivative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a two-layer neural network that features two input dimensions, one fully-connected hidden layer, and a fully-connected output layer.\n",
    "\n",
    "The hidden layer will receive an activation function, whereas the output layer will simply be a linear layer, as we are trying to solve a regression problem (again, why do we not give this layer an activation function like tanh???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a two-layer neural network\n",
    "def create_model(X,hidden_nodes,output_dim=2):\n",
    "    # this will hold a dictionary of layers\n",
    "    model = {}\n",
    "    # input dimensionality\n",
    "    input_dim = X.shape[1]\n",
    "    # first set of weights from input to hidden layer 1\n",
    "    model['W1'] = ...\n",
    "    # set of biases\n",
    "    model['b1'] = ...\n",
    "    \n",
    "    # second set of weights from hidden layer 1 to output\n",
    "    model['W2'] = ...\n",
    "    # set of biases\n",
    "    model['b2'] = ...\n",
    "    return model\n",
    "\n",
    "# defines the forward pass given a model and data\n",
    "def feed_forward(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # first layer\n",
    "    z1 = ...\n",
    "    \n",
    "    # activation function\n",
    "    #a1 = ...\n",
    "    #a1 = ...\n",
    "    a1 = ...\n",
    "    \n",
    "    # second layer\n",
    "    z2 = ...\n",
    "    \n",
    "    # no activation function as this is simply a linear layer!!\n",
    "    out = ...\n",
    "    return z1, a1, z2, out\n",
    "\n",
    "# define the regression loss\n",
    "def calculate_loss(model,X,y,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out = feed_forward(model, X)\n",
    "    \n",
    "    # calculate L2 loss\n",
    "    loss = ...\n",
    "    \n",
    "    # add regulatization term to loss\n",
    "    loss += ...\n",
    "    \n",
    "    # return per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "# back-propagation for the two-layer network\n",
    "def backprop(X,y,model,z1,a1,z2,output,reg_lambda):\n",
    "    \n",
    "    # derivative of loss function\n",
    "    delta3 = ...\n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = ...\n",
    "    # and over all neurons\n",
    "    db2 = ...\n",
    "    \n",
    "    # derivative of activation function\n",
    "    #delta2 = ... #if logistic\n",
    "    #delta2 = ... #if tanh\n",
    "    delta2 = ... #if ReLU\n",
    "    \n",
    "    # multiply by input data\n",
    "    dW1 = ...\n",
    "    # and sum over all neurons\n",
    "    db1 = ...\n",
    "    \n",
    "    # add regularization terms on the two weights\n",
    "    dW2 += ...\n",
    "    dW1 += ...\n",
    "    \n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop\n",
    "def train(model, X, y, num_passes=100000, reg_lambda = 0.1, learning_rate = 0.001):\n",
    "    # whether to do stochastic gradient descent\n",
    "    sgd = True\n",
    "    \n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while done == False:\n",
    "        if sgd:\n",
    "            # choose a random set of points\n",
    "            randinds = np.random.choice(np.arange(len(y)),30,False)\n",
    "            # get predictions\n",
    "            z1,a1,z2,output = feed_forward(model, X[randinds,:])\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop(X[randinds,:],y[randinds],model,z1,a1,z2,output,reg_lambda)\n",
    "        else:\n",
    "            # get predictions\n",
    "            z1,a1,z2,output = feed_forward(model, X)\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop(X,y,model,z1,a1,z2,output,reg_lambda)\n",
    "            \n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        \n",
    "        # do some book-keeping every once in a while\n",
    "        if i % 1000 == 0:\n",
    "            loss = calculate_loss(model, X, y, reg_lambda)\n",
    "            losses.append(loss)\n",
    "            print(\"Loss after iteration {}: {}\".format(i, loss))\n",
    "            # very crude method to break optimization\n",
    "            if np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "        i += 1\n",
    "        if i>=num_passes:\n",
    "            done = True\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create the data and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDataOne = 15\n",
    "numData = numDataOne*numDataOne\n",
    "# create data for regression\n",
    "xs=np.linspace(-8,8,numDataOne)\n",
    "ys=np.linspace(-8,8,numDataOne)\n",
    "counter=0\n",
    "X=np.zeros((numData,2))\n",
    "y=np.zeros((numData,1))\n",
    "for r in np.arange(0,numDataOne):\n",
    "    for c in np.arange(0,numDataOne):\n",
    "        X[counter,:]=[xs[r],ys[c]]\n",
    "        y[counter]=xs[r]**2+ys[c]**2+1\n",
    "        counter=counter+1\n",
    "\n",
    "# training set size\n",
    "num_examples = len(X) \n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2 \n",
    "# output layer dimensionality\n",
    "nn_output_dim = 1  \n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "# regularization strength\n",
    "reg_lambda = 0.01 \n",
    "\n",
    "# create the model\n",
    "model = create_model(X,10,1)\n",
    "\n",
    "# train it\n",
    "model, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n",
    "\n",
    "# determine predictions of the trained model\n",
    "output = feed_forward(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the loss curve look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration [in thousands]')\n",
    "plt.ylabel('Average per-item loss')\n",
    "plt.title('Loss evolution')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the source data and - in a separate plot - the predictions.\n",
    "\n",
    "Remember that our data is a 2D surface, so we need to generate the mesh for plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "[xsm,ysm]=np.meshgrid(xs,ys)\n",
    "target = np.zeros((numDataOne,numDataOne))\n",
    "for i,x in enumerate(xs):\n",
    "    for j,y in enumerate(ys):\n",
    "        target[i,j]=x**2+y**2+1\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(xsm, ysm, target,cmap='viridis', edgecolor='none')\n",
    "ax.set_title('True surface plot')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(xsm, ysm, np.reshape(output[3],(numDataOne,numDataOne)),cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Predicted surface plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification problems\n",
    "\n",
    "Let us return to a classification problem again - we saw that in the previous section, we had used the standard L2 loss, minimizing the point-wise squared errors between target and prediction:\n",
    "\n",
    "$$\n",
    "L = \\sum ||\\vec{t}-\\vec{y}||^2\n",
    "$$\n",
    "\n",
    "This is - despite its problems - with respect to sensitivity to outliers, a standard loss for a regression problem often called the mean squared error (MSE) loss.\n",
    "\n",
    "So, would this loss also work for classification???\n",
    "\n",
    "If you think about the problem of classification, then this loss per se is fine, as we could, for example, require the output of the final layer to be the \"index\" of the class in a multi-class classification problem.\n",
    "\n",
    "However, there is another way to phrase the classification problem that represents an easier conceptualization and visualization.\n",
    "\n",
    "Again, we have $C$ classes to classify. Instead of having one output neuron, what we do, is to change the output layer such that it has $C$ output predictions (neurons).\n",
    "\n",
    "Here, each neuron $y_i$, $i=1...C$ in the final layer will express its support for the class $i$. Finding the \"correct\" class for the multi-class problem then simply becomes a problem of choosing $\\text{argmax}(y_i)$.\n",
    "\n",
    "So, how do we make such a loss?\n",
    "\n",
    "### Softmax\n",
    "\n",
    "First of all, choosing the maximum of a value is an operation that is not easily differentiable. As we have discussed before, one easy way to avoid this is to approximate the maximum operation with a \"softer\" version, the so-called softmax, which can be interpreted as a probability if properly normalized like so:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\vec{z})_i = p_i = \\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}} := \\frac{e^{z_i}}{\\Sigma N}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\vec{z}_i$ indexes the $i$-th output of the neural network.\n",
    "\n",
    "In order to do backpropagation, we need the derivative of this softmax operation.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial z_k}\n",
    "$$\n",
    "\n",
    "We know have two cases for this derivative depending on the index $k$: $k=i$ and $k\\neq i$. \n",
    "\n",
    "#### Case 1: $k=i$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial z_i}=\\frac{\\partial}{\\partial z_i}\\frac{e^{z_i}}{\\Sigma N}=\\frac{\\Sigma N \\frac{\\partial e^{z_i}}{\\partial z_i}-e^{z_i}\\cdot\\frac{\\partial \\Sigma N}{\\partial z_i}}{(\\Sigma N)^2}\n",
    "$$\n",
    "\n",
    "the two missing partial derivatives are calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\Sigma N}{\\partial z_i} = \\frac{\\partial \\sum_{i \\neq j} e^{z_j}}{\\partial z_i}+\\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}\n",
    "$$\n",
    "\n",
    "with that, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial z_i}=\\frac{\\Sigma N\\cdot e^{z_i} - e^{z_i}\\cdot e^{z_i}}{(\\Sigma N)^2}=\\frac{e^{z_i}}{\\Sigma N}\\left ( 1 - \\frac{e^{z_i}}{\\Sigma N}\\right ) = ...\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow\\frac{\\partial p_i}{\\partial z_i}=...\n",
    "$$\n",
    "\n",
    "#### Case 1: $k\\neq i$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial z_j}=\\frac{\\partial}{\\partial z_j}\\frac{e^{z_i}}{\\Sigma N}=\\frac{\\Sigma N \\frac{\\partial e^{z_i}}{\\partial z_j}-e^{z_i}\\cdot\\frac{\\partial \\Sigma N}{\\partial z_j}}{(\\Sigma N)^2}\n",
    "$$\n",
    "\n",
    "and now the first term in the nominator is 0:\n",
    "\n",
    "$$\n",
    "= 0 - \\frac{e^{z_i}\\cdot e^{z_j}}{(\\Sigma N)^2} = 0 - \\frac{e^{z_i}}{\\Sigma N}\\cdot \\frac{e^{z_j}}{\\Sigma N}=...\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow\\frac{\\partial p_i}{\\partial z_j}=-...\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss\n",
    "\n",
    "Once we have a probability distribution $\\vec{p}$ via the softmax, the next step will be to determine a loss that calculates how well this distribution fits the intended target labels $\\vec{t}$.\n",
    "\n",
    "This loss is known as cross entropy loss and for multi-class problem with $C$ classes is defined as:\n",
    "\n",
    "$$\n",
    "L(\\vec{t},\\vec{p})=-\\sum_{i=1}^C \\left( t_i \\cdot \\log{(\\vec{p}_i)} \\right)\n",
    "$$\n",
    "\n",
    "Where does this come from?\n",
    "\n",
    "The target label distribution is encoded in a special format, the so-called \"one-hot\" encoding, in which we have $C$ elements for a given datapoint belonging to class $c$, such that:\n",
    "\n",
    "$$\n",
    "t_c=1, \\text{ and } t_i=0 \\text{ for } i \\neq c\n",
    "$$\n",
    "\n",
    "With this, we can see that $C-1$ terms in the sum for the loss are simply zero, and the total loss for any given datapoint is simply:\n",
    "\n",
    "$$\n",
    "L(\\vec{t},\\vec{p})=... \\text{ for an input } \\in \\text{class }c\n",
    "$$\n",
    "\n",
    "To see how this loss works, let's make a three-class example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for a three class problem for class c=1\n",
    "true_label = np.array([1,0,0])\n",
    "\n",
    "# three normalized guesses expressed as probabilities through a softmax operation\n",
    "p_guess1 = np.array([.1,.85,.05])\n",
    "p_guess2 = np.array([.5,.3,.2])\n",
    "p_guess3 = np.array([.95,.03,.02])\n",
    "\n",
    "# let's calculate the losses\n",
    "print('Loss for Guess 1=',-np.sum(...)\n",
    "print('Loss for Guess 2=',-np.sum(...)\n",
    "print('Loss for Guess 3=',-np.sum(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as the class probabilities for class 1 go up, the corresponding loss goes down.\n",
    "\n",
    "Plotting the loss for a two-class problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0.0001,0.9999,150)\n",
    "ys = 1-xs\n",
    "loss = np.zeros_like(xs)\n",
    "for i in range(len(xs)):\n",
    "    loss[i]=...\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xs,loss)\n",
    "plt.xlabel('Probability for class 1')\n",
    "plt.ylabel('Cross entropy loss for two-class problem')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as the probability for the target class approaches 0, the loss goes quickly up, providing a nice gradient!!\n",
    "\n",
    "Finally, we note that the total cross-entropy loss for a set of $N$ datapoints is given as:\n",
    "\n",
    "$$\n",
    "L(\\vec{t},\\vec{p})=-\\sum_{j=1}^N \\sum_{i=1}^C \\left( t_{ji} \\cdot \\log{(\\vec{p}_{ji})} \\right)\n",
    "$$\n",
    "\n",
    "Let's make a little function that demonstrates the CCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition of CCE\n",
    "def categorical_cross_entropy(t_list,p_list):\n",
    "    # convert list to numpy variable\n",
    "    t_list = np.float_(t_list)\n",
    "    p_list = np.float_(p_list)\n",
    "    # initialize the loss\n",
    "    losses = []\n",
    "    for t,p in zip(t_list,p_list):\n",
    "        loss = ...\n",
    "        losses.append(loss)\n",
    "        print(\"t {}, p {},loss {}\".format(t,p,loss))\n",
    "    return np.sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = [[1,0,0],[0,1,0],[0,0,1],[1,0,0]]\n",
    "p_list = [[0.91,0.04,0.05],[0.11,0.8,0.09],[0.3,0.1,0.6],[0.25,0.4,0.35]]\n",
    "print('\\nTotal loss for three class problem:',categorical_cross_entropy(t_list,p_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of cross entropy with softmax\n",
    "\n",
    "In order to do backpropagation, we need the derivative of the cross entropy function.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_i}=-\\sum_{j=1}^N \\frac{\\partial}{\\partial z_i}(t_j\\log p_j) = -\\sum_{j=1}^N t_j \\frac{\\partial}{\\partial z_i}\\log p_j = -\\sum_{j=1}^N t_j \\frac{1}{p_j}\\frac{\\partial p_j}{\\partial z_i} \n",
    "$$\n",
    "\n",
    "now, splitting the sum again:\n",
    "\n",
    "$$\n",
    "= -\\frac{t_i}{p_i}\\frac{\\partial p_i}{\\partial z_i}-\\sum_{j\\neq i} \\frac{t_j}{p_j}\\frac{\\partial p_j}{\\partial z_i} \n",
    "$$\n",
    "\n",
    "where we now need the derivatives of the probabilities $p_k$ with respect to $z_i$. We calculated these above for the two cases $k=i$ and $k\\neq i$, and can therefore substitute:\n",
    "\n",
    "$$\n",
    "= -\\frac{t_i}{p_i}p_i(1-p_i)-\\sum_{j\\neq i} \\frac{t_j}{p_j}(-p_i p_j) = -t_i +t_i p_i +\\sum_{j\\neq i} t_j p_i\n",
    "$$\n",
    "\n",
    "so the missing term $t_i p_i$ can be put back INTO the sum to yield:\n",
    "\n",
    "$$\n",
    "= ...\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\rightarrow\\frac{\\partial L}{\\partial z_i}= ...\n",
    "$$\n",
    "\n",
    "which simply states that the derivative of the loss for a given example is the ....\n",
    "\n",
    "A LOT of number wrangling for a VERY simple result!!\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "So, let's implement this for classification. For this, we will take the exact same code as above and create a one-hidden layer neural network. This time, however, we will add the softmax-cross-entropy part in order to support classification of the results.\n",
    "\n",
    "We can actually re-use some of the functions from above for this, and most functions here only are changed very little to demonstrate the updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp_scores = np.exp(X)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True) \n",
    "\n",
    "def feed_forward_class(model, x):\n",
    "    # get weights and biases\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # first layer\n",
    "    z1 = ...\n",
    "    \n",
    "    # activation function\n",
    "    #a1 = logistic(z1)\n",
    "    #a1 = tanh(z1)\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    # second layer\n",
    "    z2 = ...\n",
    "    \n",
    "    # softmax to get probabilities\n",
    "    out = ...\n",
    "    \n",
    "    return z1, a1, z2, out, W1, W2\n",
    "\n",
    "def calculate_loss_class(model,X,y,reg_lambda):\n",
    "    num_examples = X.shape[0]\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    \n",
    "    # what are the current predictions\n",
    "    z1, a1, z2, out, tmp1, tmp2 = feed_forward_class(model, X)\n",
    "    \n",
    "    # convert to probabilities via normalizing\n",
    "    probs = ...\n",
    "    \n",
    "    # convert to \"logits\"\n",
    "    corect_logprobs = ...\n",
    "    \n",
    "    # and sum to get cross entropy loss\n",
    "    loss = ...\n",
    "    \n",
    "    # add regulatization term to loss)\n",
    "    loss += ...\n",
    "    \n",
    "    # return average per-item loss\n",
    "    return 1./num_examples * loss\n",
    "\n",
    "def backprop_class(X,y,model,z1,a1,z2,output,reg_lambda):\n",
    "    \n",
    "    # loss derivative\n",
    "    delta3 = output\n",
    "    \n",
    "    # we have to think long and hard about this line!\n",
    "    # this should be (output - y) as the outer loss derivative\n",
    "    # from the cross-entropy + softmax result shown above\n",
    "    #\n",
    "    # why does this code work??\n",
    "    ...\n",
    "    \n",
    "   \n",
    "    # multiply this by activation outputs of hidden layer\n",
    "    dW2 = ...\n",
    "    # and over all neurons\n",
    "    db2 = ...\n",
    "    \n",
    "    # derivative of activation function\n",
    "    #delta2 = delta3.dot(model['W2'].T) * logistic_derivative(a1) #if logistic\n",
    "    #delta2 = delta3.dot(model['W2'].T) * tanh_derivative(a1) #if tanh\n",
    "    delta2 = delta3.dot(model['W2'].T) * relu_derivative(a1) #if ReLU\n",
    "    \n",
    "    # multiply by input data\n",
    "    dW1 = ...\n",
    "    # and sum over all neurons\n",
    "    db1 = ...\n",
    "    \n",
    "    # add regularization terms on the two weights\n",
    "    dW2 += ...\n",
    "    dW1 += ...\n",
    "    \n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "# simple training loop - identical to above, only changing the function calls!\n",
    "def train(model, X, y, num_passes=100000, reg_lambda = 0.1, learning_rate = 0.001):\n",
    "    # whether to do stochastic gradient descent\n",
    "    sgd = True\n",
    "    \n",
    "    # variable that checks whether we break iteration\n",
    "    done = False\n",
    "    \n",
    "    # keeping track of losses\n",
    "    previous_loss = float('inf')\n",
    "    losses = []\n",
    "\n",
    "    # iteration counter\n",
    "    i = 0\n",
    "    while done == False:\n",
    "        if sgd:\n",
    "            # choose a random set of points\n",
    "            randinds = np.random.choice(np.arange(len(y)),30,False)\n",
    "            # get predictions\n",
    "            z1,a1,z2,output,w1,w2 = feed_forward_class(model, X[randinds,:])\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop_class(X[randinds,:],y[randinds],model,z1,a1,z2,output,reg_lambda)\n",
    "        else:\n",
    "            # get predictions\n",
    "            z1,a1,z2,output,w1,w2 = feed_forward_class(model, X)\n",
    "            # feed this into backprop\n",
    "            dW1, dW2, db1, db2 = backprop_class(X,y,model,z1,a1,z2,output,reg_lambda)\n",
    "            \n",
    "        # given the results of backprop, update both weights and biases\n",
    "        model['W1'] -= learning_rate * dW1\n",
    "        model['b1'] -= learning_rate * db1\n",
    "        model['W2'] -= learning_rate * dW2\n",
    "        model['b2'] -= learning_rate * db2\n",
    "        \n",
    "        # do some book-keeping every once in a while\n",
    "        if i % 1000 == 0:\n",
    "            loss = calculate_loss_class(model, X, y, reg_lambda)\n",
    "            losses.append(loss)\n",
    "            print(\"Loss after iteration {}: {}\".format(i, loss))\n",
    "            # very crude method to break optimization\n",
    "            if np.abs((previous_loss-loss)/previous_loss) < 0.001:\n",
    "                done = True\n",
    "            previous_loss = loss\n",
    "        i += 1\n",
    "        if i>=num_passes:\n",
    "            done = True\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(100, noise=0.10)\n",
    "\n",
    "#X, y = datasets.make_circles(n_samples=100, shuffle=True, noise=0.1, factor=0.4)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=50, edgecolor=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set size\n",
    "num_examples = len(X) \n",
    "# input layer dimensionality\n",
    "nn_input_dim = 2 \n",
    "# output layer dimensionality - now TWO!\n",
    "nn_output_dim = 2  \n",
    "# learning rate for gradient descent\n",
    "learning_rate = 0.001\n",
    "# regularization strength\n",
    "reg_lambda = 0.01 \n",
    "\n",
    "# create the model\n",
    "model = create_model(X,5,2)\n",
    "\n",
    "# train it\n",
    "model, losses = train(model,X, y, reg_lambda=reg_lambda, learning_rate=learning_rate)\n",
    "\n",
    "# determine predictions of the trained model\n",
    "output = feed_forward(model, X)\n",
    "\n",
    "# and get the max activating output to determine class membership\n",
    "preds = np.argmax(output[3], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration [in thousands]')\n",
    "plt.ylabel('Average per-item loss')\n",
    "plt.title('Loss evolution')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will plot the decision surface of the resulting classifier. \n",
    "\n",
    "For this, we will sample the full space and then use the matplotlib function \"contourf\" to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=np.linspace(-1.5,2.2,150)\n",
    "ys=np.linspace(-0.7,1.3,150)\n",
    "[xsm,ysm]=np.meshgrid(xs,ys)\n",
    "predarray = np.zeros((len(xs),len(ys)))\n",
    "\n",
    "for i,xt in enumerate(xs):\n",
    "    for j,yt in enumerate(ys):\n",
    "        tmp = feed_forward_class(model,np.array([xt,yt]))\n",
    "        predarray[j,i] = ...\n",
    "        \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.contourf(xs,ys,predarray,alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=50, edgecolor=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the output of the neural network is actually the probability that the point belongs to a certain class. We can actually plot that directly, by simply taking the \"max\" instead of the \"argmax\" above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=np.linspace(-1.5,2.2,150)\n",
    "ys=np.linspace(-0.7,1.3,150)\n",
    "[xsm,ysm]=np.meshgrid(xs,ys)\n",
    "predarray = np.zeros((len(xs),len(ys)))\n",
    "\n",
    "for i,xt in enumerate(xs):\n",
    "    for j,yt in enumerate(ys):\n",
    "        tmp = feed_forward_class(model,np.array([xt,yt]))\n",
    "        predarray[len(ys)-1-j,i] = ...\n",
    "        \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(predarray,extent=(-1.5,2.2,-0.7,1.3),aspect='auto')\n",
    "#plt.contourf(xs,ys,predarray,alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=y, s=50, edgecolor=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universality\n",
    "\n",
    "It has been shown that a neural network with one hidden layer, containing sigmoid activation functions (so, non-linear!) can approximate any function!\n",
    "\n",
    "Therefore, such networks are also called universal approximators.\n",
    "\n",
    "Cybenko, G. Math. Control Signal Systems (1989) 2: 303:\n",
    "\n",
    "\"In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.\"\n",
    "\n",
    "The full paper can be found here:\n",
    "\n",
    "https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf\n",
    "\n",
    "\n",
    "**Breakout**\n",
    "\n",
    "1. The visual proof that explicitly constructs a NN to approximate \"any\" function:\n",
    "\n",
    "https://www.mathematik.uni-wuerzburg.de/fileadmin/10040900/2019/Seminar__Artificial_Neural_Network__24_9__.pdf\n",
    "\n",
    "\n",
    "2. The excellent exposition of the proof of Cybenko by Daniela McNeela:\n",
    "\n",
    "https://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breakout**\n",
    "\n",
    "Playing around with a neural network simulator\n",
    "\n",
    "https://playground.tensorflow.org/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
