{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ON4WbEBvk9Z9"
   },
   "source": [
    "# 002 Linear Regression with Gradient Descent\n",
    "\n",
    "Let's try to do linear regression in yet another way.\n",
    "\n",
    "## Predicting Student's grades from Study Hours\n",
    "\n",
    "Let's predict how well students do based on how much they study. Hey, there should be something there, right??\n",
    "\n",
    "We take data from an online source, which will give us 100 data points, which we then split into X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lq6qDjhHnbak"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1a23JYWn5oM"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('https://raw.githubusercontent.com/kaustubholpadkar/Linear_Regression-Gradient_Descent-Octave/master/data.csv', delimiter=',')\n",
    "print(data.shape)\n",
    "X = data[:, 0]\n",
    "Y = data[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPSTKDAXpLAN"
   },
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "qdTK7EOQpL5Y",
    "outputId": "251e0c8c-2186-411a-df29-502c90b4cc85"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.scatter(X, Y)\n",
    "plt.title('Student Data : Expected Grades vs Hours of Study', fontsize=18)\n",
    "plt.xlabel('study time [hours]', fontsize=14)\n",
    "plt.ylabel('expected grade [points]', fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there seems to be some sort of trend here ^^\n",
    "\n",
    "### Model function\n",
    "\n",
    "We would like to fit a line to our data, so let's first define the model of a line, given its two parameters (slope $m$ and intercept $b$):\n",
    "\n",
    "$y = mX+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "enVE4tHWsBBF"
   },
   "outputs": [],
   "source": [
    "def line(m, b, X):\n",
    "    return m * X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Next, we need to define our loss function that we want to optimize. As usual, we take the L2, Least-Squares-Loss:\n",
    "\n",
    "$E(m,b) = \\sum_i\\left ( Y_i-\\text{line}(m,b,X_i) \\right )^2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KOQVUjBs-Ub"
   },
   "outputs": [],
   "source": [
    "def loss (m, b, X, Y) :\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is a function of two parameters and describes a surface: $E(m,b): \\mathrm{R}^2\\rightarrow\\mathrm{R}$\n",
    "\n",
    "Let's plot this for a range of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.linspace(-20,20,50)\n",
    "bs = np.linspace(-40,80,50)\n",
    "\n",
    "[msm,bsm] = ...\n",
    "\n",
    "lossSurface = np.zeros((50,50)) \n",
    "\n",
    "for i,m in enumerate(ms):\n",
    "    for j,b in enumerate(bs):\n",
    "        lossSurface[i,j]=loss(m,b,X,Y)\n",
    "        \n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(msm, bsm, lossSurface,cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there seems to be a valley - and hopefully in this valley, we can find a minimum point, which is the lowest point of the loss function and our global optimum!!\n",
    "\n",
    "Q: How do we find this??\n",
    "\n",
    "A: We start at a point and walk ...\n",
    "\n",
    "Q: In which direction do we walk??\n",
    "\n",
    "A: In the direction that has the ...!\n",
    "\n",
    "Q: How do we find this direction??\n",
    "\n",
    "A: It's the ... (two-dimensional!!)\n",
    "\n",
    "Q: How far do we walk down in this direction\n",
    "\n",
    "A: ... and may miss our optimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7TE79NG-tyLG"
   },
   "source": [
    "### Calculate Gradients\n",
    "\n",
    "Now, we have to do gradient descent. We will walk in the direction of the negative gradient of the loss function $E$.\n",
    "\n",
    "The loss function has two parameters $E(m,b)$:\n",
    "\n",
    "$\\frac{\\partial E}{\\partial m}=-2*\\sum (Y-\\text{line})*X$\n",
    "\n",
    "and\n",
    "\n",
    "$\\frac{\\partial E}{\\partial b}=-2*\\sum (Y-\\text{line})*(1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yy9yxye4t0BJ"
   },
   "outputs": [],
   "source": [
    "# this is the total gradient\n",
    "def gradient (m, b, X, Y) :\n",
    "    dm = ...\n",
    "    db = ...\n",
    "    return (dm, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVo5TbKyu9KL"
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Now, the very simple gradient descent. In each step, we simply go a little bit in the direction of the negative gradient:\n",
    "\n",
    "$m_{t+1}=m_t-\\text{lr}\\frac{\\partial E}{\\partial m}$\n",
    "\n",
    "$b_{t+1}=b_t-\\text{lr}\\frac{\\partial E}{\\partial b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fprg4-tcu-4v"
   },
   "outputs": [],
   "source": [
    "def gradient_descent (m, b, X, Y, learning_rate, max_iteration,waittime=0.02,onlineplot=True,lossplot=True) :\n",
    "    errors = np.zeros(max_iteration)\n",
    "    x = np.arange(25, 75)\n",
    "    for i in range(max_iteration):\n",
    "        # calculate the gradients\n",
    "        dm, db = gradient (m, b, X, Y)\n",
    "        # update the parameters with a little gradient\n",
    "        m -= learning_rate * dm\n",
    "        b -= learning_rate * db\n",
    "        # evaluate the current line prediction\n",
    "        y = [m * x_i + b for x_i in x]\n",
    "        errors[i]=loss(m, b, X, Y)\n",
    "        print(errors[i])\n",
    "        # plot it into plot\n",
    "        if onlineplot:\n",
    "            plt.figure(figsize=(10,6))\n",
    "            display.clear_output(wait=True)\n",
    "            plt.xlim(20, 80)\n",
    "            plt.ylim(0, 140)\n",
    "            plt.scatter(X,Y,c='b')\n",
    "            plt.plot(x,y,c='k')\n",
    "            plt.grid()\n",
    "            plt.title('current iteration {}, total loss = {:.2f}'.format(i+1,loss(m,b,X,Y)))\n",
    "            time.sleep(waittime)\n",
    "            plt.show()\n",
    "    # plot the total loss over iterations\n",
    "    if lossplot:\n",
    "        fig,ax = plt.subplots(figsize=(8,6))\n",
    "        plt.plot(errors,'b.-')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Summed Squared Error')\n",
    "        plt.grid()\n",
    "    return (m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMIZFxUhwQZO"
   },
   "source": [
    "### Train Model\n",
    "\n",
    "We have to initialize gradient descent in order to start. Let's start at $m=b=0$, which is a flat line. This is obviously going to be a bad model with a high loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "-0okDGSCwUC7",
    "outputId": "221dd109-c839-49b7-e354-29ab97c6fa8b"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "m = 0\n",
    "b = 0\n",
    "\n",
    "# print out current total loss and loss per point \n",
    "print(loss(m,b,X,Y),loss(m,b,X,Y)/len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the evolution of the optimization slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "-0okDGSCwUC7",
    "outputId": "221dd109-c839-49b7-e354-29ab97c6fa8b"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-7\n",
    "max_iteration = 20\n",
    "waittime = 1\n",
    "\n",
    "m = 0\n",
    "b = 0\n",
    "m, b = gradient_descent (m, b, X, Y, learning_rate, max_iteration, waittime=waittime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MFp2MUlexVd5"
   },
   "source": [
    "We can see that the total error goes down nicely across iterations.\n",
    "\n",
    "Just to see the effect of the learning rate, let's try to learn \"faster\", by making bigger steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tcMQWkuvwYu7",
    "outputId": "02ba6dd8-b680-4582-e5ad-95a81c741121"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "max_iteration = 10\n",
    "\n",
    "m = 0\n",
    "b = 0\n",
    "m, b = gradient_descent (m, b, X, Y, learning_rate, max_iteration, waittime=waittime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsa_VlHd8ged"
   },
   "source": [
    "And, that led the optimization to go crazy.\n",
    "\n",
    "**Remember: gradient descent needs adequate step sizes and hence and adequate learning rate!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJajnpzW8p2L"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-7\n",
    "max_iteration = 500\n",
    "\n",
    "m = 0\n",
    "b = 0\n",
    "m, b = gradient_descent (m, b, X, Y, learning_rate, max_iteration, onlineplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dh6PmQOwxcKk"
   },
   "source": [
    "Let's plot the resulting fit together with numpy's ```polyfit``` function result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "B0h7MJE0xcKo",
    "outputId": "5218e8ce-c82d-47cd-9631-29c1fd90a2e3"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "# Generate data for plotting line after gradient descent\n",
    "x = np.arange(25, 75)\n",
    "y = [m * x_i + b for x_i in x]\n",
    "\n",
    "# numpy linear regression\n",
    "np_m, np_b = np.polyfit(X, Y, 1)\n",
    "# Generate data for plotting line from numpy linear regression\n",
    "np_x = np.arange(25, 75)\n",
    "np_y = [np_m * x_i + np_b for x_i in np_x]\n",
    "\n",
    "# plot data and best-fit lines\n",
    "plt.plot(X, Y, 'o', x, y, '-', np_x, np_y, '--')\n",
    "plt.legend(loc='upper left', labels=['data points', 'best-fit line', 'numpy line'])\n",
    "\n",
    "fig.suptitle('Student Data : Expected Grades vs Hours of Study', fontsize=18)\n",
    "plt.xlabel('study time [hours]', fontsize=14)\n",
    "plt.ylabel('expected grade [points]', fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear optimization with Gradient Descent\n",
    "Obviously, fitting a line is not really the fanciest of applications, since it is well-known that this problem has an analytic solution.\n",
    "\n",
    "What about non-linear data? Let's first turn to the simple problem of one-dimensional optimization.\n",
    "\n",
    "We have a function $f(x)$, for which we are trying to find an optimal point, i.e., a point $x_{opt}$, at which $f(x_{opt})=f_{opt}$ is either a minimum or a maximum. \n",
    "\n",
    "If we know the gradient then this of course means that $\\frac{d}{dx}f(x) = g(x)$ at this point needs to be 0: $g(x_{opt})=0$. \n",
    "\n",
    "So, we simply choose a starting point and walk along the function in the direction of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the total \"loss\" function to optimize\n",
    "def loss_nl(x):\n",
    "    return(np.cos(x+0.5)*np.exp(0.5*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient of the function\n",
    "def gradient_nl(x):\n",
    "    return(-np.sin(x+0.5)*np.exp(0.5*x)+0.5*np.cos(x+0.5)*np.exp(0.5*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our usual gradient descent algorithm\n",
    "def gradient_descent_nl(xmax, learning_rate, max_iteration,waittime=0.02,onlineplot=True,lossplot=True) :\n",
    "    errors = np.zeros(max_iteration)\n",
    "    x = np.arange(-5,10,0.1)\n",
    "    for i in range(max_iteration):\n",
    "        dxmax = gradient_nl(xmax)\n",
    "        xmax -= learning_rate * dxmax\n",
    "        y = loss_nl(x)\n",
    "        errors[i]=dxmax\n",
    "        if onlineplot:\n",
    "            plt.figure(figsize=(10,6))\n",
    "            display.clear_output(wait=True)\n",
    "            plt.xlim(-5, 10)\n",
    "            plt.ylim(-200, 200)\n",
    "            plt.plot(x,y,c='k')\n",
    "            plt.plot([xmax,xmax],[-200,200],c='r')\n",
    "            plt.grid()\n",
    "            plt.title('current iteration {}, optimal point at = {:.2f}'.format(i+1,xmax))\n",
    "            time.sleep(waittime)\n",
    "            plt.show()\n",
    "    if lossplot:\n",
    "        fig,ax = plt.subplots(figsize=(8,6))\n",
    "        plt.plot(errors,'b.-')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Summed Squared Error')\n",
    "        plt.grid()\n",
    "    return (xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "max_iteration = 30\n",
    "waittime = 0.1\n",
    "xstart = 1.01\n",
    "xmax = gradient_descent_nl (xstart, learning_rate, max_iteration, waittime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complex function\n",
    "def loss_nl(x):\n",
    "    return(np.cos(x+0.5)*np.exp(0.5*np.sin(x)+np.log(x**2)))\n",
    "\n",
    "# fun derivative\n",
    "def gradient_nl(x):\n",
    "    return(np.cos(x + 1/2)*np.exp(np.log(x**2) + np.sin(x)/2)*(np.cos(x)/2 + 2/x) - np.sin(x + 1/2)*np.exp(np.log(x**2) + np.sin(x)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "max_iteration = 30\n",
    "waittime = 0.1\n",
    "xstart = -4.9\n",
    "xmax = gradient_descent_nl (xstart, learning_rate, max_iteration, waittime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting to an exponential function with least squares and gradient descent\n",
    "\n",
    "So, let's apply this idea to another dataset. Here, we take data from the COVID outbreak in the US, available at...\n",
    "\n",
    "I cut out the 20 days of the initial data, when the death toll due to COVID started to rise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('national-history.csv', delimiter=',')\n",
    "print(data.shape)\n",
    "X = np.arange(20)\n",
    "Y = data[340:360, 2]\n",
    "Y = np.flip(Y)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(X,Y,'ko-')\n",
    "plt.grid()\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Number of COVID-related deaths in the US')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the exponential model\n",
    "def nl_exp(l, b, X):\n",
    "    return(np.exp(l*X+b))\n",
    "\n",
    "# define the least squares loss\n",
    "def loss_nl (l, b, X, Y) :\n",
    "    return np.sum(np.square(Y - nl_exp(l, b, X)))\n",
    "\n",
    "# derivatives of the loss function with regard to the two parameters\n",
    "def gradient_nl (l, b, X, Y) :\n",
    "    dl = - 2*np.sum((Y - nl_exp(l, b, X)) * X * np.exp(l*X+b))\n",
    "    db = - 2*np.sum((Y - nl_exp(l, b, X)) * np.exp(l*X+b))\n",
    "    return (dl, db)\n",
    "\n",
    "# actual gradient descent [identical in its logic to the function above]\n",
    "def gradient_descent_nl (l, b, X, Y, learning_rate, max_iteration,waittime=0.02,onlineplot=True,lossplot=True) :\n",
    "    errors = np.zeros(max_iteration)\n",
    "    x = np.arange(0, 20)\n",
    "    for i in range(max_iteration):\n",
    "        \n",
    "        dl, db = gradient_nl (l, b, X, Y)\n",
    "        l -= learning_rate * dl\n",
    "        b -= learning_rate * db\n",
    "        y = [np.exp(l* x_i + b) for x_i in x]\n",
    "        errors[i]=loss(l, b, X, Y)\n",
    "        if onlineplot:\n",
    "            plt.figure(figsize=(10,6))\n",
    "            display.clear_output(wait=True)\n",
    "            plt.xlim(0, 20)\n",
    "            plt.ylim(0, 1200)\n",
    "            plt.scatter(X,Y,c='b')\n",
    "            plt.plot(x,y,c='k')\n",
    "            plt.grid()\n",
    "            plt.title('current iteration {}, total loss = {:.2f}, l={}, b={}'.format(i+1,loss(m,b,X,Y),l,b))\n",
    "            time.sleep(waittime)\n",
    "            plt.show()\n",
    "    if lossplot:\n",
    "        fig,ax = plt.subplots(figsize=(8,6))\n",
    "        plt.plot(errors,'b.-')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Summed Squared Error')\n",
    "        plt.grid()\n",
    "    return (m, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_nl(0.1,1,X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an **extremely** fiddly optimization, as the learning rate can easily overshoot in the later stages. You are welcome to try different starting points and learning rates for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-10\n",
    "max_iteration = 1000\n",
    "waittime = 0.01\n",
    "l = 0.1\n",
    "b = 1\n",
    "l, b = gradient_descent_nl (l, b, X, Y, learning_rate, max_iteration, waittime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the question now of course is: is this really an exponential growth? Given the processes of virus transmission, we may have reason to believe so, but as you can see, the overall growth seems sub-exponential.\n",
    "\n",
    "In addition, we have tried to optimize this with the \"raw\" exponential function, which - as you can see from the fiddling with learning rates - is really not very convenient for optimization. \n",
    "\n",
    "So, there are many better ways to do this -> see your homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Linear Regression Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
