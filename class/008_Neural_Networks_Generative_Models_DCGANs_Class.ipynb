{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WteDoo9y3OXB"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision import transforms, utils\n",
        "import glob\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X41LXf13OXG"
      },
      "source": [
        "# Generative Adversarial Networks (GANs)\n",
        "\n",
        "\n",
        "The following code heavily borrows from the nice introduction available at: https://github.com/diegoalejogm/gans/blob/master/1.%20Vanilla%20GAN%20PyTorch.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilBX16SA3OXL"
      },
      "source": [
        "## Face dataset as class\n",
        "Let's first define a class for the faces that we would like to learn from. We will derive this class from the PyTorch class `Dataset`, and will need to overload the constructor `__init__` and the indexing operator `__getitem__`. For convenience, we also define the length of the dataset and a function that shows a picture of the loaded data.\n",
        "\n",
        "The dataset is already pickled and consists of >13000 cropped faces from the Labelled-Faces-In-The-Wild (LFW) dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s3EWpgEp3OXO"
      },
      "outputs": [],
      "source": [
        "class faceDataset(Dataset):\n",
        "    \"\"\"Face dataset.\"\"\"\n",
        "    def __init__(self, pickle_file, normalize=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pickle_file (string): full path to pickled face data from LFW.\n",
        "            normalize (callable, optional): Optional normalization of images\n",
        "        \"\"\"\n",
        "        self.pickle_file = pickle_file\n",
        "        self.normalize = normalize\n",
        "        with open(self.pickle_file, 'rb') as fo:\n",
        "            tmp = pickle.load(fo,encoding='bytes')\n",
        "            tmp = tmp.reshape((tmp.shape[0],1,32,32))\n",
        "            self.images = torch.from_numpy(tmp)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sample=self.images[idx,:]\n",
        "\n",
        "        if self.normalize==1:\n",
        "            sample = (sample)/sample.max()\n",
        "        return sample\n",
        "    def show(self,idx):\n",
        "        plt.figure()\n",
        "        plt.imshow(self.images[idx,0,:].numpy().squeeze(),cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "2PP3EByr3OXQ",
        "outputId": "f12b475a-a7ca-4a8a-c869-69980a96349f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f560b976aaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mface_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'faces_python.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mface_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c68de0ceb0b7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pickle_file, normalize)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'faces_python.pkl'"
          ]
        }
      ],
      "source": [
        "face_dataset = faceDataset(pickle_file='faces_python.pkl',normalize=0)\n",
        "face_dataset.show(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSfV88x73OXR"
      },
      "source": [
        "In order to make training more comfortable, we can use PyTorch's own `DataLoader` class, which takes a dataset and returns batches from it in an iterator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or1sOTUx3OXR"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(face_dataset, batch_size=100,\n",
        "                        shuffle=True, num_workers=0)\n",
        "num_batches = len(dataloader)\n",
        "print('face dataset has',len(face_dataset),'faces and',num_batches,'batches')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTcSMB7f3OXS"
      },
      "outputs": [],
      "source": [
        "# sampling from dataloader and converting it to NCHW format\n",
        "face_sample = next(iter(dataloader)).reshape(100,1,32,32)\n",
        "face_grid = utils.make_grid(face_sample, nrow=10).numpy()\n",
        "\n",
        "# Converting torch CHW format to matplotlib HWC format\n",
        "face_grid = np.moveaxis(face_grid, 0, -1)\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
        "ax.imshow(face_grid, cmap='gray')\n",
        "ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DGmpfA63OXT"
      },
      "source": [
        "## Discriminator\n",
        "\n",
        "We first define the discriminator, which is supposed to detect whether an image is fake or not.\n",
        "\n",
        "It is derived from PyTorch's `nn.Module` and needs to provide the constructor `__init__` in which we set up the network layers and the `forward` function, which defines how to pass information from one layer to the next.\n",
        "\n",
        "Instead of our simple, fully-connected, three-layer network from before, we now use a convolutional architecture with increasing filter size, which we know works well with images.\n",
        "\n",
        "The activation function is set to a leaky ReLU, which improves stability a bit. Instead of dropout we use batch normalization, which is known to improve generalization for CNNs.\n",
        "\n",
        "Instead of a fully-connnected layer, the last layer in the discriminator is implemented as a 1D convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AdkVZDmn3OXU"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A four-layer discriminative CNN\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # three convolutional layers\n",
        "        self.conv1 = nn.Sequential(\n",
        "           nn.Conv2d(in_channels = 1, out_channels=32, kernel_size = 4, stride = 2 , padding = 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 32, out_channels=64, kernel_size = 4, stride = 2 , padding = 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 64, out_channels=128, kernel_size = 4, stride = 2 , padding = 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 128*4*4, out_channels = 1, kernel_size=1, stride = 1, padding = 0, bias =False ),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        # Flatten and apply sigmoid\n",
        "        x = x.view(-1, 128*4*4, 1, 1)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    # reset weights\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                m.reset_parameters()\n",
        "                m.weight.data.normal_(0.00, 0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vP-zCGS3OXW"
      },
      "source": [
        "We next define a helper function for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwt_SKe83OXW"
      },
      "outputs": [],
      "source": [
        "def numpy_to_plotly(arr):\n",
        "    tmp = (arr-arr.min())/(arr.max()-arr.min())*255\n",
        "    return np.dstack((tmp,tmp,tmp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpVSefzp3OXX"
      },
      "source": [
        "## Generator\n",
        "\n",
        "This class defines the generator, which takes noise as input and then generates a 32x32 pixel image from this.\n",
        "\n",
        "Note that in order to do this, we have to use Transposed Convolution, which takes a filter so as to upsample an input image.\n",
        "\n",
        "To keep the discriminator and generator symmetric, we use the same amount of filters when upsampling, as when downsampling in the discriminator.\n",
        "\n",
        "In addition, the generator also uses a 1D upsampling step, which generates an image-like filter input to the next layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2os3uvOl3OXY"
      },
      "outputs": [],
      "source": [
        "class GeneratorNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A four-layer generative CNN\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # this first layer converts the hidden layer to the initial \n",
        "        # image-like filter representation via 1D convolution\n",
        "        self.conv0 = nn.ConvTranspose2d(in_channels= 100,out_channels=128, kernel_size =1 , stride=2, padding=1, bias=False)\n",
        "        \n",
        "        # now upsample three times\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=128, out_channels=64, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels=64, out_channels=32, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "           nn.ConvTranspose2d(\n",
        "                in_channels=32, out_channels=1, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "        )\n",
        "        self.out = torch.nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project and reshape\n",
        "        x = x.view(x.shape[0], 100, 1, 1)\n",
        "        \n",
        "        # Convolutional layers\n",
        "        x = self.conv0(x)\n",
        "        x = x.view(x.shape[0], 128, 4, 4)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        # Apply Tanh\n",
        "        return self.out(x)\n",
        "    # reset weights\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "                m.reset_parameters()\n",
        "                m.weight.data.normal_(0.00, 0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rVFWFzs3OXZ"
      },
      "source": [
        "In order to create the noise distribution from which the generator is supposed to work, we create a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Xvcy6kDO3OXZ"
      },
      "outputs": [],
      "source": [
        "# Noise\n",
        "def noise(size):\n",
        "    n = torch.randn(size, 100)\n",
        "    if torch.cuda.is_available(): return n.cuda() \n",
        "    return n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLOggDte3OXa"
      },
      "source": [
        "## Training the GAN\n",
        "\n",
        "First we instantiate the two networks, and push them to CUDA, if possible."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modelsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq5D_i1p6QLE",
        "outputId": "a908cc26-0184-4858-b7ac-6ec18bd3dbdd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: modelsummary in /usr/local/lib/python3.7/dist-packages (1.1.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from modelsummary) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from modelsummary) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from modelsummary) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->modelsummary) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "qaiD39zz3OXa",
        "outputId": "2654b662-d08f-4acb-fae0-9a1808586d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------\n",
            "             Layer (type)                Input Shape         Param #\n",
            "=======================================================================\n",
            "                 Conv2d-1            [-1, 1, 32, 32]             512\n",
            "              LeakyReLU-2           [-1, 32, 16, 16]               0\n",
            "                 Conv2d-3           [-1, 32, 16, 16]          32,768\n",
            "            BatchNorm2d-4             [-1, 64, 8, 8]             128\n",
            "              LeakyReLU-5             [-1, 64, 8, 8]               0\n",
            "                 Conv2d-6             [-1, 64, 8, 8]         131,072\n",
            "            BatchNorm2d-7            [-1, 128, 4, 4]             256\n",
            "              LeakyReLU-8            [-1, 128, 4, 4]               0\n",
            "                 Conv2d-9           [-1, 2048, 1, 1]           2,048\n",
            "               Sigmoid-10              [-1, 1, 1, 1]               0\n",
            "=======================================================================\n",
            "Total params: 166,784\n",
            "Trainable params: 166,784\n",
            "Non-trainable params: 0\n",
            "-----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-348b8fee1b90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/modelsummary/modelsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, *inputs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# register hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-ddbd3e13aad9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    950\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    951\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given input size per channel: (1 x 1). Calculated output size per channel: (-1 x -1). Output size is too small"
          ]
        }
      ],
      "source": [
        "discriminator = DiscriminatorNet()\n",
        "generator = GeneratorNet()\n",
        "\n",
        "from modelsummary import summary\n",
        "summary(discriminator, torch.zeros((1, 1, 32, 32)), show_input=True)\n",
        "\n",
        "summary(generator, torch.zeros((1,100,1,1)), show_input=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FGEvi2Nn3OXb"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    discriminator.cuda()\n",
        "    generator.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6Lueile3OXb"
      },
      "source": [
        "Next, we set up the networks' optimizing schemes. As usual, we take Adam, but with a rather low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nea_wycp3OXc"
      },
      "outputs": [],
      "source": [
        "# Optimizers\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "E2lSETs53OXc"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "loss = nn.BCELoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YeODnp-3OXd"
      },
      "source": [
        "Now we will define two helper functions that create targets for real data ($y_n=1$) and fake data ($y_n=0$) for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wKU0WP6-3OXe"
      },
      "outputs": [],
      "source": [
        "def real_data_target(size):\n",
        "    '''\n",
        "    Tensor containing ones, with shape = size\n",
        "    '''\n",
        "    data = torch.ones(size, 1, 1, 1)\n",
        "    if torch.cuda.is_available(): return data.cuda()\n",
        "    return data\n",
        "\n",
        "def fake_data_target(size):\n",
        "    '''\n",
        "    Tensor containing zeros, with shape = size\n",
        "    '''\n",
        "    data = torch.zeros(size, 1, 1, 1)\n",
        "    if torch.cuda.is_available(): return data.cuda()\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9z3nhy3OXe"
      },
      "source": [
        "Here comes the real beef of the training. \n",
        "\n",
        "The `train_discriminator` function takes the real and fake labels and the optimizer and then first trains the discriminator on the real data, and then on the fake data, returning the total error of the two steps.\n",
        "\n",
        "The `train_generator` function does the same by taking the error of the discriminator for predicting fake data and pretending that to be real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dxmnh5G_3OXf"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(optimizer, real_data, fake_data):\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # 1.1 Train on Real Data\n",
        "    prediction_real = discriminator(real_data)\n",
        "    # Calculate error and backpropagate\n",
        "    # Error is measured against real data targets\n",
        "    error_real = loss(prediction_real, real_data_target(real_data.size(0)))\n",
        "    error_real.backward()\n",
        "\n",
        "    # 1.2 Train on Fake Data\n",
        "    prediction_fake = discriminator(fake_data)\n",
        "    # Calculate error and backpropagate\n",
        "    # Error is measured against fake targets\n",
        "    error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))\n",
        "    error_fake.backward()\n",
        "    \n",
        "    # 1.3 Update weights with gradients\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Return error\n",
        "    return error_real + error_fake, prediction_real, prediction_fake\n",
        "\n",
        "def train_generator(optimizer, fake_data):\n",
        "    # 2. Train Generator\n",
        "    # Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Sample noise and generate fake data\n",
        "    prediction = discriminator(fake_data)\n",
        "    # Calculate error and backpropagate\n",
        "    # Note that here the error is pretending to be real\n",
        "    error = loss(prediction, real_data_target(prediction.size(0)))\n",
        "    error.backward()\n",
        "    # Update weights with gradients\n",
        "    optimizer.step()\n",
        "    # Return error\n",
        "    return error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfPDAcJD3OXf"
      },
      "source": [
        "The core of the generator is the noise from which the images are supposed to be generated. Let's see what that looks like initially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFPcruyB3OXg"
      },
      "outputs": [],
      "source": [
        "num_test_samples = 50\n",
        "test_noise = noise(num_test_samples)\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
        "ax.imshow(test_noise.cpu())\n",
        "ax.set_xlabel('feature dimensions')\n",
        "ax.set_ylabel('number')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtxNoTX63OXh"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "from ipywidgets import interact\n",
        "fig = make_subplots(rows=4, cols=4)\n",
        "\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        fig.append_trace(go.Image(z=np.zeros((32,32,3))),row=i+1,col=j+1)\n",
        "fig = go.FigureWidget(fig)\n",
        "fig.update_xaxes(showticklabels=False)\n",
        "fig.update_yaxes(showticklabels=False)\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=800,\n",
        "    height=800,\n",
        ")\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3ZnJWVR3OXh"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(face_dataset, batch_size=100,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "discriminator.reset()\n",
        "generator.reset()\n",
        "\n",
        "# Number of epochs for training the GAN\n",
        "num_epochs = 50\n",
        "\n",
        "t_start = time.time()\n",
        "duration_avg = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for n_batch, real_data in enumerate(dataloader):\n",
        "        # 1. Train Discriminator with real images\n",
        "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
        "        # Generate fake data from noise, do not update\n",
        "        # gradients here - hence add \".detach()\"\n",
        "        noi = noise(real_data.size(0))\n",
        "        fake_data = generator(noi).detach()\n",
        "        # Train discriminator with the real and fake data\n",
        "        d_error, d_pred_real, d_pred_fake = \\\n",
        "                            train_discriminator(d_optimizer,                                    \n",
        "                            real_data.float(), fake_data)\n",
        "\n",
        "        # 2. Train Generator\n",
        "        # Generate fake data from noise\n",
        "        fake_data = generator(noi)#noise(real_batch.size(0)))\n",
        "        # Train G\n",
        "        g_error = train_generator(g_optimizer, fake_data)\n",
        "        # Display Progress\n",
        "        if (n_batch) % 50 == 0:\n",
        "            test_images = generator(test_noise).data.cpu()\n",
        "            p=0\n",
        "            with fig.batch_update():\n",
        "                for i in range(4):\n",
        "                    for j in range(4):\n",
        "                        tmp=test_images[p,0,:,:].numpy()\n",
        "                        fig.data[p]['z']=numpy_to_plotly(tmp)\n",
        "                        p+=1\n",
        "            fig.update_layout(\n",
        "                title={\n",
        "                    'text':'epoch {0:} batch {1:} L(D)={2: .3f} L(G)={3: .3f} t={4: .1f}s/it'.format(\n",
        "                        epoch,n_batch,d_error,g_error,duration_avg),\n",
        "                    'xanchor':'center',\n",
        "                    'x':0.5\n",
        "                })\n",
        "                \n",
        "    t_end = time.time()\n",
        "    duration_avg = (t_end - t_start) / (epoch + 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1e_-L1b3OXj"
      },
      "source": [
        "As we can see for a lower number of parameters, this network produces well-represented faces."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FCN 이랑 Convolution 차이 \"inductive bias\" \n",
        "FCN 은 보통 \n",
        "Convolution 은 share . local filter 들이 있다는 것에 FCN 에 차이가 있음"
      ],
      "metadata": {
        "id": "KDvYE5yA8Mrh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDgXnj-081F8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}