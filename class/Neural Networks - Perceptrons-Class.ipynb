{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Optimization - Neural Networks - The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Neural networks are excellent examples of (often non-linear) function optimization. \n",
    "\n",
    "Even though real neurons look like this:\n",
    "\n",
    "![Real Neuron](https://static.turbosquid.com/Preview/2014/12/03__09_02_06/all.jpg5254a144-6d9d-4a4f-b9b0-6fcbba4f64bcOriginal.jpg)\n",
    "\n",
    "and have highly complicated biochemical processes that control their firing given input in the many dendrites, people have long thought about how to approximate their function mathematically.\n",
    "\n",
    "\n",
    "\n",
    "### The artificial neuron\n",
    "\n",
    "We will briefly review some \"proper\" Nobel-prize-winning equations in the later part of the course, when we talk about partial derivative equations - however, as you may imagine, a real neuron's complexity is rather daunting, so for simulations, simpler models would be good as a start. \n",
    "\n",
    "Here's an extremely simplified model - a so-called artificial neuron which simply \n",
    "1. takes several inputs $x_i$\n",
    "2. sums them up as $\\sum x_i$\n",
    "3. pushes them through an activation function $f(\\sum x_i)$ and \n",
    "4. delivers one output $y=f(\\sum x_i)$ to downstream neurons.\n",
    "\n",
    "![Simplified neuron and model](https://cdn-images-1.medium.com/max/1200/1*SJPacPhP4KDEB1AdhOFy_Q.png)\n",
    "\n",
    "This model is based on the ground-breaking modeling work outlined in the paper by McCullough and Pitts from 1943. In this work, they used a step-function as $f$. \n",
    "\n",
    "McCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:115â€“133.\n",
    "\n",
    "### Activation functions \n",
    "\n",
    "The purpose of the activation function is to introduce more complicated processing of the inputs - we will talk about this more later. The most common choices are plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x,type_ = 'tanh'): \n",
    "    if type_=='tanh':\n",
    "        out = np.tanh(x)\n",
    "    else:\n",
    "        if 'logistic' == type_:\n",
    "            out = 1.0 / (1 + np.exp(- x))\n",
    "        else:\n",
    "            if 'relu' == type_:\n",
    "                out = np.maximum(np.zeros_like(x),x)\n",
    "            else:\n",
    "                if 'perceptron' == type_:\n",
    "                    out = ...\n",
    "                else:\n",
    "                    if 'linear' == type_:\n",
    "                        out = ...\n",
    "                    else:\n",
    "                        raise Exception(np.array(['do not know type ',type_]))\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-values\n",
    "x=np.arange(-2,2,0.02)\n",
    "\n",
    "# activation functions\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,activation(x,'linear'),label='linear');\n",
    "plt.plot(x,activation(x,'tanh'),label='tanh');\n",
    "plt.plot(x,activation(x,'logistic'),label='logistic');\n",
    "plt.plot(x,activation(x,'perceptron'),label='perceptron');\n",
    "plt.plot(x,activation(x,'relu'),label='relu');\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning networks - the perceptron\n",
    "\n",
    "Let's try to do something simple now. Let's assume that our data we would like to learn consists of pairs $\\vec{x}_i,y_i$, and that the $y_i$ can only take on **two values** ($y_i=-1,1$). \n",
    "\n",
    "Furthermore, we assume that the data has a **linear structure**. We can quantify this like so: \n",
    "\n",
    "There exists some $\\vec{w}^* \\in \\mathbb{R}^d$, such that $\\|\\vec{w}^*\\|=1$  and for\n",
    "some $\\alpha > 0$, for all $i$ $\\in {1, 2, \\dots , n}$\n",
    "it holds that:\n",
    "\n",
    "$$\n",
    "y_i(\\vec{w}^*\\vec{x}_i)>\\alpha\n",
    "$$\n",
    "\n",
    "Which, when you remember that $y_i=-1,1$, simply says that the sign of $(\\vec{w}^*\\vec{x}_i)$ is the same as the corresponding $y_i$.\n",
    "\n",
    "The constant $\\alpha$ is introduced as a lower bound on the value of $y_i(\\vec{w}^*\\vec{x}_i)$. \n",
    "\n",
    "Note, that if you find such a vector $\\vec{w}^*$, then all points that satisify\n",
    "\n",
    "$$\n",
    "(\\vec{w}^*\\vec{x})=0\n",
    "$$\n",
    "\n",
    "define a line (through the origin - if we augment our x-s with a leading \"1\" and add another leading $w_0$ to $\\vec{w}$, then we can also model the intercept or bias of that line)!\n",
    "\n",
    "\n",
    "Finally, we will assume that all input values $\\vec{x}_i$ are bounded from above (so the maximum distance from the origin for these points is some number $D$, such that for all $i$, $\\|\\vec{x}_i\\|<D$.\n",
    "\n",
    "How do we update the weights now? Let's write the algorithm created by Frank Rosenblatt in pseudo-code:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "set weights $\\vec{w}= ...$\n",
    "\n",
    "while any $y_i(\\vec{w}*\\vec{x}_i)\\leq 0$:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;choose one index $k$, for which $y_k(\\vec{w}*\\vec{x}_k)...$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;set weights to $\\vec{w} = \\vec{w} + ...\\vec{x}_k$\n",
    "\n",
    "</div>\n",
    "\n",
    "Let's write down the algorithm in Python - this is much longer than the pseudocode above, but only because of the plotting of the updated decision hyperplane that happens during the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPerceptron(x,y,maxIter,doPlot):\n",
    "    \n",
    "    # init weights\n",
    "    w = np.zeros((x.shape[1],1))+0.01\n",
    "    print(w)\n",
    "\n",
    "    # check wrong outputs\n",
    "    outputs = ...\n",
    "    print(outputs)\n",
    "    \n",
    "    ite=1\n",
    "\n",
    "    # plot data and initial guess\n",
    "    xs=np.arange(np.min(x[:,1]),np.max(x[:,1]))\n",
    "\n",
    "    # as long as there any misclassified points\n",
    "    # and we are within iteration limits, do:\n",
    "    while(np.sum(outputs)>0 & ite<=maxIter):\n",
    "        # get all misclassified points\n",
    "        ind=np.where(outputs>0)[0]\n",
    "\n",
    "        # update the weight with one misclassified point\n",
    "        update = y[ind[0]]*x[ind[0],:]\n",
    "        # necessary to switch row vector back to column vector\n",
    "        w = w + update[:,None]\n",
    "        print(w)\n",
    "        ite=ite+1\n",
    "        # and determine the new, wrong classification outputs\n",
    "        outputs = \n",
    "        \n",
    "        if (doPlot):\n",
    "            plt.figure(figsize=(10,6))\n",
    "            display.clear_output(wait=True)\n",
    "            indPos = y==1\n",
    "            indNeg = y==-1\n",
    "            plt.figure(figsize=(10,8))\n",
    "            plt.scatter(x[indPos.ravel(),1],x[indPos.ravel(),2])\n",
    "            plt.scatter(x[indNeg.ravel(),1],x[indNeg.ravel(),2])\n",
    "            # plotting the line from the weights\n",
    "            ys=...\n",
    "            plt.plot(xs,ys,'b-')\n",
    "            plt.xlim((-10,10))\n",
    "            plt.ylim((-10,10))\n",
    "            plt.title('{}: {} wrong\\n'.format(ite,np.sum(outputs)))\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            time.sleep(1)\n",
    "       \n",
    "    return(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this algorithm with two reasonably well-separated point clouds in two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "x = np.vstack((rng.standard_normal((20,2)),rng.standard_normal((20,2))+4))\n",
    "y = np.vstack((-1*np.ones((20,1)),np.ones((20,1))))\n",
    "x = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "\n",
    "w = myPerceptron(x,y,50,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see how the algorithm adjusts the line so that it tries to better capture the distribution of the data.\n",
    "\n",
    "The perceptron algorithm like this has two important properties:\n",
    "\n",
    "<div class=\"alert: alert-warning\">\n",
    "<p> 1. It immediately stops ...\n",
    "\n",
    "<p>2. It is ...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.vstack((rng.standard_normal((20,2)),rng.standard_normal((20,2))+1))\n",
    "y = np.vstack((-1*np.ones((20,1)),np.ones((20,1))))\n",
    "x = np.hstack((np.ones((x.shape[0],1)),x))\n",
    "\n",
    "w = myPerceptron(x,y,50,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can see that the algorithm (by definition) will not stop as it is not possible to find a line that splits the data.\n",
    "\n",
    "In this case, you will have to settle for the last decision-plane that the algorithm finds!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of convergence for perceptrons\n",
    "\n",
    "How can we prove that this algorithm does its job? \n",
    "\n",
    "We can see that if it finishes, the line will separate the classes. So, what we need to prove is that the algorithm converges in a limited amount of steps $k$.\n",
    "\n",
    "In detail: Assume that there exists some parameter vector $\\vec{w}^*$ such that $||\\vec{w}^*||=1$, and some $\\alpha > 0$ such that for all k = 1...n, $y_k(\\vec{x}\\cdot\\vec{w}^*) \\geq \\alpha$. \n",
    "\n",
    "The latter comes as we formulate the correct classifications for the Perceptron!\n",
    "\n",
    "#### Lower bound\n",
    "\n",
    "Let's try to find a lower bound for $k$ first. Let's take a misclassified point $i$ and update it:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}=\\vec{w}_k+y_i\\vec{x}_i\n",
    "$$\n",
    "\n",
    "we can multiply this by $\\vec{w}^*$ to get\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}\\vec{w}^*=\\vec{w}_k\\vec{w}^*+y_i\\vec{x}_i\\vec{w}^*\n",
    "$$\n",
    "\n",
    "but we required that the second term on the right - if we find a solution - is $>\\alpha$, so:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}\\vec{w}^*>\\vec{w}_k\\vec{w}^*+\\alpha\n",
    "$$\n",
    "\n",
    "Now, let's start the process with $\\vec{w}_0=\\vec{0}$, which is our initialization of the weights. This means the update becomes:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{1}\\vec{w}^*>\\alpha\n",
    "$$\n",
    "\n",
    "So, if we just did this $k$ times we therefore get:\n",
    "\n",
    "$$\n",
    "\\vec{w}_{k+1}\\vec{w}^*>k\\alpha\n",
    "$$\n",
    "\n",
    "And since $\\|\\vec{w}_{k+1}\\|\\|\\vec{w}^*\\|>\\vec{w}_{k+1}\\vec{w}^*$, we get:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|>k\\alpha\n",
    "$$\n",
    "\n",
    "\n",
    "#### Upper bound\n",
    "\n",
    "For the upper bound, let's write the norm of the update step:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|^2=\\|\\vec{w}_k+y_i\\vec{x}_i\\|^2=\\|\\vec{w}_k\\|^2+2y_i\\vec{w}_k\\vec{x}_i+\\|\\vec{x}_i\\|^2<\\|\\vec{w}_k\\|^2+\\|\\vec{x}_i\\|^2\n",
    "$$\n",
    "\n",
    "where we've used the fact that the $|y_i|=1$. But we required bounded points: $\\|\\vec{x}_i\\|<D$, so:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|^2<\\|\\vec{w}_k\\|^2+D^2\n",
    "$$\n",
    "\n",
    "Again, we start with $\\vec{w}_0=\\vec{0}$, so we get:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{1}\\|^2<D^2\n",
    "$$\n",
    "\n",
    "and doing that $k$ times, we get:\n",
    "\n",
    "$$\n",
    "\\|\\vec{w}_{k+1}\\|^2<kD^2\n",
    "$$\n",
    "\n",
    "\n",
    "#### Putting it together\n",
    "Now, we've got two results - an upper and a lower bound:\n",
    "\n",
    "$$\n",
    "k^2{\\alpha}^2<\\|\\vec{w}_{k+1}\\|^2<kD^2\n",
    "$$\n",
    "\n",
    "so we \"ignore\" the middle term and get:\n",
    "\n",
    "$$\n",
    "k<\\frac{D^2}{{\\alpha}^2}\n",
    "$$\n",
    "\n",
    "What that means is that the algorithm is **guaranteed** to converge in a maximum number of steps, provided the data is linearly separable (related to the constant $\\alpha$) and that it is bounded (related to the constant $D$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical functions with a perceptron\n",
    "\n",
    "The original paper by McCulloch and Pitts talked about using the neuronal model as a substitute for **logical** operations. So, let's try to following their reasoning.\n",
    "\n",
    "Let's say I want to build a neuron that receives two inputs $x_1,x_2$ - these inputs are logical values, true or false. I want the neuron to run a simple logical operation, so that, for example, its output $y$ will be equal to a target function, such as $t = x_1 \\text{ AND } x_2$. \n",
    "\n",
    "We can see that $t$ is a function with exactly two possible values as well. Given that the original perceptron was formulated as a two-class learner, this means we can try to apply this here as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA=np.array([[0],[0],[1],[1]])\n",
    "inputB=np.array([[0],[1],[0],[1]])\n",
    "\n",
    "targetLogical=inputA&inputB\n",
    "targetLogical[targetLogical==0]=-1\n",
    "\n",
    "print(targetLogical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack((np.ones((4,1)),inputA,inputB))\n",
    "\n",
    "myPerceptron(x,targetLogical,20,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So AND works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetLogical=inputA|inputB\n",
    "targetLogical[targetLogical==0]=-1\n",
    "print(targetLogical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack((np.ones((4,1)),inputA,inputB))\n",
    "\n",
    "myPerceptron(x,targetLogical,20,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So OR works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack((np.ones((4,1)),inputA,inputB))\n",
    "\n",
    "myPerceptron(x,targetLogical,20,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, XOR does not work. We can see why since it cannot be linearly separated. \n",
    "\n",
    "The fact that such a simple logical function could not be processed by the perceptron was known for a long time. \n",
    "\n",
    "However, given that you can get all possible values of two logical inputs with a suitable **chain** of AND and NOT (or with OR and NOT) operations, you can see that implementing this function would be possible if you simply string enough perceptrons together!\n",
    "\n",
    "1969 saw the publication of a book \"Perceptrons\" by Marvin Minsky and Seymour Papert. Often it is said that the XOR problem was popularized by this book, which attributed to a dramatic decline in the popularity of neural networks - the first so-called AI winter. This is, however, only partly true since the result for XOR only holds for ONE SINGLE perceptron (again, a network of perceptrons would be fully capable of doing an XOR operation) - what Minsky and Papert showed, instead, is some limitations about those networks of perceptrons that related mostly to their EFFICIENCY, and not to the fact that they cannot in principle compute something. \n",
    "\n",
    "Regardless, the first AI winter did happen and funding for neural network based research did decline dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptrons\n",
    "\n",
    "It was already mentioned above that it should be possible to put multiple perceptron units together to create an actual neural **network**.\n",
    "\n",
    "We will talk about this in the next lecture, when we derive a more general way to train neurons and neural networks.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
