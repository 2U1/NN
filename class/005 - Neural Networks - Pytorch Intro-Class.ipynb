{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import pandas\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import time\n",
    "from torchviz import make_dot\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick intro to PyTorch\n",
    "\n",
    "Based on [this excellent intro](https://github.com/jcjohnson/pytorch-examples/blob/master/README.md)\n",
    "\n",
    "[PyTorch](https://github.com/pytorch/pytorch) is a high-level API to facilitate the construction and optimization of computational graphs. These graphs basically encode certain kinds of (connected, recurrent, subsequent) operations on so-called Tensors. Tensors are an extension to Matrices and in our case can simply be seen as multi-dimensional arrays with some linear algebra sauce added.\n",
    "\n",
    "Importantly, we know that designing neural networks will require us to set up \n",
    "- an architecture (layers)\n",
    "- suitable activation functions (tanh, ReLU, etc)\n",
    "- a loss function (our task)\n",
    "\n",
    "Once we have done this, we need to differentiate the loss function by means of backpropagation to determine how each layer would update its weights given the error observed at the next layer downstream.\n",
    "\n",
    "To facilitate this, PyTorch provides two main features:\n",
    "- n-dimensional tensors that can run on **GPUs** (hence, fast!)\n",
    "- automatic differentiation for building and training computational graphs (i.e., neural networks)\n",
    "\n",
    "The following will use a fully-connected network with rectified linear units (ReLUs) as activation functions as the basic architecture. To make matters simple, we will only use one hidden layer (like in the previous example), and we will use gradient descent to fit some random data by minimizing the L2-distance between the network output and the desired  output.\n",
    "\n",
    "## Warm-up: numpy\n",
    "\n",
    "Let's quickly re-implement the 1-hidden-layer network from before using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create normal random input and output data\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = ...\n",
    "w2 = ...\n",
    "\n",
    "learning_rate = 1e-6\n",
    "max_iterations = 500\n",
    "l2_errors = np.zeros(max_iterations)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = ...\n",
    "    h_relu = ...\n",
    "    y_pred = ...\n",
    "    \n",
    "    # Compute and print loss\n",
    "    l2_errors[it] = ...\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = ...\n",
    "    grad_w2 = ...\n",
    "    grad_h_relu = ...\n",
    "    grad_h = ...\n",
    "    grad_h[h < 0] = ...\n",
    "    grad_w1 = ...\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "total_time = time.time()-start\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(l2_errors)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Summed Squared Error')\n",
    "plt.title('numpy network: {:0.6f}s'.format(total_time))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Tensors\n",
    "\n",
    "Now, let's convert the code to make use of PyTorch tensors that can calculate on the GPU. For this, we make use of the `device` argument when constructing a tensor to either put it on the CPU (default) or on a specific GPU.\n",
    "\n",
    "Let's reimplement the network using PyTorch commands - for most commands, this simply means to substitute ```numpy``` by ```torch``` and add the device!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if (torch.has_cuda): # do this for automatic switching\n",
    "\n",
    "doGPU=True\n",
    "if (doGPU):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "start=time.time()\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10000, 10\n",
    "\n",
    "# Create normal random input and output data\n",
    "# note, we simply substitute numpy by torch and add the device!\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-9\n",
    "max_iterations = 500\n",
    "l2_errors = np.zeros(max_iterations)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    # Forward pass: compute predicted y\n",
    "    # the dot-product here is called matrix multiplication \"mm\"\n",
    "    h = ...\n",
    "    # relu \n",
    "    h_relu = ...\n",
    "    y_pred = ...\n",
    "\n",
    "    # Compute and print loss; the loss is a scalar, \n",
    "    # and is stored in a PyTorch Tensor\n",
    "    # of shape (); we can get its value as a Python \n",
    "    # number with loss.item().\n",
    "    loss = ...\n",
    "    l2_errors[it]=...\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = ...\n",
    "    grad_w2 = ...\n",
    "    grad_h_relu = ...\n",
    "    grad_h = ...\n",
    "    grad_h[h < 0] = ...\n",
    "    grad_w1 = ...\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "total_time = time.time()-start\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(l2_errors)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Summed Squared Error')\n",
    "plt.title('PyTorch {0:} network: {1: 0.6f}s'.format(device,total_time))\n",
    "plt.grid()\n",
    "\n",
    "del w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for a \"small\" network, numpy (or the CPU version of PyTorch) is comparable in terms of execution time to the GPU-version. This is because there is an overhead in sending the information to and from the GPU - this overhead, however, is completely negligible once larger networks (and with this, larger tensors) have to be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: Autograd\n",
    "\n",
    "Let's come to the second, big advantage of having a high-level API like PyTorch work for us: [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). We can use this to do the backpropagation of the error through the layers for us!\n",
    "\n",
    "The **autograd** package in PyTorch provides this functionality, and with that will translate the layer architecture of the network and the loss function into a\n",
    "**computational graph**. Nodes in the graph will be tensors, and edges will be functions that produce output Tensors from input Tensors. Once the graph has been formulated properly, automatic differentation can be used to derive the gradients for backpropagation.\n",
    "\n",
    "If you therefore want to implement this using PyTorch, you specify `requires_grad=True`\n",
    " during the construction of the tensor. Then, as soon as you apply any **PyTorch operations** on that Tensor the\n",
    "computational graph will be built, allowing us to do backpropagation. If `x` is a Tensor with `requires_grad=True`, then after\n",
    "backpropagation another tensor `x.grad` will be added to it which contains the gradient of `x`.\n",
    "\n",
    "Not all operations require the construction of the graph, however, for example weight updates during\n",
    "training do **not** need to be derived even though they of course are PyTorch operations working on tensors! For this, specify `torch.no_grad()` for these operations to prevent the construction of a computational graph.\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if (torch.has_cuda): # do this for automatic switching\n",
    "\n",
    "doGPU=True\n",
    "if (doGPU):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "start=time.time()\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10000, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; \n",
    "# setting requires_grad=True means that we\n",
    "# want to compute gradients for these \n",
    "# tensors during the backward pass.\n",
    "# -- we of course do NOT want to calculate gradients\n",
    "# for our data input and output tensors x,y!!\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-9\n",
    "max_iterations = 500\n",
    "l2_errors = np.zeros(max_iterations)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    y_pred =...\n",
    "\n",
    "    # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "    # is a Python number giving its value.\n",
    "    loss = ...\n",
    "    l2_errors[it] = ...\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    ...\n",
    "\n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    with ...:\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "\n",
    "total_time = time.time()-start\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(l2_errors)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Summed Squared Error')\n",
    "plt.title('PyTorch {0:} network: {1: 0.6f}s'.format(device,total_time))\n",
    "plt.grid()\n",
    "\n",
    "del w1, w2, y_pred, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that there is a small overhead in terms of performance, when using the computational graph structure, as PyTorch needs to keep track of the gradients through the autograd functionality.\n",
    "\n",
    "Since autograd passes basically require a forward and backward step, it is of course possible to code your own versions of these and use them instead - here, we are not going to do this, since we want to have it more convenient at the moment. We want PyTorch to do even more heavy lifting for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: nn\n",
    "Computational graphs and autograd are a very powerful paradigm for defining\n",
    "complex operators and automatically taking derivatives; however for large\n",
    "neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation\n",
    "into **layers**, some of which have **learnable parameters** which will be\n",
    "optimized during learning.\n",
    "\n",
    "In PyTorch, the `nn` package defines a set of\n",
    "**Modules**, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as\n",
    "Tensors containing learnable parameters. The `nn` package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
    "\n",
    "In this example we use the `nn` package to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if (torch.has_cuda): # do this for automatic switching\n",
    "\n",
    "doGPU=True\n",
    "if (doGPU):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "start=time.time()\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10000, 10\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "# After constructing the model we use the .to() method to move it to the\n",
    "# desired device.\n",
    "model = ...\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        size = m.weight.size() # returns a tuple\n",
    "        fan_out = size[0] # number of rows\n",
    "        fan_in = size[1] # number of columns\n",
    "        m.weight.data = torch.randn(fan_out, fan_in, device=device)\n",
    "\n",
    "model...\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function. Setting\n",
    "# reduction='sum' means that we are computing the *sum* of squared errors rather\n",
    "# than the mean; this is for consistency with the examples above where we\n",
    "# manually compute the loss, but in practice it is more common to use mean\n",
    "# squared error as a loss by setting reduction='elementwise_mean'.\n",
    "loss_fn = torch...\n",
    "\n",
    "learning_rate = 1e-9\n",
    "max_iterations = 500\n",
    "l2_errors = np.zeros(max_iterations)\n",
    "\n",
    "for it in range(max_iterations):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = ...\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the loss.\n",
    "    loss = ...\n",
    "    l2_errors[it] = ...\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    ...\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    ...\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    with ...:\n",
    "        for ...\n",
    "\n",
    "total_time = time.time()-start\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(l2_errors)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Summed Squared Error')\n",
    "plt.title('PyTorch {0:} network: {1: 0.6f}s'.format(device,total_time))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that once we have defined a model via the pytorch abstracted layers and have therefore constructed a computational graph, we can easily \"print\" a summary of the model via the overloaded print command in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the torchviz helper package, we can get a deeper visualization of the full computational graph like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512, 1000).requires_grad_(True).to(device)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"torchviz\", format=\"png\")\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(10,20))\n",
    "img = mpimg.imread('torchviz.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch: optim\n",
    "Up to this point we have updated the weights of our models by manually mutating\n",
    "Tensors holding learnable parameters. This is not a huge burden\n",
    "for simple optimization algorithms like stochastic gradient descent, but in practice\n",
    "we often train neural networks using more sophisiticated optimizers like AdaGrad,\n",
    "RMSProp, Adam, etc.\n",
    "\n",
    "The `optim` package in PyTorch abstracts the idea of an optimization algorithm and\n",
    "provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "In this example we will use the `nn` package to define our model as before, but we\n",
    "will optimize the model using the Adam algorithm provided by the `optim` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only done to start fresh and to avoid issues with GPU and pytorch\n",
    "del model \n",
    "\n",
    "doGPU=True\n",
    "if (doGPU):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "start=time.time()\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 10000, 10\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = ...\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        size = m.weight.size() # returns a tuple\n",
    "        fan_out = size[0] # number of rows\n",
    "        fan_in = size[1] # number of columns\n",
    "        m.weight.data = torch.randn(fan_out, fan_in, device=device)\n",
    "\n",
    "model....\n",
    "\n",
    "loss_fn = ...\n",
    "\n",
    "max_iterations = 500\n",
    "l2_errors = np.zeros(max_iterations)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = ...\n",
    "for it in range(max_iterations):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = ...\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = ...\n",
    "    l2_errors[it] = ...\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the Tensors it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    ...\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "    ...\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    ...\n",
    "    \n",
    "total_time = time.time()-start\n",
    "%matplotlib notebook\n",
    "fig,ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(l2_errors)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Summed Squared Error')\n",
    "plt.title('PyTorch {0:} network: {1: 0.6f}s'.format(device,total_time))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
