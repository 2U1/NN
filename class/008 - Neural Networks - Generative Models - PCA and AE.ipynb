{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision as tv\n",
    "from torchvision import transforms, utils\n",
    "import glob\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Let's first briefly implement the Principal Component Analysis to illustrate how this works.\n",
    "\n",
    "We are going to take the MNIST dataset again as our guinea pig as this will offer us some labels that we can use to illustrate the potential of PCA to form clusters in a lower-dimensional space.\n",
    "\n",
    "First, we are going to load MNIST via torchvision, and transform the data back to numpy arrays.\n",
    "\n",
    "For working with these for PCA we have to compress the 28x28 pixels images into vectors of 784 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-10d6a8e38910>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-10d6a8e38910>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    data[1].detach().cpu().numpy()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = tv.datasets.MNIST(root='./',  train=True,download=True,transform=transform)\n",
    "dataloaderSorted = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=60000, shuffle=False, num_workers=4)\n",
    "\n",
    "for n_batch, data in enumerate(dataloaderSorted):\n",
    "    img, labels = data[0].detach().cpu().numpy(), data[1].detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "# get rid of 1D channel dimension\n",
    "img = np.squeeze(img)\n",
    "\n",
    "# reshape to desired vectors\n",
    "img = np.reshape(img,(60000,28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA requires input to be zero-centered and with a variance of 1. We are going to use sklearn for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_img = StandardScaler().fit_transform(img)\n",
    "print(std_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to get the covariance matrix of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of variance matrix =  (784, 784)\n"
     ]
    }
   ],
   "source": [
    "#find the co-variance matrix which is : A^T * A\n",
    "sample_data = std_img\n",
    "\n",
    "# matrix multiplication using numpy\n",
    "covar_matrix = np.matmul(sample_data.T , sample_data)\n",
    "\n",
    "print ( \"The shape of variance matrix = \", covar_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we do PCA on the covariance matrix. We are not going to be interested in the actual PCA matrices that are created here, but rather only will take the Eigenvectors and Eigenvalues of the matrix, which we know to be the solutions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of eigen vectors =  (784, 2)\n",
      "Updated shape of eigen vectors =  (2, 784)\n"
     ]
    }
   ],
   "source": [
    "# finding the top two eigen-values and corresponding eigen-vectors \n",
    "# for projecting onto a 2D space.\n",
    "# for this we use eigh\n",
    "\n",
    "from scipy.linalg import eigh \n",
    "\n",
    "# the parameter 'eigvals' is defined (sorted low to high value) \n",
    "# eigh function will return the eigen values in ascending order\n",
    "# this code generates only the top 2 (782 and 783) (index) eigenvalues.\n",
    "values, vectors = eigh(covar_matrix, eigvals=(782,783))\n",
    "\n",
    "print(\"Shape of eigen vectors = \",vectors.shape)\n",
    "# converting the eigen vectors into (2D) shape\n",
    "vectors = vectors.T\n",
    "\n",
    "print(\"Updated shape of eigen vectors = \",vectors.shape)\n",
    "# here the vectors[1] represent the eigen vector corresponding to 1st principal eigen vector\n",
    "# here the vectors[0] represent the eigen vector corresponding to 2nd principal eigen vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the eigenvectors look like? We can simply plot them as images like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5f6526f370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(vectors[0,:].reshape(28,28))\n",
    "ax2.imshow(vectors[1,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the projections of our data in the new coordinate system defined by the two principal eigenvectors.\n",
    "\n",
    "For this, we simply do the projection of Eigenvectors onto the transpose of the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projecting the original data sample on the plane \n",
    "# formed by two principal eigen vectors by vector-vector multiplication.\n",
    "\n",
    "new_coordinates = np.matmul(vectors, sample_data.T)\n",
    "\n",
    "print (\"projected data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data, we are going through it, digit-label by digit-label to create a 2D representation of the embedding of our data.\n",
    "\n",
    "Note that we will need to do this by asking about the indices of each digit class, as the MNIST training set does not have equal numbers of examples in each digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "# go through each label\n",
    "for l in np.arange(10):\n",
    "    # find the indices\n",
    "    inds = labels==l\n",
    "    # select the correct vectors\n",
    "    tmp = new_coordinates[:,inds]\n",
    "    \n",
    "    # and scatter giving the label as the digit string\n",
    "    plt.scatter(tmp[0,:],tmp[1,:],label=str(l))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we see some separation of digits, the individual clusters are not yet well defined. \n",
    "\n",
    "So, next, let's take a look at the non-linear dimensionality embedding methods offered via Autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "Autoencoders constitute perhaps the simplest class of generative models. \n",
    "\n",
    "To recap, in a generative model we learn the joint probability distribution of the input data and labels simultaneously, i.e. $P(x,y)$. This can be converted to $P(y|x)$ for classification via Bayes' rule, of course. Importantly, however, we can use $P(x,y)$ to create new samples $(x,y)$ that have highlikelihood.\n",
    "\n",
    "So, how do we go about this from scratch? \n",
    "\n",
    "We have already seen encoders in our discussions of CNNs and fully-connected models for classification. They are simply taking data - let's say in image - and mapping it via a series of filters/feature detectors down to a (very) low dimensional label space (the output neurons).\n",
    "\n",
    "We have also touched already on the idea of bottlenecks in our discussion of 1x1 convolutional filters for CNNs - the idea here was that we force the information through a lower-dimensional gate by weighted downsampling, followed by upsampling again.\n",
    "\n",
    "Autoencoders are the natural extension of this idea with a low-dimensional bottleneck - the so-called latent space - inside an architecture that does: **Encoder (down-scaling) - bottleneck (latent space) - Decoder (up-scaling)**.\n",
    "\n",
    "So, let's try this first with standard fully-connected layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders with fully-connected layers:\n",
    "\n",
    "We need an Encoder - Bottleneck - Decoder architecture using fully-connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_FC(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(AE_FC,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, latent_dim),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.decoder = nn.Sequential( \n",
    "            nn.Linear(latent_dim, 28*28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    # we define this function so we can access only the\n",
    "    # latent space of the model\n",
    "    def getlatent(self,x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders on MNIST\n",
    "\n",
    "We are going to apply the autoencoders on the MNIST. The reason for this is that MNIST has 10 different classes of digits, which will allow us later to try to visualize what is going on in that latent (bottleneck) space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = tv.datasets.MNIST(root='./',  train=True,download=True, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=1)\n",
    "testset = tv.datasets.MNIST(root='./', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick helper function to combine 20 images of original and predicted pictures into one picture for display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertBatchToCombined(inp,out):\n",
    "    # this should be extended to arbitrary batch length\n",
    "    # assuming this to be 10 here\n",
    "    combinedUp = np.zeros((28,28*10))\n",
    "    combinedDown = np.zeros((28,28*10))\n",
    "    for b in np.arange(10):\n",
    "        combinedUp[:,b*28:b*28+28]=np.reshape(inp[:,b],(28,28))\n",
    "        combinedDown[:,b*28:b*28+28]=np.reshape(out[:,b],(28,28))\n",
    "    tmp = np.vstack([combinedUp,combinedDown])\n",
    "    tmp = (tmp-tmp.min())/(tmp.max()-tmp.min())*255.\n",
    "    return(np.repeat(tmp[:,:,np.newaxis], 3, axis=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our friendly real-time update function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.append_trace(go.Image(z=np.zeros((28,28*10,3))),row=1,col=1)\n",
    "fig = go.FigureWidget(fig)\n",
    "fig.update_xaxes(showticklabels=False)\n",
    "fig.update_yaxes(showticklabels=False)\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=800,\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this autoencoder with a latent dimensionality of d=10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "model = AE_FC(10)\n",
    "\n",
    "# put to device\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('cpu'))\n",
    "\n",
    "model.to(device=device)\n",
    "\n",
    "# use MSE loss\n",
    "distance = nn.MSELoss()\n",
    "\n",
    "# standard ADAM\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# pre-load the test pictures for visualization of online prediction\n",
    "timg, tlabels = list(dataloader)[0]\n",
    "timg = timg.to(device)\n",
    "timg = timg.view(timg.size(0), -1)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, data in enumerate(dataloader):\n",
    "        # we don't need the labels here\n",
    "        img, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # flatten\n",
    "        img = img.view(img.size(0), -1)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss_mse = distance(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss_mse.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # visualize output\n",
    "        if (n_batch) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                output = model(timg)\n",
    "                inp = timg[0:10, :].detach().cpu()\n",
    "                out = output[0:10, :].detach().cpu()\n",
    "\n",
    "                combined = convertBatchToCombined(inp.permute(1,0),out.permute(1,0))\n",
    "                \n",
    "                with fig.batch_update():\n",
    "                    fig.data[0]['z'] = combined\n",
    "                    fig.update_layout(\n",
    "                        title={\n",
    "                            'text':'epoch {0:} batch {1:} L={2: .3f}'.format(\n",
    "                                epoch+1,n_batch,loss_mse.item()),\n",
    "                            'xanchor':'center',\n",
    "                            'x':0.5\n",
    "                        })\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss_mse.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the reconstructions of the digits start out noisy and then converge to a somewhat blurred representation. Our autoencoder is a bit \"shallow\", however, with only one hidden layer that does the heavy lifting of discovering features.\n",
    "\n",
    "How about going \"deeper\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_FC_deep(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim):\n",
    "        super(AE_FC_deep,self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512,128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128,latent_dim),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.decoder = nn.Sequential( \n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128,512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 28*28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    # we define this function so we can access only the\n",
    "    # latent space of the model\n",
    "    def getlatent(self,x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "model = AE_FC_deep(10)\n",
    "\n",
    "# put to device\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('cpu'))\n",
    "\n",
    "model.to(device=device)\n",
    "\n",
    "# use MSE loss\n",
    "distance = nn.MSELoss()\n",
    "\n",
    "# standard ADAM\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# pre-load the test pictures for visualization of online prediction\n",
    "timg, tlabels = list(dataloader)[0]\n",
    "timg = timg.to(device)\n",
    "timg = timg.view(timg.size(0), -1)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, data in enumerate(dataloader):\n",
    "        # we don't need the labels here\n",
    "        img, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # flatten\n",
    "        img = img.view(img.size(0), -1)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss_mse = distance(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss_mse.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # visualize output\n",
    "        if (n_batch) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                output = model(timg)\n",
    "                inp = timg[0:10, :].detach().cpu()\n",
    "                out = output[0:10, :].detach().cpu()\n",
    "\n",
    "                combined = convertBatchToCombined(inp.permute(1,0),out.permute(1,0))\n",
    "                \n",
    "                with fig.batch_update():\n",
    "                    fig.data[0]['z'] = combined\n",
    "                    fig.update_layout(\n",
    "                        title={\n",
    "                            'text':'epoch {0:} batch {1:} L={2: .3f}'.format(\n",
    "                                epoch+1,n_batch,loss_mse.item()),\n",
    "                            'xanchor':'center',\n",
    "                            'x':0.5\n",
    "                        })\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss_mse.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that works better, resulting in sharper, more well-defined reconstructions of the digits. \n",
    "\n",
    "We were talking about the latent space, however - how can we visualize this nicely? \n",
    "\n",
    "How about we force the latent, bottleneck layer to be two-dimensional, so that we can visualize all digits in this two-dimensional space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# let's make things two-dimensional!\n",
    "model = AE_FC_deep(2)\n",
    "\n",
    "# put to device\n",
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('cpu'))\n",
    "\n",
    "model.to(device=device)\n",
    "\n",
    "# use MSE loss\n",
    "distance = nn.MSELoss()\n",
    "\n",
    "# standard ADAM\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# pre-load the test pictures for visualization of online prediction\n",
    "timg, tlabels = list(dataloader)[0]\n",
    "timg = timg.to(device)\n",
    "timg = timg.view(timg.size(0), -1)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, data in enumerate(dataloader):\n",
    "        # we don't need the labels here\n",
    "        img, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # flatten\n",
    "        img = img.view(img.size(0), -1)\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss_mse = distance(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss_mse.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # visualize output\n",
    "        if (n_batch) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                output = model(timg)\n",
    "                inp = timg[0:10, :].detach().cpu()\n",
    "                out = output[0:10, :].detach().cpu()\n",
    "\n",
    "                combined = convertBatchToCombined(inp.permute(1,0),out.permute(1,0))\n",
    "                \n",
    "                with fig.batch_update():\n",
    "                    fig.data[0]['z'] = combined\n",
    "                    fig.update_layout(\n",
    "                        title={\n",
    "                            'text':'epoch {0:} batch {1:} L={2: .3f}'.format(\n",
    "                                epoch+1,n_batch,loss_mse.item()),\n",
    "                            'xanchor':'center',\n",
    "                            'x':0.5\n",
    "                        })\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss_mse.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a two-dimensional latent space, let's poll it with our full dataset of 60000 digits to see what it looks like.\n",
    "\n",
    "For this, we first create a non-shuffled `DataLoader` that simply loads **all** MNIST training digits into memory - including the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaderSorted = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=False, num_workers=4)\n",
    "\n",
    "for n_batch, data in enumerate(dataloaderSorted):\n",
    "    img, labels = data[0].to(device), data[1].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data, we are going through it, digit-label by digit-label to create a 2D representation of the embedding of our data.\n",
    "\n",
    "Note that we will need to do this by asking about the indices of each digit class, as the MNIST training set does not have equal numbers of examples in each digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# don't forget to do this\n",
    "model.eval()\n",
    "\n",
    "# go through each label\n",
    "for l in np.arange(10):\n",
    "    # find the indices\n",
    "    inds = labels==l\n",
    "    # select the images\n",
    "    tmp = img[inds,:,:,:]\n",
    "    # flatten them\n",
    "    tmp = tmp.view(tmp.size(0), -1)\n",
    "    # put them through the model to get their 2D latent\n",
    "    # space representation\n",
    "    output = model.getlatent(tmp).detach().cpu().numpy()\n",
    "    \n",
    "    # and scatter giving the label as the digit string\n",
    "    plt.scatter(output[:,0],output[:,1],label=str(l))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that worked out \"fine\" to some degree - we do not see clusters of digits, but rather \"rays\" of digits, with some degree of clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
