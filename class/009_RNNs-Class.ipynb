{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQio_c0uPA8e"
   },
   "source": [
    "# Recurrent Networks for character processing\n",
    "\n",
    "In this notebook, we will be dealing with a vanilla implementation of a recurrent neural network based on Andrej Karpathy's simple \"gist\" code.\n",
    "\n",
    "This code does not use pytorch but was rather intended to give you the absolute basic implementation of a RNN.\n",
    "\n",
    "The idea of the code was to implement an RNN that is able to read a file of text and to learn the probabilities of characters following other characters from that corpus - a typical natural language processing (NLP) application with a long history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bokE2pG_PHL_"
   },
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5guw4joPA8q",
    "outputId": "7412bd42-9ab2-49d6-88d7-212f9b6494c0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# this reads the data (corpus), builds a vocabulary and \n",
    "# dictionary to index the characters of the corpus\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        # uncomment below and comment the next two lines, \n",
    "        # if you don't want to use an input file\n",
    "        # self.data = \"In this notebook, we will be dealing with a vanilla implementation of a recurrent neural network based on Andrej Karpathy's simple gist code. This code does not use pytorch but was rather intended to give you the absolute basic implementation of a RNN. The idea of the code was to implement an RNN that is able to read a file of text and to learn the probabilities of characters following other characters from that corpus - a typical natural language processing (NLP) application with a long history.\"\n",
    "        # read the file\n",
    "        #self.fp = open(path, \"r\")\n",
    "        #self.data = self.fp.read()\n",
    "        self.data = \"hello i want to learn\"\n",
    "        # make a list of all unique characters in the file\n",
    "        chars = list(set(self.data))\n",
    "        # create dictionaries that map each character to where\n",
    "        # they appear in the corpus and vice versa\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        # length of dataset\n",
    "        self.data_size = len(self.data)\n",
    "        # length of vocabulary (unique characters)\n",
    "        self.vocab_size = len(chars)\n",
    "        # current processing position\n",
    "        self.pointer = 0\n",
    "        # processing window\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        # for our current window\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        # take all characters in that window\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        # ... and try to predict the NEXT character\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        # go to next window\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barebone RNN class\n",
    "Again, just using numpy calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i3MRUAGwdAGe"
   },
   "outputs": [],
   "source": [
    "# bare-bones implementation of RNN\n",
    "class RNN:\n",
    "    # inputs: size of hidden layer, size of vocabulary,\n",
    "    # window size, learning rate for gradient descent\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # initialization of model weight matrices\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # the original implementation from Andrej uses\n",
    "        # ADAGRAD, which tracks the weight changes and\n",
    "        # makes use of these additional matrices\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    # standard softmax implementation for probability output     \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "    \n",
    "    # forward pass through the RNN \n",
    "    # given the previous state\n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = np.zeros((self.vocab_size,1))\n",
    "                # one hot encoding, 1-of-k\n",
    "                xs[t][inputs[t]] = 1\n",
    "                # hidden state of the RNN\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b)\n",
    "                # non-normalized log probalities for the next characters\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c\n",
    "                # probabilities for the next character\n",
    "                ycap[t] = self.softmax(os[t])\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    # backprop through the network given all states and targets    \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            # go backwards\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                # through softmax\n",
    "                # backprop into y [BCE loss]\n",
    "                dy[targets[t]] -= 1\n",
    "                # calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                # dh includes gradient from two sides, \n",
    "                # the next cell and the current output\n",
    "                # backprop into h\n",
    "                dh = np.dot(self.V.T, dy) + dhnext \n",
    "                # backprop through tanh non-linearity\n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh\n",
    "                # bias change\n",
    "                db += dhrec\n",
    "                # calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                # pass the gradient from next cell \n",
    "                # to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to avoid *exploding* gradients\n",
    "            # note that this does NOT avoid vanishing\n",
    "            # gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    # defines loss for a window of characters\n",
    "    def loss(self, ps, targets):\n",
    "            # cross-entropy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    # parameter update using ADAGRAD\n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # take all parameters\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            # apply memory\n",
    "            mem += dparam*dparam\n",
    "            # update parameters dampened by memory\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "    # this function samples from the model\n",
    "    # inputs are the hidden memory state and a seed character,\n",
    "    # and the number of samples to be made\n",
    "    def sample(self, h, seed_ix, n):\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            # one-hot encoding\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                # go through the model and get the probabilities\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                # choose among the characters\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                # set to chosen character and append\n",
    "                x = np.zeros((self.vocab_size,1))\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "        \n",
    "    # train loop\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            threshold = 0.01\n",
    "            # set loss to random predictor\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "            while (smooth_loss > threshold):\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                # forward, backward, loss, update\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    # sample 200 characters from the model\n",
    "                    # given the current first batch character as seed\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                iter_num += 1\n",
    "                \n",
    "    # predict a set of characters\n",
    "    def predict(self, data_reader, start, n):\n",
    "        # initialize input vector\n",
    "        x = np.zeros((self.vocab_size,1))\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = np.zeros((self.vocab_size,1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GGa0XI3MPA8r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size of 11\n"
     ]
    }
   ],
   "source": [
    "seq_length = 15\n",
    "# read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"t8.shakespeare.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,\n",
    "          seq_length=seq_length,learning_rate=1e-1)\n",
    "print(\"vocabulary size of\",rnn.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GGa0XI3MPA8r",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lwollro trehohll o   nwtnloo i  wnnlao tw iirwant o  innwnnaoeo oo wanealteoho wannaooantr rloe tail i wantni iioaewnitloo r nwaitni  r i awnnlor  htnntntoi o hwnt oa wawrlle eaa iinanl   o iinweall  \n",
      "\n",
      "\n",
      "iter :0, loss:35.968673\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :500, loss:22.443530\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :1000, loss:13.622876\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to wanr to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :1500, loss:8.268312\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :2000, loss:5.019044\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :2500, loss:3.047520\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :3000, loss:1.851268\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :3500, loss:1.125364\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to wait to want to \n",
      "\n",
      "\n",
      "iter :4000, loss:0.684811\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :4500, loss:0.417380\n",
      "want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to want to \n",
      "\n",
      "\n",
      "iter :5000, loss:0.254986\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_reader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mRNN.train\u001b[0;34m(self, data_reader)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# forward, backward, loss, update\u001b[39;00m\n\u001b[1;32m    133\u001b[0m xs, hs, ps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(inputs, hprev)\n\u001b[0;32m--> 134\u001b[0m dU, dW, dV, db, dc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(ps, targets)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_model(dU, dW, dV, db, dc)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mRNN.backward\u001b[0;34m(self, xs, hs, ps, targets)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# go backwards\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length)):\n\u001b[0;32m---> 57\u001b[0m     dy \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# through softmax\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# backprop into y [BCE loss]\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     dy[targets[t]] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:804\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(a, order, subok)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_copy_dispatcher)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(a, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m    Return an array copy of the given object.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m \n\u001b[1;32m    803\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.train(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
