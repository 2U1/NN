{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT0S5jTmynFU",
        "outputId": "5b86b68a-64dc-4ade-a6ed-f7bebfb791dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f607f61f310>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vk3ZiOcynFZ"
      },
      "source": [
        "# Classifying objects\n",
        "\n",
        "In the following, we will use the old-school CIFAR-10 dataset that contains low-resolution pictures of objects of 10 categories. This - and other - dataset is available as part of the `torchvision` package, which you should install."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1-IfMh7TynFc"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f5220d8de32b4201bdcecb0a9b700a54",
            "8cca6e3c839e48f395ad7e72f60cc77e",
            "312782531b4b4394a5c16a2e317348de",
            "bd45b523924544e7ac1f5491f4253d06",
            "1634931e860540dcbe8f7a62a1eaccb6",
            "e1767402b20241c987dab26907ab1adb",
            "102292e4ae1d47a6a020e9954ffcb82e",
            "b265cad13b724f50b99be913e2d767a8",
            "5504cd6f70024482ab73f370276e017f",
            "4ca64e0b99b74dddbb9badc5e0e1d76e",
            "4073f565ff8f48bfa25785e7b1a48550"
          ]
        },
        "id": "p8L2ueaEynFd",
        "outputId": "15cc033e-d726-4284-ae3f-a4a2efa64617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5220d8de32b4201bdcecb0a9b700a54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms, utils\n",
        "# where to put the data\n",
        "data_path = './'\n",
        "\n",
        "# this constructs a CIFAR10 dataset, selects the training part of it, \n",
        "# downloads it if necessary, and adds additional transforms that we \n",
        "# will need to convert each image to a pytorch tensor AND to provide\n",
        "# a nice conversion of the RGB images into greyscale\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zkt4rexynFe"
      },
      "source": [
        "This has now downloaded the images if not already done so - you will notice that this is quite a \"hefty\" dataset already at 170MB. Now let's download our validation set or test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuDKREFwynFe",
        "outputId": "a56e9f61-9757-42f0-cbf4-2b652493f8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMIsB4KPynFf"
      },
      "source": [
        "## Two-class problem on the GPU\n",
        "\n",
        "One of the reasons for the success of CNNs has been the realization that computation can be done efficiently on GPUs - originally designed to help with calculation of 3D graphics. \n",
        "\n",
        "So, let's push everything onto the GPU now. Let's redefine the same two-class problem as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n46EQQp1ynFg"
      },
      "outputs": [],
      "source": [
        "label_map = {0: 0, 2: 1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4eCgRwsynFi"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QAahFVJYynFj"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0C3UtsjynFl"
      },
      "source": [
        "Now, let's define a device that consists of CUDA (Compute Unified Device Architecture - a standard for general-purpose computing on GPU devices first introduced by NVIDIA in 2007) if supported, or the CPU otherwise.\n",
        "\n",
        "It is considered good standard to put code similar to this at the beginning of any script so that your code will run no matter if a GPU is present or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9lxilbyynFl",
        "outputId": "0a3b2cf0-68d7-43e9-b3ed-9fe8c36a72a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ]
        }
      ],
      "source": [
        "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "print(f\"Training on device {device}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_ypwhUynFm"
      },
      "source": [
        "We now have to modify our training loop slightly - the only difference is that we will need to tell Pytorch explicitly where the data will need to be processed.\n",
        "\n",
        "**Note that if data or model reside in different places, Pytorch will produce a run-time error!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "36L-ut0AynFn"
      },
      "outputs": [],
      "source": [
        "import datetime \n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            # put the data onto the correct pytorch calculation device\n",
        "            imgs = imgs.to(device = device)\n",
        "            labels =labels.to(device = device)\n",
        "            # put a batch through the model\n",
        "            outputs = model(imgs)\n",
        "            # determine the loss\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            # zero the gradients and determine backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # do one step of optimization\n",
        "            optimizer.step()\n",
        "            # keep track of the loss\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxeq3XGDynFo"
      },
      "source": [
        "Now, let's train this (small) convolutional neural network with this training_loop class for 100 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaRSpj0gynFp",
        "outputId": "c8625a3b-0036-44c2-9afc-b7d6f2ca4d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-14 06:36:46.548054 Epoch 1, Training loss 0.5481963028573686\n",
            "2022-11-14 06:36:49.812214 Epoch 10, Training loss 0.3275259368738551\n",
            "2022-11-14 06:36:53.340026 Epoch 20, Training loss 0.28943249118176234\n",
            "2022-11-14 06:36:56.858684 Epoch 30, Training loss 0.2651279278716464\n",
            "2022-11-14 06:37:00.347545 Epoch 40, Training loss 0.2503316754558284\n",
            "2022-11-14 06:37:03.840861 Epoch 50, Training loss 0.23127564004841883\n",
            "2022-11-14 06:37:07.359849 Epoch 60, Training loss 0.21492449327069482\n",
            "2022-11-14 06:37:10.918820 Epoch 70, Training loss 0.20169869905254642\n",
            "2022-11-14 06:37:14.432324 Epoch 80, Training loss 0.18960698309597696\n",
            "2022-11-14 06:37:17.909665 Epoch 90, Training loss 0.17358152748672825\n",
            "2022-11-14 06:37:21.409495 Epoch 100, Training loss 0.15804541708937117\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda import device_of\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# get the model and put it onto the proper device\n",
        "model = Net().to(device =device)\n",
        "# standard optimizer \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
        "# classification loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# go for 100 epochs\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-rlHCJgynFq"
      },
      "source": [
        "Compared to our previous version this takes less time - the exact speed-up depends of course on your GPU/CPU combination. \n",
        "\n",
        "The validation also needs to change accordingly - here, we will also need to put the data onto the device. Note that the call to the `predicted` variable below also produces a device-fixed output!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWW6v22gynFq",
        "outputId": "d47c4880-d137-4e07-90d6-ccd9553fee9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy train: 0.94\n",
            "Accuracy val: 0.88\n"
          ]
        }
      ],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    results = []\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device= device)\n",
        "                labels = labels.to(device = device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1)\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "        results.append(correct / total)\n",
        "\n",
        "    return (*results,)\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxpNk_ZCynFr"
      },
      "source": [
        "## Optimizing CNNs\n",
        "\n",
        "In the following, we will discuss a few basic ways how to advance the architecture of CNNs.\n",
        "\n",
        "### Width\n",
        "\n",
        "One of the easiest ways to enhance the capacity of a CNN is of course to change the \"width\" of the network. This means that you will add more filters to a layer. Changing the width can be done, for example, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TEctf8g3ynFs"
      },
      "outputs": [],
      "source": [
        "class NetWidth(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 //2, kernel_size=3,padding=1)\n",
        "        self.fc1 = nn.Linear(8*8*n_ch1//2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8*8*self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrRR5JuFynFt"
      },
      "source": [
        "### Dropout\n",
        "\n",
        "As we increase the number of parameters, however, we will quickly see that it become easy to overfit the NN on pretty much any data. The increased capacity of a wider network is something that we would like to keep nonetheless, so how can we do this?\n",
        "\n",
        "One solution for this is the so-called **dropout** - a technique proposed by Srivastava and Hinton in 2014. \n",
        "\n",
        "The idea for this is actually very simple and implemented like a layer in Pytorch: in every iteration of the training, you zero out a random fraction of outputs of the preceding layer to reduce their influence. \n",
        "\n",
        "What this does is to train slightly different \"models\" in each iteration that try to solve your task, preventing the individual filters (neurons) from talking to each other too much and forming overfitting connections. \n",
        "\n",
        "This technique is not dependent on CNNs actually and can also be used with fully-connected neural networks, btw. \n",
        "\n",
        "In the case of Pytorch and CNNs we can specify 2D dropouts or 3D dropouts that zero entire channel outputs.\n",
        "\n",
        "In order to make the network function properly, we have to be aware, however, whether we are training (dropout should be active) or evaluating (dropout should not be active or have probability zero). In Pytorch, you can control this via the parameters `model.train()` and `model.eval()` for a `nn.Model` subclass like ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-87KCBnynFu"
      },
      "outputs": [],
      "source": [
        "class NetDropout(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        # add the dropout layer\n",
        "        self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3, padding=1)\n",
        "        # add the dropout layer\n",
        "        self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        # add call to dropout\n",
        "        out = self.conv1_dropout(out)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        # add call to dropout\n",
        "        out = self.conv2_dropout(out)\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_s3CyYhynFv"
      },
      "source": [
        "### Batch normalization\n",
        "\n",
        "Another idea, published in 2015 by Ioffe and Szegedy, is Batch Normalization - a technique that rescales the inputs to the activation functions of the networks so that each (mini)batch has a desired, nicely-behaving distribution. \n",
        "\n",
        "Why would we want to do this? Remember that activation functions have saturation points that may prevent efficient learning - we therefore would like to present the activation function with a range of values that make full use of the optimization gradients.\n",
        "\n",
        "In practice, this normalization is done by shifting and scaling an intermediate input using mean and standard deviation across the samples of a given (mini)batch.\n",
        "\n",
        "This results in a regularization of sorts as any individual sample and its following activations are viewed by the full model as shifted and scaled.\n",
        "\n",
        "As such, the normalization was proposed by the authors to obviate the need for dropout as the regularization through shifting and scaling was supposed to help also with overfitting.\n",
        "\n",
        "The place for the batch norm layer in Pytorch is directly before the activation function. Again, as values for the layers are automatically updated with each call, we have to be aware, however, whether we are training (batch norm should be active) or evaluating (batch norm should not be active). In Pytorch, you can control this via the parameters `model.train()` and `model.eval()` for a `nn.Model` subclass like ours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkFY3K3YynFx"
      },
      "outputs": [],
      "source": [
        "class NetBatchNormalization(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        # note that it is also possible to use \"bias=False\" here, as the \n",
        "        # batch normalization layer \"cancels\" the effect of any bias - see below\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1, bias = \"False\")\n",
        "        # add the batch normalization layer\n",
        "        self.conv1_batchnorm = nn.BatchNorm2d(num_features = n_ch1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3, padding=1, bias = \"False\")\n",
        "        # add the batch normalization layer\n",
        "        self.conv2_batchnorm = nn.BatchNorm2d(num_features = n_ch1 // 2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        # add call to batch normalization layer **before activation func**\n",
        "        out = self.conv1_batchnorm(self.conv1(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        # add call to batch normalization layer **before activation func**\n",
        "        out = self.conv2_batchnorm(self.conv2(out))\n",
        "        out = F.max_pool2d(torch.tanh(out), 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQqEe8ByynFy"
      },
      "source": [
        "### Depth\n",
        "\n",
        "Maybe you have wondered why we did not talk about going deeper until now - after all, it's called \"deep learning\"... \n",
        "\n",
        "What are the advantages of going deep again? Depth allows you to discover hierarchies of features - although a shallow network is in principle - via the Universal Approximation Theorem - capable of solving any task for you, splitting the network into layers enables it to discover hierarchical structures in your data. \n",
        "\n",
        "Example: a shallow network will be able to tell apart birds from airplanes, but a \"deep\" network may be able to tell you that birds and airplanes consist of different parts (wings and bodies in both cases, eyes and legs for birds, windows and perhaps landing gears for airplanes, etc.), hence allowing you to describe the data with more structure. \n",
        "\n",
        "So why don't we simply go deep? The reason for that was already hinted at several times previously: each additional layer means that gradients in backpropagation will need to be multiplied, and if you are in the tails of the activation functions that will lead to saturation and to either exploding or vanishing gradients, which will make learning unstable or not moving forward.\n",
        "\n",
        "We have already mentioned batch normalization as one possible way to overcome this problem, and, indeed, this will help us to construct deeper networks.\n",
        "\n",
        "Another approach was presented in 2015 by He et al., with their publication of residual networks (ResNets), which uses the trick of adding a \"skip connection\" to a layer.\n",
        "\n",
        "#### Skip connections\n",
        "\n",
        "A skip connection is simply the addition of the input to the output of a layer. Like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq8Ec19nynFz"
      },
      "outputs": [],
      "source": [
        "class NetSkip(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3, padding=1)\n",
        "        # go deeper!\n",
        "        self.conv3 = nn.Conv2d(n_ch1 //2, n_ch1//2, kernel_size = 3, padding = 1)\n",
        "        # we have now half the size from before\n",
        "        self.fc1 = nn.Linear(4 * 4 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "        # save the input to layer3\n",
        "        ln_layer3 = out\n",
        "        # and add it to the output\n",
        "        out = F.max_pool2d(torch.relu(self.conv3d(out)) + ln_layer3, 2)\n",
        "        out = out.view(-1, 4 * 4 * self.n_ch1 // 2)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN5qbkZSynF0"
      },
      "source": [
        "Good, so now we have added another layer, and we have also added its input to its output. Because we have done this, the skip connection is now part of the computational graph and hence part of the loss gradient path.\n",
        "\n",
        "Since they are more directly connected to the loss (note, they are outside the activation functions!), their addition to the network means that the optimizer can decide to bypass a layer. The effect of this is that gradients across multiple layers are less likely to vanish.\n",
        "\n",
        "#### Blocks of layers\n",
        "\n",
        "If we really want to go deep, we need a better way to initialize our networks, as the \"manual\" way of adding layers that we have done so far is not going to be feasible.\n",
        "\n",
        "We do this by first defining a `ResNetBlock` class, which includes one block of convolutions, activations, and skip connections. In addition, we will add back batch normalization and also add a special type of weight initialization that aids optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wnArNEu5ynF0"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_ch):\n",
        "        super(ResBlock, self).__init__()\n",
        "        # here we now explicitly get rid of the biases as we use BN\n",
        "        self.conv = nn.Conv2d(n_ch, n_ch, kernel_size = 3, padding =1, bias = False)\n",
        "        # batch normalization \n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_ch)\n",
        "        # BN is initialized to have 0.5 \"variance\" and 0 mean\n",
        "        # 이거 안하면 -1 과 1 사이로 initialization 됨\n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "        # this is a special initialization of the convolution weights \n",
        "        # that was found to help with optimization\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
        "    def forward(self, x):\n",
        "        # simple forward function that includes skip connection\n",
        "        out = self.conv(x)\n",
        "        out = self.batch_norm(out)\n",
        "        out = torch.relu(out)\n",
        "        return out + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWlP5OTqynF1"
      },
      "source": [
        "Now that we have one block, we will use this to create a deep architecture like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PrR2J5RjynF1"
      },
      "outputs": [],
      "source": [
        "class NetResDeep(nn.Module):\n",
        "    # the constructor now has a parameter n_blocks that determines how many\n",
        "    # ResBlocks will be used\n",
        "    def __init__(self, n_ch1=32, n_blocks=10):\n",
        "        super().__init__()\n",
        "        self.n_ch1 = n_ch1\n",
        "        # initial convolution for embedding\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        # add the ResBlocks to the network\n",
        "        # nn.Sequential makes sure that we can add them together nicely\n",
        "        self.resblocks = nn.Sequential(*(n_blocks * [ResBlock(n_ch = n_ch1)]))\n",
        "        # note that the \"net\" effect of the blocks does not reduce the dimensionality\n",
        "        # of the features - hence, we again have 8x8 \"pixels\" resolution here\n",
        "        self.fc1 = nn.Linear(8*8*n_ch1, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "        out = self.resblocks(out)\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1)\n",
        "        out = torch.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOv_7Xo4ynF2"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "# get the model and put it onto the proper device\n",
        "model = NetResDeep().to(device=device)\n",
        "# standard optimizer \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3) \n",
        "# classification loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# go for 100 epochs\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C7-dBaJynF2"
      },
      "outputs": [],
      "source": [
        "validate(model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMUv_EFlynF2"
      },
      "source": [
        "As we can see, however, although the last - and possibly most advanced design - is easily capable of achieving 100% training accuracy, its validation accuracy is still roughly the same. \n",
        "\n",
        "This is in part due to the fact that we've tried to go deep on images that only have 32x32 pixels - discovering hierarchies in such a low-dimensional input space may therefore be limited. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WULGIHwIynF3"
      },
      "source": [
        "## 1x1 convolutions\n",
        "\n",
        "Another architecture element that is often used in CNNs is the 1x1 convolution layer. It is often called a projection or embedding layer as well.\n",
        "\n",
        "Now, mathematically, a 1x1 convolution basically takes each input and weights it with a single number, producing another number. So, there is no influence of neighboring elements in this operation - it is purely **local**.\n",
        "\n",
        "Note that if we treat a 1x1 convolution as a layer, however, its output can be fed into an activation function, which in turn means that the full layer can perform complex, non-linear operations on inputs.\n",
        "\n",
        "In addition, note, that this layer can perform its operations along the channel dimension, generating again a single number as output across all input channels. This means that in a deep architecture, the 1x1 operation will basically \"summarize\" (other words that are often used are \"project\" / \"embed\" / \"pool\" in this context) the full set of channels into one feature map, preserving its width x height dimensions. \n",
        "\n",
        "The following is an example of a projection, in which the dimensionality is preserved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XrbIs4_ynF3"
      },
      "outputs": [],
      "source": [
        "class NetWidthProject(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3,padding=1)\n",
        "        # keeps the number of filters\n",
        "        self.conv3 = nn.Conv2d(n_ch1 // 2, n_ch1 //2, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 2, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        # Conv1d -> Keep the number of filter\n",
        "        out = torch.tanh(self.conv3(out))\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 2)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kMO0uXwynF4"
      },
      "outputs": [],
      "source": [
        "model = NetWidthProject().to(device=device)\n",
        "print(model)\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7I6gZqQynF5"
      },
      "source": [
        "The following is an example of how to use the 1x1 layer for embedding or dimensionality reduction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBVLRKUUynF6"
      },
      "outputs": [],
      "source": [
        "class NetWidthEmbed(nn.Module):\n",
        "    def __init__(self, n_ch1=32):\n",
        "        super().__init__()\n",
        "        # needed later in the forward function\n",
        "        self.n_ch1 = n_ch1\n",
        "        self.conv1 = nn.Conv2d(3, n_ch1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(n_ch1, n_ch1 // 2, kernel_size=3,padding=1)\n",
        "        # reduces the number of filters by 2\n",
        "        # Reduce dimenionality ; Embed or summarize to lower dimension\n",
        "        self.conv3 = nn.Conv2d(n_ch1 // 2, n_ch1 //4, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_ch1 // 4, 32)\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = torch.tanh(self.conv3(out))\n",
        "        out = out.view(-1, 8 * 8 * self.n_ch1 // 4)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoP1crm8ynF6"
      },
      "outputs": [],
      "source": [
        "model = NetWidthEmbed().to(device=device)\n",
        "print(model)\n",
        "\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmVRpIqXynF7"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "# get the model and put it onto the proper device\n",
        "model = NetWidthEmbed().to(device=device)\n",
        "# standard optimizer \n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
        "# classification loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# go for 100 epochs\n",
        "training_loop(\n",
        "    n_epochs = 1000,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NjYrfCIynF7"
      },
      "source": [
        "Just in the same way is it can be used for dimensionality reduction, the 1x1 convolution can also be used for dimensionality increase! This can be useful for upscaling images or feature maps.\n",
        "\n",
        "The idea of using these filters was first proposed in 2013 and put to full use in the 2014 Inception architecture.\n",
        "\n",
        "In the original ResNet architecture, for example, these have also been used to introduce 3x3 \"bottleneck\" layers, in which a 1x1 convolution first decreased the dimensionality and then increased it again after the actual 3x3 convolutions like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfS7PMZsynF8"
      },
      "outputs": [],
      "source": [
        "# reduces the number of filters\n",
        "self.convNNm1 = nn.Conv2d(n_ch1, n_ch1 // 4, kernel_size=1)\n",
        "# applies 3x3 convolution \n",
        "self.convNN = nn.Conv2d(3, n_ch1 // 4, kernel_size=3, padding=1)\n",
        "# upsamples again\n",
        "self.convNNp1 = nn.Conv2d(n_ch1 // 4, n_ch1, kernel_size=1,padding=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "102292e4ae1d47a6a020e9954ffcb82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1634931e860540dcbe8f7a62a1eaccb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "312782531b4b4394a5c16a2e317348de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b265cad13b724f50b99be913e2d767a8",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5504cd6f70024482ab73f370276e017f",
            "value": 170498071
          }
        },
        "4073f565ff8f48bfa25785e7b1a48550": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca64e0b99b74dddbb9badc5e0e1d76e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5504cd6f70024482ab73f370276e017f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cca6e3c839e48f395ad7e72f60cc77e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1767402b20241c987dab26907ab1adb",
            "placeholder": "​",
            "style": "IPY_MODEL_102292e4ae1d47a6a020e9954ffcb82e",
            "value": "100%"
          }
        },
        "b265cad13b724f50b99be913e2d767a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd45b523924544e7ac1f5491f4253d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca64e0b99b74dddbb9badc5e0e1d76e",
            "placeholder": "​",
            "style": "IPY_MODEL_4073f565ff8f48bfa25785e7b1a48550",
            "value": " 170498071/170498071 [00:14&lt;00:00, 13363288.04it/s]"
          }
        },
        "e1767402b20241c987dab26907ab1adb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5220d8de32b4201bdcecb0a9b700a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8cca6e3c839e48f395ad7e72f60cc77e",
              "IPY_MODEL_312782531b4b4394a5c16a2e317348de",
              "IPY_MODEL_bd45b523924544e7ac1f5491f4253d06"
            ],
            "layout": "IPY_MODEL_1634931e860540dcbe8f7a62a1eaccb6"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
