{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 002 Optimization with Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at how linear algebra can help us in optimization!\n",
    "\n",
    "Let's again start with an example which is defined by a linear system of equations. \n",
    "\n",
    "I want to know the masses of three very light objects. These objects are so light (<1g), that one object is not going to work with our scale that we have, which only starts to measure objects from >1g. \n",
    "\n",
    "So, we simply put on multiple objects. Doing this, we get, for example:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "o_1&+o_2& &=1.8\\\\\n",
    "   & o_2&+o_3 &=1.3\\\\\n",
    "o_1&    &+o_3 &=1.4\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now we have three equations for three unknowns, the columns are all independent, so we can invert the matrix, and our solution becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,1,0],[0,1,1],[1,0,1]])\n",
    "b=np.array([1.8,1.3,1.4])\n",
    "masses = ...\n",
    "print('Solution for the masses is',masses)\n",
    "print('Sanity check of A*masses yields',...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But our scale is not very good, so each of the measurements may have a slight error. So we decide to measure one more combination with all objects:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "o_1&+o_2& &=1.8\\\\\n",
    "   & o_2&+o_3 &=1.3\\\\\n",
    "o_1&    &+o_3 &=1.4\\\\\n",
    "o_1&+o_2&+o_3 &=2.2\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "But now we have four equations for three unknowns, so we have more information than we need. We still would like to find some sort of solution, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Rows than Columns\n",
    "\n",
    "This is the $ n \\times k $ case with $ n > k $ and is extremely important in lots of applications, including **linear regression** (where $ n $ is the number of inputs, and $ k $ is the number of model variables).\n",
    "\n",
    "Given arbitrary $\\vec{y} \\in \\mathbb R ^n $, we seek an $ \\vec{x} \\in \\mathbb R ^k $ such that $ \\vec{y} = A\\vec{x} $\n",
    "\n",
    "Let's assume that the columns of $ A $ are linearly independent, which means that the span of $ A $ is a $ k $-dimensional subspace of $ \\mathbb R ^n $. Given that this is a subspace, it is highly improbable that this subspace can contain a random vector $ \\vec{y} \\in \\mathbb R ^n $.\n",
    "\n",
    "Imagine the two-dimensional subspace of $\\mathbb R^3$, so $ k=2 $ and $ n=3 $. Now, let's pick a random point (vector) in $\\vec{y}=\\mathbb R^3$ - in essence, we're asking now: what is the probability that $\\vec{y}$ is exactly on the plane? \n",
    "\n",
    "This probability is intuitively very low, since the plane is infinitesimally thin. So, the chances for a random $\\vec{y}$ to be part of the subspace are vanishingly low.\n",
    "\n",
    "Therefore, if $ n > k $, we usually do not assume a unique solution. \n",
    "\n",
    "The key idea, however, is that we can still look for an **approximate solution**, for example, by saying that we would like to find the point $\\vec{x}$ in the plane-subspace that is **closest** to $\\vec{y}$, or in other words: \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>Look for $ \\vec{x} $, such that $ \\| \\vec{y} - A\\vec{x}\\| $ ....\n",
    "</div>\n",
    "\n",
    "Let's solve this first using calculus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underset{\\vec{x}}{\\arg\\min}{\\| \\vec{y} - A\\vec{x}\\|}$ using calculus\n",
    "\n",
    "Let's start. We need to find the **vector**-derivative of our expression and set it to zero:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\| \\vec{y} - A\\vec{x}\\|^2 =\\min\\rightarrow\\frac{d}{d\\vec{x}}\\| \\vec{y} - A\\vec{x}\\|=0\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\vec{x}}:=\\left (\\begin{matrix}\\frac{d}{dx_1}\\\\\\frac{d}{dx_2}\\\\\\vdots\\\\\\frac{d}{dx_n}\\end{matrix}\\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "&\\frac{d}{d\\vec{x}}(\\vec{y} - A\\vec{x})^{\\top}\\cdot(\\vec{y} - A\\vec{x})\n",
    "\\\\\n",
    "=&\\frac{d}{d\\vec{x}}(\\|\\vec{y}\\|^2 - \\vec{y}^{\\top}\\cdot A\\vec{x}-\\vec{x}^{\\top}A^{\\top}\\cdot\\vec{y}+\\vec{x}^{\\top}A^{\\top}A\\vec{x})\n",
    "\\\\\n",
    "=&\\frac{d}{d\\vec{x}}(\\|\\vec{y}\\|^2-2\\vec{x}^{\\top}A^{\\top}\\cdot\\vec{y}+\\vec{x}^{\\top}A^{\\top}A\\vec{x})\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Hmm, but we do not know yet how to take vector derivatives. So let's take a $2\\times 3$-example, and multiply the term after the derivative explicitly:\n",
    "\n",
    "$$\n",
    "(-2(x_1,x_2)\\left (\\begin{matrix}a_{11}&a_{21}&a_{31}\\\\a_{12}&a_{22}&a_{32}\\end{matrix}\\right ) \\left (\\begin{matrix}y_1\\\\y_2\\\\y_3\\end{matrix}\\right )+\\\\(x_1,x_2)\\left  (\\begin{matrix}a'_{11}&a'_{12}\\\\a'_{21}&a'_{22}\\end{matrix}\\right ) \\left ( \\begin{matrix}x_1\\\\x_2\\end{matrix}\\right ) )\n",
    "$$\n",
    "\n",
    "$$\n",
    "=(-2(x_1,x_2)\\left (\\begin{matrix}y'_1\\\\y'_2\\end{matrix}\\right )+(x_1,x_2)\\left  (\\begin{matrix}a'_{11}x_1+a'_{12}x_2\\\\a'_{21}x_1+a'_{22}x_2\\end{matrix}\\right ) )\n",
    "$$\n",
    "\n",
    "$$\n",
    "=(-2x_1 y'_1-2x_2 y'_2+(x_1 a'_{11}x_1+x_1 a'_{12}x_2+x_2 a'_{21}x_1+x_2 a'_{22}x_2))\n",
    "$$\n",
    "\n",
    "So, now we take the derivatives to the two variables:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx_1}=(-2y'_1+(2x_1 a'_{11}+a'_{12}x_2+x_2 a'_{21}))\n",
    "$$\n",
    "\n",
    "realizing that $A^{\\top}A$ is a **symmetric** matrix by definition, so that $a'_{12}=a'_{21}$, we have\n",
    "\n",
    "$$\n",
    "=(-2y'_1+(2x_1 a'_{11}+2x_2 a'_{12}))\n",
    "$$\n",
    "\n",
    "and, similarly:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx_2}=(-2y'_2+(2x_1 a'_{12}x_1+2x_2 a'_{22}))\n",
    "$$\n",
    "\n",
    "and collecting terms into a vector:\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\vec{x}}:=\\left (\\begin{matrix}\\frac{d}{dx_1}\\\\\\frac{d}{dx_2}\\end{matrix}\\right )=\n",
    "...\n",
    "$$\n",
    "\n",
    "So, we can see that vector derivatives cancel out the $\\vec{x}^{\\top}$ in the expression, so to speak.\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\vec{x}}=0\\rightarrow\n",
    "-2A^{\\top}\\vec{y}+2A^{\\top}A\\vec{x}=0\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\rightarrow A^{\\top}\\vec{y}=A^{\\top}A\\vec{x}\n",
    "$$\n",
    "\n",
    "Given the existence of $(A^{\\top}A)^{-1}$, we multiply with this inverse from the left-hand side and obtain:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>$$\n",
    "\\vec{x}=...\n",
    "$$\n",
    "</div>\n",
    "\n",
    "as our solution.\n",
    "\n",
    "\n",
    "Now, we are not done yet, as we still have to prove that this extremum is actually a **minimum**. For this, we need to collect the **Hessian** matrix of our problem and prove that it is positive definite. For now, we will table this and hope for the best (spoiler: yes, it is). \n",
    "\n",
    "More rules on how to differentiate with respect to vectors are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example ctd.\n",
    "\n",
    "Let's return to our example with the three objects that were measured four times. The resulting matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[1,1,0],[0,1,1],[1,0,1],[1,1,1]])\n",
    "b=np.array([1.8,1.3,1.4,2.2])\n",
    "x=np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(A),A)),np.transpose(A)),b)\n",
    "errors = np.matmul(A,x)-b\n",
    "residual=np.sum(np.matmul(errors,np.transpose(errors)))\n",
    "print('least squares solution is',x,'with residual',residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that our fourth measurement has slightly changed all previous measurements a bit.\n",
    "\n",
    "We also can see that our solution is not \"perfect\", as it does not explain the full situation and we get a residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical stability\n",
    "As an additional comment, it turns out that calculating inverses like this is not very stable. Rather, people have developed better algorithms based on orthogonal transformations that work in a similar fashion to Gaussian Elimination in order to reduce the system of equations to a solvable form.\n",
    "\n",
    "Since orthogonal transformations are important in general, however, let me quickly introduce them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal transformations\n",
    "\n",
    "An orthogonal transformation $M$ is one for which:\n",
    "\n",
    "$$M^{\\top}=M^{-1}$$\n",
    "\n",
    "Or in other words, the inverse of $M$ is simply its transpose.\n",
    "\n",
    "Why are these transformations special? Well, they keep the length of a vector unchanged!!\n",
    "\n",
    "To see why this is so, consider a vector $\\vec{x}$, which has length $\\|\\vec{x}\\|^2=<\\vec{x}^{\\top},\\vec{x}>$.\n",
    "\n",
    "The transformed vector $M\\vec{x}$ has length \n",
    "\n",
    "$$\n",
    "\\|M\\vec{x}\\|^2=<\\vec{x}^{\\top}M^{\\top},M\\vec{x}>=\\vec{x}^{\\top}M^{\\top}M\\vec{x}\n",
    "$$\n",
    "$$=\\vec{x}^{\\top}M^{-1}M\\vec{x}=\\vec{x}^{\\top}I\\vec{x}=\\vec{x}^{\\top}\\vec{x}=\\|\\vec{x}\\|^2\n",
    "$$\n",
    "\n",
    "Famous examples of orthogonal transformations include:\n",
    "\n",
    "1. $M=I$ trivial to see\n",
    "2. $M=\\left( \\begin{matrix}\\cos{\\theta}&-\\sin{\\theta}\\\\\\sin{\\theta}&\\cos{\\theta}\\end{matrix}\\right )$. This is the rotation matrix, which from intuition alone should tell you that it doesn't change the length of something\n",
    "\n",
    "Now, these transformations are used in the application where we have more rows than columns - for those interested, the name of this technique relies on <a href=\"https://en.wikipedia.org/wiki/Householder_transformation\">Householder Transformations</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\underset{\\vec{x}}{\\arg\\min}{\\| \\vec{y} - A\\vec{x}\\|}$ using projections\n",
    "\n",
    "Let $S$ be a linear subspace of $\\mathbb{R}^n$ and let $\\vec{y}\\in\\mathbb{R}^n$\n",
    "\n",
    "Given that the columns of $n\\times k$ matrix $X$ form a basis of $S$. Then, we can construct $P=X(X^{\\top}X)^{−1}X^{\\top}$ as a projection matrix that takes elements of $\\mathbb{R}^n$ and projects them into $S$.\n",
    "\n",
    "So, we need to show that given an arbitrary $\\vec{y}\\in\\mathbb{R}^n$ and $P=X(X^{\\top}X)^{−1}X^{\\top}$\n",
    "\n",
    "1. $P\\vec{y}\\in S$: $P\\vec{y}=(X(X^{\\top}X)^{−1}X^{\\top})\\vec{y}=X\\vec{a}$ with $\\vec{a}:=(X^{\\top}X)^{−1}X^{\\top}y$. Given that we said that the columns of $X$ spans $S$, we see that $X\\vec{a}\\in S$, and so we are done.\n",
    "2. $\\vec{y}−P\\vec{y}\\perp S$, or, equivalently:\n",
    "$\\vec{y}−(X(X^{\\top}X)^{−1}X^{\\top})\\vec{y}\\perp X\\vec{b}$ $\\forall\\vec{b}\\in\\mathbb{R}^k$. So, let's do the inner product: $(X\\vec{b})^{\\top}(\\vec{y}−(X(X^{\\top}X)^{−1}X^{\\top})\\vec{y})$\n",
    "$=\\vec{b}^{\\top}(X^{\\top}\\vec{y}-X^{\\top}X(X^{\\top}X)^{-1}X^{\\top}\\vec{y})$\n",
    "$=\\vec{b}^{\\top}(X^{\\top}\\vec{y}-X^{\\top}\\vec{y})=0$\n",
    "\n",
    "#### Example \n",
    "\n",
    "Let's project stuff onto a plane going through the origin in $\\mathbb{R}^3$ and spanned by the two vectors $\\vec{x}_1=\\left (\\begin{matrix}1\\\\1\\\\1\\end{matrix}\\right )$ and $\\vec{x}_2=\\left (\\begin{matrix}-1\\\\1\\\\0\\end{matrix}\\right )$.\n",
    "\n",
    "It's easy to see that $<\\vec{x}_1,\\vec{x}_2>=0$ and hence they are orthogonal, which means that we have a subspace of Span 2.\n",
    "\n",
    "Now, our projection matrix of the full space onto this becomes:\n",
    "\n",
    "$P=X(X^{\\top}X)^{−1}X^{\\top}$ with $X=\\left (\\begin{matrix}1&-1\\\\1&1\\\\1&0\\end{matrix}\\right )$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,-1],[1,1],[1,0]]);\n",
    "Xt = np.transpose(X)\n",
    "P=np.matmul(np.matmul(X,np.linalg.inv(np.matmul(Xt,X))),Xt)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the plane and a point in $\\mathbb{R}^3$, as well as its projection onto the plane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%plot native\n",
    "[x,y]=meshgrid([-3:6:3]);\n",
    "surf(x,y,0.5*x+0.5*y,'FaceAlpha',0.5); hold on\n",
    "quiver3(0,0,0,1,1,1,'LineWidth',2,'Color','k')\n",
    "quiver3(0,0,0,-1,1,0,'LineWidth',2,'Color','k')\n",
    "v = [1;1;2.5];\n",
    "plot3(v(1),v(2),v(3),'ko')\n",
    "pv = P*v;\n",
    "plot3(pv(1),pv(2),pv(3),'kx')\n",
    "plot3([pv(1) v(1)],[pv(2) v(2)],[pv(3) v(3)],'k--')\n",
    "axis equal\n",
    "view(17,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof continued\n",
    "\n",
    "We want to show that\n",
    "$\\vec{x}^*=(A^{\\top}A)^{−1}A^{\\top}\\vec{y}$ is a minimal solution for the overdetermined system $A\\vec{x}=\\vec{y}$.\n",
    "\n",
    "Note that \n",
    "\n",
    "$A\\vec{x}^*=A(A^{\\top}A)^{−1}A^{\\top}\\vec{y}=P\\vec{y}$, which means that this is the orthogonal projection of our input vector $\\vec{y}$ onto the span of $A$.\n",
    "\n",
    "Because it's orthogonal, it means that:\n",
    "\n",
    "$\\|\\vec{y}-P\\vec{y}\\|\\leq\\|\\vec{y}-\\vec{z}\\|$ for any $\\vec{z}\\in\\text{span}(A)$\n",
    "\n",
    "But, because $A\\vec{x}\\in\\text{span}(A)$, it means that\n",
    "\n",
    "$\\|\\vec{y}-A\\vec{x}^*\\|\\leq\\|\\vec{y}-A\\vec{x}\\|$, and hence it is optimal for arbitrary choice of $\\vec{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Polynomial fitting\n",
    "\n",
    "So, how do we apply this knowledge now to real-world problems?\n",
    "\n",
    "Let's say we want to fit a polynomial to data - in general, a polynomial is a **linear** function in its coefficients $a_i$ like so:\n",
    "\n",
    "$$\n",
    "p_a(x):=\\sum_i^n a_i x^i\n",
    "$$\n",
    "\n",
    "So, if we are given $k>>n$ tuples of input and output data $x_i,y_i$ and we want to fit a polynomial of known degree $n$ to it in the least squares sense, we have to minimize: \n",
    "\n",
    "$$\n",
    "\\underset{\\vec{a}}{\\arg\\min}{\\| \\vec{y} - A(\\vec{x})\\vec{a}\\|}\n",
    "$$\n",
    "\n",
    "where the matrix $A(\\vec{x})$ encodes the input data and $\\vec{a}$ is the vector of polynomial coefficients we want to look for.\n",
    "\n",
    "So what does the matrix $A$ look like?\n",
    "\n",
    "At each input, output pair $x_i,y_i$, we get the following equation:\n",
    "\n",
    "$$\n",
    "a_0+a_1 x_{i} + a_2 x_{i}^2 + ... + a_n x_{i}^n = y_i\n",
    "$$\n",
    "\n",
    "so $A \\in \\mathbb{R}^{k\\times n}$:\n",
    "\n",
    "$$\n",
    "\\left (\n",
    "\\begin{matrix}\n",
    "1 &  x_{1} &  x_{1}^2 & ... &  x_{1}^n\\\\\n",
    "1 &  x_{2} &  x_{2}^2 & ... &  x_{2}^n\\\\\n",
    "\\vdots &\\vdots & \\vdots& \\vdots & \\vdots\\\\\n",
    "1 &  x_{k} &  x_{k}^2 & ... &  x_{k}^n\\\\\n",
    "\\end{matrix}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "This matrix with increasing powers of $x_i$ is called the **Vandermonde** matrix. \n",
    "\n",
    "To fit the polynomial, we simply do:\n",
    "\n",
    "$\\vec{a}=(A^{\\top}A)^{-1}A^{\\top}\\vec{y}$\n",
    "\n",
    "and we're done!\n",
    "\n",
    "Let's try it with some linear data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data\n",
    "y = np.array([[1,3.7],[2,5.4],[3,5.6],[3.2,7.0],[4,7.6],[5,7.9],[6,9.5]])\n",
    "\n",
    "# scatter the data\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(y[:,0],y[:,1])\n",
    "plt.grid()\n",
    "\n",
    "# create the Vandermonde matrix with 1 x_i for linear polynomial\n",
    "A=...\n",
    "\n",
    "# define y_i\n",
    "b = y[:,1]\n",
    "\n",
    "# solve with numpy least squares algorithm (numerically MUCH more stable!)\n",
    "coeff=...\n",
    "print('Solution according to least squares algorithms',coeff[0])\n",
    "\n",
    "# solve with transpose (yes, that will be the same ^^)\n",
    "coeff=np.matmul(np.matmul(np.linalg.inv(np.matmul(np.transpose(A),A)),np.transpose(A)),b)\n",
    "print('Solution according to matrix manipulation',coeff)\n",
    "\n",
    "# plot best fitting line\n",
    "xplot = np.arange(0.5,6.5,0.05)\n",
    "plt.plot(xplot,coeff[0]+coeff[1]*xplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the same data with polynomials of degrees 2-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter the data\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(y[:,0],y[:,1])\n",
    "plt.grid()\n",
    "\n",
    "for n in np.arange(1,4):\n",
    "    # create the Vandermonde matrix for quadratic polynomial\n",
    "    # sooo many Python hacks for array broadcasting here:\n",
    "    # https://stackoverflow.com/questions/66242475/how-to-raise-every-element-of-a-vector-to-the-power-of-every-element-of-another\n",
    "    A=...\n",
    "\n",
    "    \n",
    "    # define y_i\n",
    "    b = y[:,1]\n",
    "    \n",
    "    print(A,b)\n",
    "\n",
    "    # solve with backslash (numerically MUCH more stable!)\n",
    "    coeff=np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "    print('Solution according to least squares algorithms',coeff)\n",
    "\n",
    "\n",
    "    # y-values for plotting points\n",
    "    xplott=np.transpose(xplot)\n",
    "    yplot = np.matmul(xplott[:,None]**np.arange(0,n+1),coeff);\n",
    "    \n",
    "    # error\n",
    "    err=np.linalg.norm(np.matmul(A,coeff)-b);\n",
    "    print('Error',err)\n",
    "    plt.plot(xplot,yplot,label='Degree {:d} - Error {:.2f}'.format(n,err))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, which fit is better? Obviously, we can see that the error reduces for each higher degree. In fact, we know that when $k==n$, we will have $n$ unknowns for $n$ equations and we will therefore be able to find a unique solution to fitting the polynomial with **zero error**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter the data\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(y[:,0],y[:,1])\n",
    "plt.grid()\n",
    "\n",
    "n = len(y[:,0])\n",
    "# create the Vandermonde matrix for quadratic polynomial\n",
    "# sooo many Python hacks for array broadcasting here:\n",
    "# https://stackoverflow.com/questions/66242475/how-to-raise-every-element-of-a-vector-to-the-power-of-every-element-of-another\n",
    "A=...\n",
    "\n",
    "\n",
    "# define y_i\n",
    "b = y[:,1]\n",
    "\n",
    "print(A,b)\n",
    "\n",
    "# solve with backslash (numerically MUCH more stable!)\n",
    "coeff=np.linalg.lstsq(A,b,rcond=None)[0]\n",
    "print('Solution according to least squares algorithms',coeff)\n",
    "\n",
    "\n",
    "# y-values for plotting points\n",
    "xplott=np.transpose(xplot)\n",
    "yplot = np.matmul(xplott[:,None]**np.arange(0,n+1),coeff);\n",
    "\n",
    "# error\n",
    "err=np.linalg.norm(np.matmul(A,coeff)-b);\n",
    "print('Error',err)\n",
    "plt.plot(xplot,yplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erg. But that polynomial fluctuates quite crazily. This brings us to a very important point:\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<p>\n",
    "...\n",
    "</div>\n",
    "\n",
    "If we believe that our data was completely fine and we have zero noise and there are no additional points to be measured in between or outside the given points, then we are fine. \n",
    "\n",
    "However, we seemed quite happy initially to claim that our data was simply following a **linear** trend - this is because we assumed (intuitively) that the data was sampled from an underlying model. In essence, what we are trying to do with the fitting is to use the **limited** samples to **guess the underlying model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Columns than Rows\n",
    "\n",
    "Let's talk briefly about another case of solving systems of equations. Let's say we have $ n \\times k $ case with $ n < k $, and so we have fewer equations than unknowns. Our system is **underdetermined**.\n",
    "\n",
    "Intuitively it should be clear that in this case there are either no solutions or infinitely many — in other words, we can have no unique solution.\n",
    "\n",
    "When $ k=3 $ and $ n=2 $, the columns of $ A $ consist of 3 vectors in $ \\mathbb R ^2 $, and this set cannot be linearly independent, since only two vectors are enough to span $ \\mathbb R ^2 $ and therefore a third column is a linear combination of the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiating vectors\n",
    "\n",
    "The following relationships are useful in many applications and tell you how to differentiate expressions with respect to vectors and even matrices (i.e., linear transformations).  \n",
    "\n",
    "We define the following quantities:\n",
    "\n",
    "- $ \\vec{z}, \\vec{x} $ and $ \\vec{a} $ all be $ n \\times 1 $ vectors  \n",
    "- $ A $ be an $ n  \\times n $ matrix  \n",
    "- $ B $ be an $ m \\times n $ matrix and $ \\vec{y} $ be an $ m  \\times 1 $ vector  \n",
    "\n",
    "\n",
    "Then\n",
    "\n",
    "1. $ \\frac{\\partial \\vec{a}^{\\top} \\vec{x}}{\\partial \\vec{x}} = a $  \n",
    "1. $ \\frac{\\partial A \\vec{x}}{\\partial \\vec{x}} = A^{\\top} $  \n",
    "1. $ \\frac{\\partial \\vec{x}^{\\top} A\\vec{x}}{\\partial \\vec{x}} = (A + A^{\\top}) \\vec{x} $, if $A$ symmetric, this reduces to  $2A\\vec{x} $\n",
    "1. $ \\frac{\\partial \\vec{y}^{\\top}B \\vec{z}}{\\partial \\vec{y}} = B \\vec{z} $  \n",
    "1. $ \\frac{\\partial \\vec{y}^{\\top}B \\vec{z}}{\\partial B} = \\vec{y} \\vec{z}^{\\top} $  "
   ]
  }
 ],
 "metadata": {
  "filename": "linear_algebra.rst",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "title": "Linear Algebra"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
