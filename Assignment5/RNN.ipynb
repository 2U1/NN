{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from tqdm.notebook import tqdm\n",
    "import subprocess\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "\n",
    "    path_list = []\n",
    "    \n",
    "    for c in tqdm(alphabet):\n",
    "        target_path = os.path.join(rootpath + c + '/*.abc')\n",
    "        for path in glob.glob(target_path):\n",
    "            path_list.append(path)\n",
    "\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_data(text):\n",
    "    # extract song from abc notation\n",
    "    # remove header\n",
    "    text = re.sub(r'%%.*', '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n{2,}',r'\\n',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_song_to_abc(song, filename):\n",
    "    save_name = \"/result/{}.abc\".format(filename)\n",
    "    with open(save_name, 'w') as f:\n",
    "        f.write(song)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def abc2wav(abc_file):\n",
    "    suf = abc_file.rsrip('.abc')\n",
    "    cmd = \"abc2midi {} -o {}\".format(abc_file, suf + \".mid\")\n",
    "    os.system(cmd)\n",
    "    cmd = \"timidity {}.mid -Ow {}.wav\".format(suf, suf)\n",
    "\n",
    "    return os.system()\n",
    "\n",
    "def play_wav(wav_file):\n",
    "    return ipythondisplay.Audio(wav_file)\n",
    "\n",
    "def play_song(song):\n",
    "    basename = save_song_to_abc(song)\n",
    "    ret = abc2wav(basename + \".abc\")\n",
    "    if ret == 0:\n",
    "        return play_wav(basename + \".wav\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178c79b9cd254b0e98b9517b26460678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "rootpath = cwd + '/notation/'\n",
    "notation_list = make_datapath_list(rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9c7048ec104f648445c24d72203506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs = []\n",
    "\n",
    "for notation in tqdm(notation_list):\n",
    "    with open(notation, 'r') as f:\n",
    "        text = f.read()\n",
    "        song = extract_song_data(text)\n",
    "        songs.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example song: \n",
      "X: 0\n",
      "T: Austria\n",
      "Z: Franz Joseph Hayden, 1797\n",
      "Z: Public  domain\n",
      "L: 1/4\n",
      "M: 4/4\n",
      "V: P1 name=\"Unnamed-000\"\n",
      "V: P2 name=\"Unnamed-001\"\n",
      "V: P3 name=\"Tempo Track\"\n",
      "K: Eb\n",
      "[V: P1]  [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,F] [B,G] [B,/F/]D/ B, | [C/A/]D/ [EG] [D/F/][B,/D/] B, | [B,B] [C/A/]D/ [E3/G3/] [E/G/] | [E3/=A3/] [E/A/] [D2B2] | [E3/e3/] [E/d/] [E/d/]c/ [EB] | [E3/c3/] [E/B/] [D/B/]A/ [EG] | [DF] [D/G/]A/ [E/B/]c/ [C/A/]F/ | [B,E] [D/G/]F/ E2- | E2z2|]\n",
      "[V: P2]  [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [B,,D,] [B,,E,] [B,,/D,/]F,/ [B,,/A,/]G,/ | [B,,F,] [B,,/E,/]G,/ [B,,/B,/]F,/ [B,,/D,/]A,,/ | [G,,G,] [A,,/F,/]B,,/ [C,3/E,3/] [C,/C/] | [F,3/C3/] [F,,/F,/] [B,,2F,2] | [G,3/C3/] [G,/B,/] A, [E,G,] | A,3/ [G,/B,/] [F,B,] [E,B,] | [B,,B,] [A,,B,] [G,,/B,/]E,/ [A,,/C/]A,/ | [B,,G,] [B,,A,] [E,,2-E,2-G,2-] | [E,,2E,2G,2]z2|]\n",
      "[V: P3]  z4|]\n"
     ]
    }
   ],
   "source": [
    "example_song = songs[119]\n",
    "print(\"Example song: \")\n",
    "print(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to read data\n",
    "# this class is used to read data from a list of songs and return a data\n",
    "# the input is a list of songs, sequence length, batch size\n",
    "# the output is a batch of data\n",
    "\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, songs, seq_len):\n",
    "        self.songs = songs\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = set()\n",
    "        self.vocab_to_int = {}\n",
    "        self.int_to_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.song_index = 0\n",
    "        self.song = self.songs[self.song_index]\n",
    "        self.song_len = len(self.song)\n",
    "        self.song_index += 1\n",
    "        self.create_vocab()\n",
    "        self.create_int()\n",
    "        self.n_batch = int(self.song_len / self.seq_len)\n",
    "        \n",
    "    def create_vocab(self):\n",
    "        for song in self.songs:\n",
    "            self.vocab = self.vocab.union(set(song))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_int(self):\n",
    "        self.vocab_to_int = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = dict(enumerate(self.vocab))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x_str = self.song[index : index + self.seq_len]\n",
    "        y_str = self.song[index+1 : index + self.seq_len + 1]\n",
    "\n",
    "        x = torch.tensor([self.vocab_to_int[c] for c in x_str])\n",
    "        y = torch.tensor([self.vocab_to_int[c] for c in y_str])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to build a RNN model\n",
    "# the input is the number of hidden units, the number of layers, the size of the vocabulary, the batch size, the sequence length\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers, embedding_dim, vocab_size, batch_size, seq_len):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        x = self.embed(x)\n",
    "        r_out, hidden = self.lstm(x, prev_state)\n",
    "        \n",
    "        out = self.fc(r_out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, self.batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a training function\n",
    "# the input is the model, the number of epochs, optimzer, loss function, the dataloader, the sequence length, the batch size\n",
    "\n",
    "def train(model, epochs, optimizer, criterion, dataloader, seq_len, batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        h = model.init_hidden()\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            output, h = model(x, h)\n",
    "            loss = criterion(output.view(batch_size * seq_len, -1), y.view(batch_size * seq_len))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "             \"Loss: {:.4f}...\".format(loss.item()))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate new text\n",
    "# the input is the model, dataset, the size of the vocabulary, the number of characters to generate, the start string\n",
    "\n",
    "def generate(model, dataset, vocab_size, size, start_str):\n",
    "    model.eval()\n",
    "    h = model.init_hidden()\n",
    "    x = torch.tensor([dataset.vocab_to_int[c] for c in start_str]).to(DEVICE)\n",
    "    x = x.view(1,-1)\n",
    "    chars = [c for c in start_str]\n",
    "    \n",
    "    for i in range(size):\n",
    "        h = tuple([e.data for e in h])\n",
    "        output, h = model(x, h)\n",
    "        # p = F.softmax(output, dim=1).data\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "        # top_c = np.random.choice(vocab_size, 1, p=p.numpy().ravel())[0]\n",
    "        chars.append(dataset.int_to_vocab[index.item()])\n",
    "        x = torch.tensor([index.item()]).to(DEVICE)\n",
    "        x = x.view(1,-1)\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 10\n",
    "N_EPOCHS = 100\n",
    "N_HIDDEN = 512\n",
    "N_LAYERS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = SongDataset(songs, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(songdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27,  3, 74, 66, 73, 76, 65, 86,  3,  0]])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(trainloader)\n",
    "inputs, targets = next(batch_iterator)\n",
    "print(inputs)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(N_HIDDEN, N_LAYERS, EMBEDDING_DIM, songdata.vocab_size, BATCH_SIZE, SEQ_LEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Loss: 2.9693...\n",
      "Epoch: 2/100... Loss: 0.6176...\n",
      "Epoch: 3/100... Loss: 0.7451...\n",
      "Epoch: 4/100... Loss: 0.4989...\n",
      "Epoch: 5/100... Loss: 0.3391...\n",
      "Epoch: 6/100... Loss: 0.5361...\n",
      "Epoch: 7/100... Loss: 0.4506...\n",
      "Epoch: 8/100... Loss: 0.7060...\n",
      "Epoch: 9/100... Loss: 0.4936...\n",
      "Epoch: 10/100... Loss: 0.1470...\n",
      "Epoch: 11/100... Loss: 0.3366...\n",
      "Epoch: 12/100... Loss: 0.3389...\n",
      "Epoch: 13/100... Loss: 0.0807...\n",
      "Epoch: 14/100... Loss: 0.0545...\n",
      "Epoch: 15/100... Loss: 0.2508...\n",
      "Epoch: 16/100... Loss: 0.4100...\n",
      "Epoch: 17/100... Loss: 0.4160...\n",
      "Epoch: 18/100... Loss: 0.0096...\n",
      "Epoch: 19/100... Loss: 0.0302...\n",
      "Epoch: 20/100... Loss: 0.1840...\n",
      "Epoch: 21/100... Loss: 0.3082...\n",
      "Epoch: 22/100... Loss: 0.2188...\n",
      "Epoch: 23/100... Loss: 0.4357...\n",
      "Epoch: 24/100... Loss: 0.3181...\n",
      "Epoch: 25/100... Loss: 0.5191...\n",
      "Epoch: 26/100... Loss: 0.1370...\n",
      "Epoch: 27/100... Loss: 0.2739...\n",
      "Epoch: 28/100... Loss: 0.1727...\n",
      "Epoch: 29/100... Loss: 0.6552...\n",
      "Epoch: 30/100... Loss: 0.1421...\n",
      "Epoch: 31/100... Loss: 0.0410...\n",
      "Epoch: 32/100... Loss: 0.2226...\n",
      "Epoch: 33/100... Loss: 0.2329...\n",
      "Epoch: 34/100... Loss: 0.1782...\n",
      "Epoch: 35/100... Loss: 0.5689...\n",
      "Epoch: 36/100... Loss: 0.0903...\n",
      "Epoch: 37/100... Loss: 0.2259...\n",
      "Epoch: 38/100... Loss: 0.2269...\n",
      "Epoch: 39/100... Loss: 0.2476...\n",
      "Epoch: 40/100... Loss: 0.0356...\n",
      "Epoch: 41/100... Loss: 0.2926...\n",
      "Epoch: 42/100... Loss: 0.2741...\n",
      "Epoch: 43/100... Loss: 0.0430...\n",
      "Epoch: 44/100... Loss: 0.0949...\n",
      "Epoch: 45/100... Loss: 0.0083...\n",
      "Epoch: 46/100... Loss: 0.2081...\n",
      "Epoch: 47/100... Loss: 0.3283...\n",
      "Epoch: 48/100... Loss: 0.3279...\n",
      "Epoch: 49/100... Loss: 0.3999...\n",
      "Epoch: 50/100... Loss: 0.0381...\n",
      "Epoch: 51/100... Loss: 0.3362...\n",
      "Epoch: 52/100... Loss: 0.1947...\n",
      "Epoch: 53/100... Loss: 0.1646...\n",
      "Epoch: 54/100... Loss: 0.1505...\n",
      "Epoch: 55/100... Loss: 0.1360...\n",
      "Epoch: 56/100... Loss: 0.1610...\n",
      "Epoch: 57/100... Loss: 0.1942...\n",
      "Epoch: 58/100... Loss: 0.1971...\n",
      "Epoch: 59/100... Loss: 0.0232...\n",
      "Epoch: 60/100... Loss: 0.0216...\n",
      "Epoch: 61/100... Loss: 0.1323...\n",
      "Epoch: 62/100... Loss: 0.1884...\n",
      "Epoch: 63/100... Loss: 0.1721...\n",
      "Epoch: 64/100... Loss: 0.2027...\n",
      "Epoch: 65/100... Loss: 0.6971...\n",
      "Epoch: 66/100... Loss: 0.1562...\n",
      "Epoch: 67/100... Loss: 0.1058...\n",
      "Epoch: 68/100... Loss: 0.2118...\n",
      "Epoch: 69/100... Loss: 0.2835...\n",
      "Epoch: 70/100... Loss: 0.2979...\n",
      "Epoch: 71/100... Loss: 0.1711...\n",
      "Epoch: 72/100... Loss: 0.0896...\n",
      "Epoch: 73/100... Loss: 0.3631...\n",
      "Epoch: 74/100... Loss: 0.3469...\n",
      "Epoch: 75/100... Loss: 0.1162...\n",
      "Epoch: 76/100... Loss: 0.2508...\n",
      "Epoch: 77/100... Loss: 0.0572...\n",
      "Epoch: 78/100... Loss: 0.1481...\n",
      "Epoch: 79/100... Loss: 0.4031...\n",
      "Epoch: 80/100... Loss: 0.0649...\n",
      "Epoch: 81/100... Loss: 0.1175...\n",
      "Epoch: 82/100... Loss: 0.0065...\n",
      "Epoch: 83/100... Loss: 0.0119...\n",
      "Epoch: 84/100... Loss: 0.7776...\n",
      "Epoch: 85/100... Loss: 0.2493...\n",
      "Epoch: 86/100... Loss: 0.3561...\n",
      "Epoch: 87/100... Loss: 0.2818...\n",
      "Epoch: 88/100... Loss: 0.0080...\n",
      "Epoch: 89/100... Loss: 0.1558...\n",
      "Epoch: 90/100... Loss: 0.2176...\n",
      "Epoch: 91/100... Loss: 0.3147...\n",
      "Epoch: 92/100... Loss: 0.2978...\n",
      "Epoch: 93/100... Loss: 0.3851...\n",
      "Epoch: 94/100... Loss: 0.2552...\n",
      "Epoch: 95/100... Loss: 0.1672...\n",
      "Epoch: 96/100... Loss: 0.0103...\n",
      "Epoch: 97/100... Loss: 0.2351...\n",
      "Epoch: 98/100... Loss: 0.0711...\n",
      "Epoch: 99/100... Loss: 0.2881...\n",
      "Epoch: 100/100... Loss: 0.2986...\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(model, N_EPOCHS, optimizer, loss_fn, trainloader, SEQ_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = generate(trained_model, songdata, songdata.vocab_size, 500, \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 0\n",
      "T: Angelus Ad Virginum\n",
      "Z: Public Domain\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "Q: 1/4=120\n",
      "V: P1 name=\"melody\"\n",
      "V: P2 name=\"mlody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: 0\n",
      "T: Angelus Ad Virginum\n",
      "Z: Publus Ad Virginum\n",
      "Z: Public Domain\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "Q: 1/4=120\n",
      "V: P1 name=\"melody\"\n",
      "V: P2 name=\"me=\"melody\"\n",
      "V: P20\n",
      "V: P1 dy\"\n",
      "V: P2 Public Domain\n",
      "L: 1/4=120\n",
      "V: P1 name=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P2 name=\"me=\"melody\"\n",
      "V: P2 name=\"melody\"\n",
      "V: P\n"
     ]
    }
   ],
   "source": [
    "print(song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe294e6606534daba89c1dc14d4f5ef002211ec8430f3955bebe6e14ba2710b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
