{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from tqdm.notebook import tqdm\n",
    "import subprocess\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "\n",
    "    path_list = []\n",
    "    \n",
    "    for c in tqdm(alphabet):\n",
    "        target_path = os.path.join(rootpath + c + '/*.abc')\n",
    "        for path in glob.glob(target_path):\n",
    "            path_list.append(path)\n",
    "\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_data(text):\n",
    "    # extract song from abc notation\n",
    "    # remove header\n",
    "    text = re.sub(r'%%.*', '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n{2,}',r'\\n',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_song_to_abc(song, filename):\n",
    "    save_name = \"/result/{}.abc\".format(filename)\n",
    "    with open(save_name, 'w') as f:\n",
    "        f.write(song)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def abc2wav(abc_file):\n",
    "    suf = abc_file.rsrip('.abc')\n",
    "    cmd = \"abc2midi {} -o {}\".format(abc_file, suf + \".mid\")\n",
    "    os.system(cmd)\n",
    "    cmd = \"timidity {}.mid -Ow {}.wav\".format(suf, suf)\n",
    "\n",
    "    return os.system()\n",
    "\n",
    "def play_wav(wav_file):\n",
    "    return ipythondisplay.Audio(wav_file)\n",
    "\n",
    "def play_song(song):\n",
    "    basename = save_song_to_abc(song)\n",
    "    ret = abc2wav(basename + \".abc\")\n",
    "    if ret == 0:\n",
    "        return play_wav(basename + \".wav\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c616d3e06bf242cf99fa7495407554d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "rootpath = cwd + '/notation/'\n",
    "notation_list = make_datapath_list(rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70448b31e0c240c5848bda9c4c120585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs = []\n",
    "\n",
    "for notation in tqdm(notation_list):\n",
    "    with open(notation, 'r') as f:\n",
    "        text = f.read()\n",
    "        song = extract_song_data(text)\n",
    "        songs.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example song: \n",
      "X: 0\n",
      "T: Austria\n",
      "Z: Franz Joseph Hayden, 1797\n",
      "Z: Public  domain\n",
      "L: 1/4\n",
      "M: 4/4\n",
      "V: P1 name=\"Unnamed-000\"\n",
      "V: P2 name=\"Unnamed-001\"\n",
      "V: P3 name=\"Tempo Track\"\n",
      "K: Eb\n",
      "[V: P1]  [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,F] [B,G] [B,/F/]D/ B, | [C/A/]D/ [EG] [D/F/][B,/D/] B, | [B,B] [C/A/]D/ [E3/G3/] [E/G/] | [E3/=A3/] [E/A/] [D2B2] | [E3/e3/] [E/d/] [E/d/]c/ [EB] | [E3/c3/] [E/B/] [D/B/]A/ [EG] | [DF] [D/G/]A/ [E/B/]c/ [C/A/]F/ | [B,E] [D/G/]F/ E2- | E2z2|]\n",
      "[V: P2]  [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [B,,D,] [B,,E,] [B,,/D,/]F,/ [B,,/A,/]G,/ | [B,,F,] [B,,/E,/]G,/ [B,,/B,/]F,/ [B,,/D,/]A,,/ | [G,,G,] [A,,/F,/]B,,/ [C,3/E,3/] [C,/C/] | [F,3/C3/] [F,,/F,/] [B,,2F,2] | [G,3/C3/] [G,/B,/] A, [E,G,] | A,3/ [G,/B,/] [F,B,] [E,B,] | [B,,B,] [A,,B,] [G,,/B,/]E,/ [A,,/C/]A,/ | [B,,G,] [B,,A,] [E,,2-E,2-G,2-] | [E,,2E,2G,2]z2|]\n",
      "[V: P3]  z4|]\n"
     ]
    }
   ],
   "source": [
    "example_song = songs[119]\n",
    "print(\"Example song: \")\n",
    "print(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to read data\n",
    "# this class is used to read data from a list of songs and return a data\n",
    "# the input is a list of songs, sequence length, batch size\n",
    "# the output is a batch of data\n",
    "\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, songs, seq_len):\n",
    "        self.songs = songs\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = set()\n",
    "        self.vocab_to_int = {}\n",
    "        self.int_to_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.song_index = 0\n",
    "        self.song = self.songs[self.song_index]\n",
    "        self.song_len = len(self.song)\n",
    "        self.song_index += 1\n",
    "        self.create_vocab()\n",
    "        self.create_int()\n",
    "        self.n_batch = int(self.song_len / self.seq_len)\n",
    "        \n",
    "    def create_vocab(self):\n",
    "        for song in self.songs:\n",
    "            self.vocab = self.vocab.union(set(song))\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_int(self):\n",
    "        self.vocab_to_int = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = dict(enumerate(self.vocab))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = np.zeros((self.seq_len))\n",
    "        y = np.zeros((self.seq_len))\n",
    "\n",
    "        for i in range(self.seq_len):\n",
    "            x[i] = self.vocab_to_int[self.song[index * self.seq_len + i]]\n",
    "            y[i] = self.vocab_to_int[self.song[index * self.seq_len + i + 1]]\n",
    "            \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to build a RNN model\n",
    "# the input is the number of hidden units, the number of layers, the size of the vocabulary, the batch size, the sequence length\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers, embedding_dim, vocab_size, batch_size, seq_len):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = self.embed(x)\n",
    "        r_out, (hidden, cell) = self.lstm(x, (hidden,cell))\n",
    "\n",
    "        r_out = r_out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        out = self.fc(r_out)\n",
    "        \n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = torch.zeros(self.n_layers, self.batch_size, self.n_hidden).cuda()\n",
    "        cell = torch.zeros(self.n_layers, self.batch_size, self.n_hidden).cuda()\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a training function\n",
    "# the input is the model, the number of epochs, optimzer, loss function, the dataloader, the sequence length, the batch size\n",
    "\n",
    "def train(model, epochs, optimizer, criterion, dataloader, seq_len, batch_size):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        h, c = model.init_hidden()\n",
    "\n",
    "        print(h.shape)\n",
    "        \n",
    "        for x, y in dataloader:\n",
    "            x = x.long().to(DEVICE)\n",
    "            y = y.long().to(DEVICE)\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            output, h, c = model(x, h, c)\n",
    "            \n",
    "            loss = criterion(output.view(batch_size * seq_len, -1), y.view(batch_size * seq_len))\n",
    "            # loss = criterion(output, y)\n",
    "            loss_value=loss.item()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(epoch + 1, epochs),\n",
    "                \"Loss: {:.4f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate new text\n",
    "# the input is the model, dataset, the size of the vocabulary, the number of characters to generate, the start string\n",
    "\n",
    "def generate(model, dataset, vocab_size, size, start='X'):\n",
    "    model.eval()\n",
    "    chars = [ch for ch in start]\n",
    "    hidden = model.init_hidden()\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = dataset.vocab_to_int[start]\n",
    "    x = torch.from_numpy(x).long()\n",
    "    \n",
    "    for i in range(size):\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        output, hidden = model(x, hidden)\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if dataset.int_to_vocab[int(torch.max(p, 1)[1])] == 'E':\n",
    "            break\n",
    "        else:\n",
    "            chars.append(dataset.int_to_vocab[int(torch.max(p, 1)[1])])\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = dataset.vocab_to_int[dataset.int_to_vocab[int(torch.max(p, 1)[1])]]\n",
    "            x = torch.from_numpy(x).long()\n",
    "            \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 10\n",
    "N_EPOCHS = 100\n",
    "N_HIDDEN = 512\n",
    "N_LAYERS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = SongDataset(songs, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(songdata, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57., 37., 10., 17., 13., 34., 17., 13., 58.,  1.],\n",
      "        [37., 13.,  1., 88.,  1., 57., 43., 24.,  1., 20.],\n",
      "        [57., 37., 10., 34., 58.,  1., 57., 37., 10., 13.],\n",
      "        [74., 62., 70., 75.,  0., 42., 24.,  1., 15., 13.],\n",
      "        [54., 24.,  1., 14.,  0., 50., 24.,  1., 31., 75.],\n",
      "        [10., 34., 58.,  1., 57., 37., 10., 13., 34., 13.],\n",
      "        [58.,  1., 57., 37., 10., 34., 58.,  1., 57., 37.],\n",
      "        [13., 34., 17., 13., 58.,  1., 88.,  1., 57., 37.],\n",
      "        [13., 37., 13.,  1., 31.,  1., 34., 13.,  1., 88.],\n",
      "        [43., 24.,  1., 23., 13., 22., 58.,  1.,  1., 31.],\n",
      "        [27.,  3., 74., 66., 73., 76., 65., 86.,  3.,  0.],\n",
      "        [24.,  1., 20., 13., 22., 58.,  1.,  1., 57., 37.],\n",
      "        [13., 34., 17., 13., 58.,  1., 57., 37., 10., 17.],\n",
      "        [13.,  1., 88.,  1., 31., 13., 37., 13., 31., 13.],\n",
      "        [88.,  1., 57., 37., 10., 34., 58.,  1., 57., 37.],\n",
      "        [10., 17., 13., 34., 17., 13., 58.,  1., 88.,  1.]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(trainloader)\n",
    "inputs, targets = next(batch_iterator)\n",
    "print(inputs)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(N_HIDDEN, N_LAYERS, EMBEDDING_DIM, songdata.vocab_size, BATCH_SIZE, SEQ_LEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2048, 256]] is at version 3; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yuwon/workspace/NN/Assignment5/RNN.ipynb 셀 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, N_EPOCHS, optimizer, loss_fn, trainloader, SEQ_LEN, BATCH_SIZE)\n",
      "\u001b[1;32m/home/yuwon/workspace/NN/Assignment5/RNN.ipynb 셀 18\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, optimizer, criterion, dataloader, seq_len, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# loss = criterion(output, y)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss_value\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2048, 256]] is at version 3; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "train(model, N_EPOCHS, optimizer, loss_fn, trainloader, SEQ_LEN, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe294e6606534daba89c1dc14d4f5ef002211ec8430f3955bebe6e14ba2710b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
