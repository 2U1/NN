{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from tqdm.notebook import tqdm\n",
    "import subprocess\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "\n",
    "    path_list = []\n",
    "    \n",
    "    for c in tqdm(alphabet):\n",
    "        target_path = os.path.join(rootpath + c + '/*.abc')\n",
    "        for path in glob.glob(target_path):\n",
    "            path_list.append(path)\n",
    "\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_data(text):\n",
    "    # extract song from abc notation\n",
    "    # remove header\n",
    "    text = re.sub(r'%%.*', '', text)\n",
    "    text = re.sub(r'Z:.*\\n', '', text)\n",
    "    text = re.sub(r'K:.*\\n', '', text)\n",
    "    text = re.sub(r'Q:.*\\n', '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n{2,}',r'\\n',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28034662e7c845868bd6a8a89b2ff00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "rootpath = cwd + '/notation/'\n",
    "notation_list = make_datapath_list(rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464f3e30f2fd4384b299c86742f86175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs = []\n",
    "\n",
    "for notation in tqdm(notation_list):\n",
    "    with open(notation, 'r') as f:\n",
    "        text = f.read()\n",
    "        song = extract_song_data(text)\n",
    "        songs.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example song: \n",
      "X: 0\n",
      "T: Austria\n",
      "L: 1/4\n",
      "M: 4/4\n",
      "V: P1 name=\"Unnamed-000\"\n",
      "V: P2 name=\"Unnamed-001\"\n",
      "V: P3 name=\"Tempo Track\"\n",
      "[V: P1]  [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,F] [B,G] [B,/F/]D/ B, | [C/A/]D/ [EG] [D/F/][B,/D/] B, | [B,B] [C/A/]D/ [E3/G3/] [E/G/] | [E3/=A3/] [E/A/] [D2B2] | [E3/e3/] [E/d/] [E/d/]c/ [EB] | [E3/c3/] [E/B/] [D/B/]A/ [EG] | [DF] [D/G/]A/ [E/B/]c/ [C/A/]F/ | [B,E] [D/G/]F/ E2- | E2z2|]\n",
      "[V: P2]  [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [B,,D,] [B,,E,] [B,,/D,/]F,/ [B,,/A,/]G,/ | [B,,F,] [B,,/E,/]G,/ [B,,/B,/]F,/ [B,,/D,/]A,,/ | [G,,G,] [A,,/F,/]B,,/ [C,3/E,3/] [C,/C/] | [F,3/C3/] [F,,/F,/] [B,,2F,2] | [G,3/C3/] [G,/B,/] A, [E,G,] | A,3/ [G,/B,/] [F,B,] [E,B,] | [B,,B,] [A,,B,] [G,,/B,/]E,/ [A,,/C/]A,/ | [B,,G,] [B,,A,] [E,,2-E,2-G,2-] | [E,,2E,2G,2]z2|]\n",
      "[V: P3]  z4|]\n"
     ]
    }
   ],
   "source": [
    "example_song = songs[119]\n",
    "print(\"Example song: \")\n",
    "print(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to read data\n",
    "# this class is used to read data from a list of songs and return a data\n",
    "# the input is a list of songs, sequence length, batch size\n",
    "# the output is a batch of data\n",
    "\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, songs, seq_len):\n",
    "        self.songs = '\\n\\n'.join(songs)\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = set()\n",
    "        self.vocab_to_int = {}\n",
    "        self.int_to_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.song_len = len(self.songs)\n",
    "        self.create_vocab()\n",
    "        self.create_int()\n",
    "        self.n_batch = int(self.song_len / self.seq_len)\n",
    "        \n",
    "    def create_vocab(self):\n",
    "        self.vocab = set(self.songs)\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_int(self):\n",
    "        self.vocab_to_int = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = dict(enumerate(self.vocab))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index = index * self.seq_len\n",
    "        x_str = self.songs[index : index + self.seq_len]\n",
    "        y_str = self.songs[index+1 : index + self.seq_len + 1]\n",
    "\n",
    "        x = torch.tensor([self.vocab_to_int[c] for c in x_str])\n",
    "        y = torch.tensor([self.vocab_to_int[c] for c in y_str])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to build a RNN model\n",
    "# the input is the number of hidden units, the number of layers, the size of the vocabulary, the batch size, the sequence length\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers, embedding_dim, vocab_size, batch_size, seq_len):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        x = self.embed(x)\n",
    "        r_out, hidden = self.lstm(x, prev_state)\n",
    "        \n",
    "        out = self.fc(r_out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, self.batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a training function\n",
    "# the input is the model, the number of epochs, optimzer, loss function, the dataloader, the sequence length, the batch size\n",
    "\n",
    "def train(model, epochs, optimizer, criterion, dataloader, seq_len, batch_size):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        h = model.init_hidden()\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            output, h = model(x, h)\n",
    "            loss = criterion(output.view(batch_size * seq_len, -1), y.view(batch_size * seq_len))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "            \"Loss: {:.4f}...\".format(loss.item()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate new text\n",
    "# the input is the model, dataset, the size of the vocabulary, the number of characters to generate, the start string\n",
    "\n",
    "def generate(model, dataset, vocab_size, size, start_str):\n",
    "    model.eval()\n",
    "    h = model.init_hidden()\n",
    "    x = torch.tensor([dataset.vocab_to_int[c] for c in start_str]).to(DEVICE)\n",
    "    x = x.view(1,-1)\n",
    "    chars = [c for c in start_str]\n",
    "    \n",
    "    for i in range(size):\n",
    "        h = tuple([e.data for e in h])\n",
    "        output, h = model(x, h)\n",
    "        # p = F.softmax(output, dim=1).data\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "        # top_c = np.random.choice(vocab_size, 1, p=p.numpy().ravel())[0]\n",
    "        chars.append(dataset.int_to_vocab[index.item()])\n",
    "        x = torch.tensor([index.item()]).to(DEVICE)\n",
    "        x = x.view(1,-1)\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 10\n",
    "N_EPOCHS = 100\n",
    "N_HIDDEN = 512\n",
    "N_LAYERS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = SongDataset(songs, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(songdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22,  1, 30, 10, 13, 22, 11,  1, 30, 10]])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(trainloader)\n",
    "inputs, targets = next(batch_iterator)\n",
    "print(inputs)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(N_HIDDEN, N_LAYERS, EMBEDDING_DIM, songdata.vocab_size, BATCH_SIZE, SEQ_LEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1770d62735e47c88c3f7ea0202851b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = train(model, N_EPOCHS, optimizer, loss_fn, trainloader, SEQ_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = generate(trained_model, songdata, songdata.vocab_size, 500, \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 0\n",
      "T: Angelus Ad Virginum\n",
      "L: Ad Virginum\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: Angelus Ad Virginum\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "M: 6/8\n",
      "V: P1 name=\" Virginum\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "Virginum\n",
      "L: 1/4\n",
      "M: 6\n"
     ]
    }
   ],
   "source": [
    "print(song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe294e6606534daba89c1dc14d4f5ef002211ec8430f3955bebe6e14ba2710b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
