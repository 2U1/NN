{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import functools\n",
    "from tqdm.notebook import tqdm\n",
    "import subprocess\n",
    "import glob\n",
    "from IPython import display as ipythondisplay\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    \n",
    "\n",
    "    path_list = []\n",
    "    \n",
    "    for c in tqdm(alphabet):\n",
    "        target_path = os.path.join(rootpath + c + '/*.abc')\n",
    "        for path in glob.glob(target_path):\n",
    "            path_list.append(path)\n",
    "\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_song_data(text):\n",
    "    # extract song from abc notation\n",
    "    # remove header\n",
    "    text = re.sub(r'%%.*', '', text)\n",
    "    text = re.sub(r'Z:.*\\n', '', text)\n",
    "    text = re.sub(r'K:.*\\n', '', text)\n",
    "    text = re.sub(r'Q:.*\\n', '', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\n{2,}',r'\\n',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72fa554e018411cbd84e4577e75b6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "rootpath = cwd + '/notation/'\n",
    "notation_list = make_datapath_list(rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d6223fc99c4915a7227de7326376dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "songs = []\n",
    "\n",
    "for notation in tqdm(notation_list):\n",
    "    with open(notation, 'r') as f:\n",
    "        text = f.read()\n",
    "        song = extract_song_data(text)\n",
    "        songs.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example song: \n",
      "X: 0\n",
      "T: Austria\n",
      "L: 1/4\n",
      "M: 4/4\n",
      "V: P1 name=\"Unnamed-000\"\n",
      "V: P2 name=\"Unnamed-001\"\n",
      "V: P3 name=\"Tempo Track\"\n",
      "[V: P1]  [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,3/E3/] [B,/F/] [EG] [DF] | [FA] [EG] [D/F/]D/ [B,E] | [C/c/]D/ [EB] [FA] [EG] | [CF] [E/G/]E/ [D2B2] | [B,F] [B,G] [B,/F/]D/ B, | [C/A/]D/ [EG] [D/F/][B,/D/] B, | [B,B] [C/A/]D/ [E3/G3/] [E/G/] | [E3/=A3/] [E/A/] [D2B2] | [E3/e3/] [E/d/] [E/d/]c/ [EB] | [E3/c3/] [E/B/] [D/B/]A/ [EG] | [DF] [D/G/]A/ [E/B/]c/ [C/A/]F/ | [B,E] [D/G/]F/ E2- | E2z2|]\n",
      "[V: P2]  [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [E,3/G,3/] [E,/A,/] [E,B,] [B,,B,] | [D,B,] [E,B,] [B,,A,] [E,G,] | A, [G,B,] [D,B,] [E,B,] | [A,,A,] [=A,,C] [B,,2F,2] | [B,,D,] [B,,E,] [B,,/D,/]F,/ [B,,/A,/]G,/ | [B,,F,] [B,,/E,/]G,/ [B,,/B,/]F,/ [B,,/D,/]A,,/ | [G,,G,] [A,,/F,/]B,,/ [C,3/E,3/] [C,/C/] | [F,3/C3/] [F,,/F,/] [B,,2F,2] | [G,3/C3/] [G,/B,/] A, [E,G,] | A,3/ [G,/B,/] [F,B,] [E,B,] | [B,,B,] [A,,B,] [G,,/B,/]E,/ [A,,/C/]A,/ | [B,,G,] [B,,A,] [E,,2-E,2-G,2-] | [E,,2E,2G,2]z2|]\n",
      "[V: P3]  z4|]\n"
     ]
    }
   ],
   "source": [
    "example_song = songs[119]\n",
    "print(\"Example song: \")\n",
    "print(example_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to read data\n",
    "# this class is used to read data from a list of songs and return a data\n",
    "# the input is a list of songs, sequence length, batch size\n",
    "# the output is a batch of data\n",
    "\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, songs, seq_len):\n",
    "        self.songs = '\\n\\n'.join(songs)\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = set()\n",
    "        self.vocab_to_int = {}\n",
    "        self.int_to_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.song_len = len(self.songs)\n",
    "        self.create_vocab()\n",
    "        self.create_int()\n",
    "        self.n_batch = int(self.song_len / self.seq_len)\n",
    "        \n",
    "    def create_vocab(self):\n",
    "        self.vocab = set(self.songs)\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def create_int(self):\n",
    "        self.vocab_to_int = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int_to_vocab = dict(enumerate(self.vocab))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index = index * self.seq_len\n",
    "        x_str = self.songs[index : index + self.seq_len]\n",
    "        y_str = self.songs[index+1 : index + self.seq_len + 1]\n",
    "        \n",
    "        if len(x_str) < self.seq_len:\n",
    "            x_str = x_str + ' ' * (self.seq_len - len(x_str))\n",
    "\n",
    "        if len(y_str) < self.seq_len:\n",
    "            y_str = y_str + ' ' * (self.seq_len - len(y_str))\n",
    "        \n",
    "\n",
    "        x = torch.tensor([self.vocab_to_int[c] for c in x_str])\n",
    "        y = torch.tensor([self.vocab_to_int[c] for c in y_str])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class to build a RNN model\n",
    "# the input is the number of hidden units, the number of layers, the size of the vocabulary, the batch size, the sequence length\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_layers, embedding_dim, vocab_size, batch_size, seq_len):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed = nn.Embedding(self.vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.n_hidden, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        x = self.embed(x)\n",
    "        r_out, hidden = self.lstm(x, prev_state)\n",
    "        \n",
    "        out = self.fc(r_out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, self.batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, self.batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a training function\n",
    "# the input is the model, the number of epochs, optimzer, loss function, the dataloader, the sequence length, the batch size\n",
    "\n",
    "def train(model, epochs, optimizer, criterion, dataloader, seq_len, batch_size):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        h = model.init_hidden()\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            h = tuple([e.data for e in h])\n",
    "            model.zero_grad()\n",
    "            output, h = model(x, h)\n",
    "            loss = criterion(output.view(batch_size * seq_len, -1), y.view(batch_size * seq_len))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "            \"Loss: {:.4f}...\".format(loss.item()))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate new text\n",
    "# the input is the model, dataset, the size of the vocabulary, the number of characters to generate, the start string\n",
    "\n",
    "def generate(model, dataset, vocab_size, size, start_str):\n",
    "    model.eval()\n",
    "    h = model.init_hidden()\n",
    "    x = torch.tensor([dataset.vocab_to_int[c] for c in start_str]).to(DEVICE)\n",
    "    x = x.view(1,-1)\n",
    "    chars = [c for c in start_str]\n",
    "    \n",
    "    for i in range(size):\n",
    "        h = tuple([e.data for e in h])\n",
    "        output, h = model(x, h)\n",
    "        # p = F.softmax(output, dim=1).data\n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample()\n",
    "        # top_c = np.random.choice(vocab_size, 1, p=p.numpy().ravel())[0]\n",
    "        chars.append(dataset.int_to_vocab[index.item()])\n",
    "        x = torch.tensor([index.item()]).to(DEVICE)\n",
    "        x = x.view(1,-1)\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 10\n",
    "N_EPOCHS = 100\n",
    "N_HIDDEN = 512\n",
    "N_LAYERS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = SongDataset(songs, SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(songdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 57, 56, 33, 10, 10, 13, 22, 35, 10],\n",
      "        [32, 57, 86,  1, 56, 35, 10, 17, 13, 18],\n",
      "        [83, 74, 10,  1, 49, 68, 65,  1, 30, 74],\n",
      "        [34, 10, 13,  1, 33, 10, 13,  1, 87,  1],\n",
      "        [10,  1, 31, 10, 10,  1, 87,  1, 30, 10],\n",
      "        [61, 67, 65,  1, 62, 78, 69, 74, 67, 11],\n",
      "        [31, 10, 10,  1, 87,  1, 34, 10, 17, 87],\n",
      "        [35, 17, 13, 63, 17, 13, 57,  1, 56, 34],\n",
      "        [10, 13, 18,  1, 32, 10,  1, 35, 10,  1],\n",
      "        [57,  1, 87,  1, 56, 66, 64,  7, 57,  1],\n",
      "        [ 1, 76, 61, 69, 78,  1, 50, 74, 11,  1],\n",
      "        [13, 18, 11, 32, 17, 13, 18, 11, 34, 17],\n",
      "        [86,  1, 56, 35, 36, 58, 30, 57, 86, 16],\n",
      "        [10, 13,  1, 87,  1,  8, 30, 10, 13, 33],\n",
      "        [ 0, 56, 51, 24,  1, 45, 16, 57,  1,  1],\n",
      "        [ 0, 51, 24,  1, 45, 16,  1, 74, 61, 73]])\n",
      "torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_iterator = iter(trainloader)\n",
    "inputs, targets = next(batch_iterator)\n",
    "print(inputs)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(N_HIDDEN, N_LAYERS, EMBEDDING_DIM, songdata.vocab_size, BATCH_SIZE, SEQ_LEN)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8196f1a3efc46cda7f190aab4e261d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (3, 14, 512), got [3, 16, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/yuwon/workspace/NN/Assignment5/RNN.ipynb 셀 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m train(model, N_EPOCHS, optimizer, loss_fn, trainloader, SEQ_LEN, BATCH_SIZE)\n",
      "\u001b[1;32m/home/yuwon/workspace/NN/Assignment5/RNN.ipynb 셀 17\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, optimizer, criterion, dataloader, seq_len, batch_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([e\u001b[39m.\u001b[39mdata \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m h])\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output, h \u001b[39m=\u001b[39m model(x, h)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m seq_len, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), y\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m seq_len))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/yuwon/workspace/NN/Assignment5/RNN.ipynb 셀 17\u001b[0m in \u001b[0;36mRNN.forward\u001b[0;34m(self, x, prev_state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, prev_state):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     r_out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, prev_state)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(r_out)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/yuwon/workspace/NN/Assignment5/RNN.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out, hidden\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/nn/modules/rnn.py:759\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[39m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[39m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 759\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    760\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    762\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/nn/modules/rnn.py:685\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m,  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    680\u001b[0m                        \u001b[39minput\u001b[39m: Tensor,\n\u001b[1;32m    681\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    682\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    683\u001b[0m                        ):\n\u001b[1;32m    684\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 685\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_hidden_size(hidden[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_expected_hidden_size(\u001b[39minput\u001b[39;49m, batch_sizes),\n\u001b[1;32m    686\u001b[0m                            \u001b[39m'\u001b[39;49m\u001b[39mExpected hidden[0] size \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, got \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    687\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_cell_size(\u001b[39minput\u001b[39m, batch_sizes),\n\u001b[1;32m    688\u001b[0m                            \u001b[39m'\u001b[39m\u001b[39mExpected hidden[1] size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL/lib/python3.9/site-packages/torch/nn/modules/rnn.py:226\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_hidden_size\u001b[39m(\u001b[39mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m],\n\u001b[1;32m    224\u001b[0m                       msg: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mExpected hidden size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m hx\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(expected_hidden_size, \u001b[39mlist\u001b[39m(hx\u001b[39m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (3, 14, 512), got [3, 16, 512]"
     ]
    }
   ],
   "source": [
    "trained_model = train(model, N_EPOCHS, optimizer, loss_fn, trainloader, SEQ_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = generate(trained_model, songdata, songdata.vocab_size, 500, \"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 0\n",
      "T: Angelus Ad Virginum\n",
      "L: Ad Virginum\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: Angelus Ad Virginum\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "M: 6/8\n",
      "V: P1 name=\" Virginum\n",
      "L: 1/4\n",
      "M: 6/8\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "V: P1 name=\"melody\"\n",
      "Virginum\n",
      "L: 1/4\n",
      "M: 6\n"
     ]
    }
   ],
   "source": [
    "print(song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe294e6606534daba89c1dc14d4f5ef002211ec8430f3955bebe6e14ba2710b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
