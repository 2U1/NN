{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will take a look at convolutional neural networks and implement some examples to get a better feeling of how they work.\n",
    "\n",
    "As we have discussed, a CNN is defined by banks of filters that are passed (convolved) over the image. The output of the filters is then typically pooled to reduce the spatial dimension, and then passed into another set of filters.\n",
    "\n",
    "The first such network was the famous LeNet, built by Yann LeCun in the late 1980s: \n",
    "\n",
    "![LeNet](lenet.png)\n",
    "\n",
    "The network that re-ignited the interest in CNNs was the AlexNet built in 2012:\n",
    "\n",
    "![AlexNet](alexnet.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying objects\n",
    "\n",
    "In the following, we will use the old-school CIFAR-10 dataset that contains low-resolution pictures of objects of 10 categories. This - and other - dataset is available as part of the `torchvision` package, which you should install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms, utils\n",
    "# where to put the data\n",
    "data_path = './'\n",
    "\n",
    "# this constructs a CIFAR10 dataset, selects the training part of it, \n",
    "# downloads it if necessary, and adds additional transforms that we \n",
    "# will need to convert each image to a pytorch tensor AND to provide\n",
    "# a nice conversion of the RGB images into greyscale\n",
    "cifar10 = ...(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has now downloaded the images if not already done so - you will notice that this is quite a \"hefty\" dataset already at 170MB. Now let's download our validation set or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = ...(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-class problem\n",
    "\n",
    "Let's make it simple first and classify just two classes for fun. We will be creating two smaller versions of the datasets and remap the label numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-class problem with fully-connected networks\n",
    "\n",
    "Let's make it simple again and try to train a standard, fully-connected neural network. We know that we need some hidden layer, so let's get one with 512 neurons and connect that to an output layer that uses the `LogSoftmax` final output from Pytorch, which - when combined with a `NLLLoss` gives us exactly cross entropy.\n",
    "\n",
    "This is a bit confusingly defined in pytorch, and most models - as we will see below - do this in a different way to make it easier to set up the classification problem with a cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = ...\n",
    "\n",
    "numel_list = [p.numel() for p in first_model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "This is a very nice convenience function that allows you to stick a properly prepared dataset and automatically use it to load batches from it for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ...(cifar2, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now let's define our training loop combining the softmax loss with a negative log likelihood loss to get our overall cross-entropy type classification going:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(first_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = ...\n",
    "        loss = ...\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to \"nicely\" reduce the loss. But of course this loss is not the same as accuracy as we discussed before. So, let's evaluate the accuracy on our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = ...\n",
    "        _, predicted = ...\n",
    "        total += ...\n",
    "        correct += ...\n",
    "\n",
    "print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the run, this will result in a performance of around 80% - certainly not perfect, but also not fully random. \n",
    "\n",
    "Can we do better? Of course, let's go deep! More parameters, more layers, more features that can be discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "loss_fn = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we have a model that goes nicely deeper, reducing the number of hidden layer neurons by factors of two each time and culminating in the two-outputs for the neurons.\n",
    "\n",
    "We have also now included a linear layer as our final output and directly use the cross-entropy loss function. When doing so, the only downside is that we cannot of course interpret the outputs of our final, linear layer as probabilities for class membership (as is possible in our previous architecture!).\n",
    "\n",
    "How many parameters do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a LOT. Over 3 million parameters to train. And of course the first hidden layer with 1024 hidden neurons has the most parameters, as it connects each pixel (3072) to each neuron. \n",
    "\n",
    "So that's of course 3072 * 1024 + 1024 parameters, which we can check directly like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this biggy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(connected_model.parameters(), lr=learning_rate)\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = ...\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = ...\n",
    "        _, predicted = ...\n",
    "        total += ...\n",
    "        correct += ...\n",
    "\n",
    "print(\"Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not achieved a massive reduction in validation error - even though in most cases, we will be able to reduce the **training** error to 0. \n",
    "\n",
    "So, we are massively overfitting our training set, apparently, producing a model that can perfectly \"remember\" all the relationships between pixels for the training data, but fails to generalize properly to the validation/test data.\n",
    "\n",
    "Why is this again?\n",
    "\n",
    "It is because our network, for example, cannot deal with something as simple as a translation of image content. Our network has been trained to capture the distribution of the exact locations of pixels in the image. Even a slight shift will result in something unpredictable.\n",
    "\n",
    "So, this motivated the idea of using networks with convolutional filters, where smaller filters are shared across the whole image.\n",
    "\n",
    "## Convolutions in Pytorch\n",
    "\n",
    "Let's first take a look at how convolutions are defined in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ...\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This as we can see is a **layer**, which uses 16 filters, each of size 3x3 pixels on a three-channel image. The stride is set to 1 in both x and y-direction, and there is **no padding**.\n",
    "\n",
    "The number of learnable parameters should hence be 16 filters * 3 color channels * 3x3 pixels + 16 biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's convolve the first image of CIFAR with our layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can show the output and the input like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4.8)) \n",
    "ax1 = plt.subplot(1, 2, 1)  \n",
    "plt.title('output') \n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)\n",
    "plt.imshow(img.mean(0), cmap='gray')\n",
    "plt.title('input')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note: first of all, weights and biases of the convolution layer have been randomly initialized, so the output is not going to be very meaningful.\n",
    "\n",
    "Second of all, as is easily noticable, the dimensionality of input and output is not the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is of course that we lose pixels around the edge due to the convolution operation. The solution then is to add padding to the image. Since, we have a kernel size of 3x3, we need a padding of 1 pixel.\n",
    "\n",
    "Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ...\n",
    "output = ...\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course force the filters to a certain value. Let's do this manually, setting the biases to 0 and the weights to normalized similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it's pretty clear what this will give us, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.figure(figsize=(10, 4.8))  \n",
    "ax1 = plt.subplot(1, 2, 1)   \n",
    "plt.title('output')   \n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  \n",
    "plt.imshow(img.mean(0), cmap='gray')  \n",
    "plt.title('input')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a bit of edge detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.figure(figsize=(10, 4.8))  \n",
    "ax1 = plt.subplot(1, 2, 1)   \n",
    "plt.title('output')   \n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  \n",
    "plt.imshow(img.mean(0), cmap='gray')  \n",
    "plt.title('input')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of convolution filter layers\n",
    "\n",
    "If we assume that our convolution written as an operator $*$ works on an input $X$ to produce an output $O$ using a filter $F$ like so: \n",
    "\n",
    "$$\n",
    "O = F * X\n",
    "$$\n",
    "\n",
    "\n",
    "where we assume that our filter $F$ is square with $n$ elements and thus we have:\n",
    "\n",
    "$$\n",
    "O_{i,j}=\\sum_{k=-n}^{n} \\sum_{l=-n}^{n} F_{k,l}X_{i+k,j+l}\n",
    "$$\n",
    "\n",
    "In order to do the backpropagation, we are given the loss $\\frac{\\partial L}{\\partial O}$ from a previous layer. We now have to push this through the convolution to get two things:\n",
    "\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "To make matters more \"visible\", we are going to use a simple example of a 3x3 input matrix $X$ and a 2x2 filter $F$. So, for the first element of the output $O_{1,1}$, we get:\n",
    "\n",
    "$$\n",
    "O_{1,1}=X_{1,1}F_{1,1}+X_{1,2}F_{1,2}+X_{2,1}F_{2,1}+X_{2,2}F_{2,2}\n",
    "$$\n",
    "\n",
    "and, similarly, for the remaining elements:\n",
    "\n",
    "$$\n",
    "O_{1,2}=X_{1,2}F_{1,1}+X_{1,3}F_{1,2}+X_{2,2}F_{2,1}+X_{2,3}F_{2,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "O_{2,1}=X_{2,1}F_{1,1}+X_{2,2}F_{1,2}+X_{3,1}F_{2,1}+X_{3,2}F_{2,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "O_{2,2}=X_{2,2}F_{1,1}+X_{2,3}F_{1,2}+X_{3,2}F_{2,1}+X_{3,3}F_{2,2}\n",
    "$$\n",
    "\n",
    "\n",
    "### Finding $\\frac{\\partial L}{\\partial F}$\n",
    "So, first, the partial derivative to update our filter $F$ can be expanded using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F} = \\frac{\\partial L}{\\partial O} \\cdot \\frac{\\partial O}{\\partial F}\n",
    "$$\n",
    "\n",
    "As written above, both $O$ and $F$ are of course matrices - when doing this, we can expand the derivatives like so, for every element of $F$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_i} = \\sum_{k=1}^{n} \\frac{\\partial L}{\\partial O_k} \\cdot \\frac{\\partial O_k}{\\partial F_i}\n",
    "$$\n",
    "\n",
    "\n",
    "now we can take the derivatives with respect to each filter coefficent $F_{i,j}$ to find:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial O_{1,1}}{\\partial F_{i,j}}=X_{i,j}\n",
    "$$\n",
    "\n",
    "Watch out though, the derivatives for the next element is not \"that easy\" anymore:\n",
    "\n",
    "$$\n",
    "O_{1,2}=X_{1,2}F_{1,1}+X_{1,3}F_{1,2}+X_{2,2}F_{2,1}+X_{2,3}F_{2,2}\n",
    "$$\n",
    "\n",
    "For this, the derivatives with respect to each filter coefficent $F_{i,j}$ yields different elements of the input matrix!\n",
    "\n",
    "\n",
    "so, now we need to go into the matrix derivatives. Using the definition of the matrix derivative, we get for the following four partial derivatives:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{1,1}}=\\frac{\\partial L}{\\partial O_{1,1}}\\frac{\\partial O_{1,1}}{\\partial F_{1,1}}+\\frac{\\partial L}{\\partial O_{1,2}}\\frac{\\partial O_{1,2}}{\\partial F_{1,1}}+\\frac{\\partial L}{\\partial O_{2,1}}\\frac{\\partial O_{2,1}}{\\partial F_{1,1}}+\\frac{\\partial L}{\\partial O_{2,2}}\\frac{\\partial O_{2,2}}{\\partial F_{1,1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{1,2}}=\\frac{\\partial L}{\\partial O_{1,1}}\\frac{\\partial O_{1,1}}{\\partial F_{1,2}}+\\frac{\\partial L}{\\partial O_{1,2}}\\frac{\\partial O_{1,2}}{\\partial F_{1,2}}+\\frac{\\partial L}{\\partial O_{2,1}}\\frac{\\partial O_{2,1}}{\\partial F_{1,2}}+\\frac{\\partial L}{\\partial O_{2,2}}\\frac{\\partial O_{2,2}}{\\partial F_{1,2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{2,1}}=\\frac{\\partial L}{\\partial O_{1,1}}\\frac{\\partial O_{1,1}}{\\partial F_{2,1}}+\\frac{\\partial L}{\\partial O_{1,2}}\\frac{\\partial O_{1,2}}{\\partial F_{2,1}}+\\frac{\\partial L}{\\partial O_{2,1}}\\frac{\\partial O_{2,1}}{\\partial F_{2,1}}+\\frac{\\partial L}{\\partial O_{2,2}}\\frac{\\partial O_{2,2}}{\\partial F_{2,1}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{2,2}}=\\frac{\\partial L}{\\partial O_{1,1}}\\frac{\\partial O_{1,1}}{\\partial F_{2,2}}+\\frac{\\partial L}{\\partial O_{1,2}}\\frac{\\partial O_{1,2}}{\\partial F_{2,2}}+\\frac{\\partial L}{\\partial O_{2,1}}\\frac{\\partial O_{2,1}}{\\partial F_{2,2}}+\\frac{\\partial L}{\\partial O_{2,2}}\\frac{\\partial O_{2,2}}{\\partial F_{2,2}}\n",
    "$$\n",
    "\n",
    "we know the second parts of these derivatives and can plug them in like so:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{1,1}}=\\frac{\\partial L}{\\partial O_{1,1}}\\cdot X_{1,1}+\\frac{\\partial L}{\\partial O_{1,2}}\\cdot X_{1,2}+\\frac{\\partial L}{\\partial O_{2,1}}\\cdot X_{2,1}+\\frac{\\partial L}{\\partial O_{2,2}}\\cdot X_{2,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{1,2}}=\\frac{\\partial L}{\\partial O_{1,1}}\\cdot X_{1,2}+\\frac{\\partial L}{\\partial O_{1,2}}\\cdot X_{1,3}+\\frac{\\partial L}{\\partial O_{2,1}}\\cdot X_{2,2}+\\frac{\\partial L}{\\partial O_{2,2}}\\cdot X_{2,3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{2,1}}=\\frac{\\partial L}{\\partial O_{1,1}}\\cdot X_{2,1}+\\frac{\\partial L}{\\partial O_{1,2}}\\cdot X_{2,2}+\\frac{\\partial L}{\\partial O_{2,1}}\\cdot X_{3,1}+\\frac{\\partial L}{\\partial O_{2,2}}\\cdot X_{3,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial F_{2,2}}=\\frac{\\partial L}{\\partial O_{1,1}}\\cdot X_{2,2}+\\frac{\\partial L}{\\partial O_{1,2}}\\cdot X_{2,3}+\\frac{\\partial L}{\\partial O_{2,1}}\\cdot X_{3,2}+\\frac{\\partial L}{\\partial O_{2,2}}\\cdot X_{3,3}\n",
    "$$\n",
    "\n",
    "So, we have four partial derivatives, which you can represent nicely as the result of **another convolution** between the input matrix $X$ and another filter $F'$ that consists of the partial derivatives of the loss with respect to each of the outputs \n",
    "\n",
    "$$\n",
    "...\n",
    "$$\n",
    "\n",
    "\n",
    "### Finding $\\frac{\\partial L}{\\partial X}$\n",
    "So, second, the partial derivative to backpropagate the loss with respect to the input $X$, we expand using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial O} \\cdot \\frac{\\partial O}{\\partial X}\n",
    "$$\n",
    "\n",
    "\n",
    "Again, these are all matrices - so, for every element of $X$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_i} = \\sum_{k=1}^{n} \\frac{\\partial L}{\\partial O_k} \\cdot \\frac{\\partial O_k}{\\partial X_i}\n",
    "$$\n",
    "\n",
    "From the forward equations of the convolution, we find for the first element:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial O_{1,1}}{\\partial X_{i,j}}=F_{i,j}\n",
    "$$\n",
    "\n",
    "we now get nine(!) different terms:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{1,1}} = \\frac{\\partial L}{\\partial O_{1,1}} \\cdot F_{1,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{1,2}} = \\frac{\\partial L}{\\partial O_{1,1}} \\cdot F_{1,2} + \\frac{\\partial L}{\\partial O_{1,2}} \\cdot F_{1,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{1,3}} = \\frac{\\partial L}{\\partial O_{1,2}} \\cdot F_{1,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{2,1}} = \\frac{\\partial L}{\\partial O_{1,1}} \\cdot F_{2,1} + \\frac{\\partial L}{\\partial O_{2,1}} \\cdot F_{1,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{2,2}} = \\frac{\\partial L}{\\partial O_{1,1}} \\cdot F_{2,2} + \\frac{\\partial L}{\\partial O_{1,2}} \\cdot F_{2,1} + \\frac{\\partial L}{\\partial O_{2,1}} \\cdot F_{1,2} + \\frac{\\partial L}{\\partial O_{2,2}} \\cdot F_{1,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{2,3}} = \\frac{\\partial L}{\\partial O_{1,2}} \\cdot F_{2,2} + \\frac{\\partial L}{\\partial O_{2,2}} \\cdot F_{1,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{3,1}} = \\frac{\\partial L}{\\partial O_{2,1}} \\cdot F_{2,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{3,2}} = \\frac{\\partial L}{\\partial O_{2,1}} \\cdot F_{2,2} + \\frac{\\partial L}{\\partial O_{2,2}} \\cdot F_{2,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X_{3,3}} = \\frac{\\partial L}{\\partial O_{2,2}} \\cdot F_{2,2}\n",
    "$$\n",
    "\n",
    "\n",
    "This funnily enough is also a type of convolution of the loss gradient matrix $F^{'}_{i,j} = \\frac{\\partial L}{\\partial O_{i,j}}$, but this time with a rotated version of the original filter $R$!\n",
    "\n",
    "So, our rotated filter $R$ is obtained by flipping $F$ first vertically and then horizontally to get:\n",
    "\n",
    "$$\n",
    "R_{1,1}=F_{2,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_{1,2}=F_{2,1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_{2,1}=F_{1,2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_{2,2}=F_{1,1}\n",
    "$$\n",
    "\n",
    "Now we slide this 2x2 filter across the our $F'$, which looks like this:\n",
    "\n",
    "![full convolution - copyright @pavisj](fullconv.gif)\n",
    "\n",
    "This is sometimes called a **full convolution**.\n",
    "\n",
    "### The full backpropagation\n",
    "\n",
    "So, in summary, our backpropagation needs two terms that can be obtained by convolutions. \n",
    "\n",
    "1. A simple convolution of the values coming on from high with the original input $X$. \n",
    "\n",
    "2. A full convolution of the values coming on from high with a rotated version of the original filter $F$.\n",
    "\n",
    "\n",
    "For another, very in-depth derivation of this, you can also refer to:\n",
    "\n",
    "https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deep\n",
    "\n",
    "Great, so now that we know how to derive the operation of discrete convolution filters, we can try to make a neural network with convolution layers!\n",
    "\n",
    "The issue is, however, that the filters that we used here in this layer are 3x3 pixels. How can we make sure that 3x3 pixels are actually sufficient to explain all types of important image features?\n",
    "\n",
    "We cannot make sure - in fact, if you look at birds versus airplanes, as our network here is supposed to do, then almost certainly we would assume that certain structures of these image categories are larger in extent (think about the wings of the aircraft, or the \"body\" of birds, etc).\n",
    "\n",
    "One solution to this will of course be to make **larger** filters - CIFAR is 32x32 pixels, so we could - in principle - go all the way up to 32x32 pixels for our filters. This, however, will be very inefficient, as it will be exactly the same as a fully-connected network (in fact, much \"worse\", since we have 16 of these filters).\n",
    "\n",
    "So, like we have seen at the very beginning of the notebook, people proposed to go deep, successively reducing the size of the image so that filters become \"larger\" in each layer.\n",
    "\n",
    "We can do this downsampling operation between layers in a variety of ways:\n",
    "\n",
    "1. Average the results in a small area and then downsample - this is called average pooling\n",
    "\n",
    "2. Take the maximum of the results in a small area (say 4x4 pixels) and use this as the downsample value - this is called max pooling\n",
    "\n",
    "3. Do a so-called **strided** convolution, in which the filter is not evaluated at each pixel, but \"skips\" input pixels.\n",
    "\n",
    "The most common approach for standard CNNs is to downsample using max-pooling.\n",
    "\n",
    "This is what that looks like as a layer in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ...\n",
    "output = ...\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what happens if you have two subsequent layers of convolutions with max-pooling on a 2x2 scale.\n",
    "\n",
    "![maxpool](maxpool.png)\n",
    "\n",
    "In the final layer of max-pooling, the number 21 is produced from the data coming from an original part of the image of 8x8 pixels!\n",
    "\n",
    "Note, how the max-pooling keeps the spatial information intact such that we can actually say that something cross-like seemed to be happening at the top-left of the image.\n",
    "\n",
    "\n",
    "### Derivatives of max-pooling\n",
    "\n",
    "In order to do backpropagation, we need to of course know the value of the derivative of this operation. Well, in this case, this is simple as the error is assigned to all the neuron/unit that contributed the \"max\". All other neurons/units did not do anything to the forward pass, and hence their backpropagation value will be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "So, in Pytorch, we will now be doing something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that we now need to convert the output of the final max-pooling, which are 2D-like into a 1D vector that we need to convert to probabilities somehow.\n",
    "\n",
    "Maybe like this???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here, we've added two sets of linear layers that will now take the output of the max-pooling to somehow create internal features that we can then map onto two classes (or more, of course).\n",
    "\n",
    "\n",
    "How many parameters do we have now??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awww. That's cute, no? Instead of 1.5 or 3 million parameters as before, we have just over 18,000 parameters. This of course, does not mean that our network can solve the birds versus airplane task - it's just that we have set it up like this.\n",
    "\n",
    "But there is something missing: in order to check this, let's try to apply this randomly-initialized model to an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives a nasty error. In fact, pytorch complains that the output of the max-pool and the linear layer do not seem to be compatible. \n",
    "\n",
    "There are two ways to remedy this: the first is the old-school way, in which we manually reshape the output of the max-pool layer so that it conforms to an input-ready shape for the linear layer.\n",
    "\n",
    "For the old-school version, we have to build a sub-class `nn.Module` and then put a `view` command like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = ...\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we find out how many parameters our model has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the output of the image being passed through the network (without the loss layer!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second, newer way is to simply use the Pytorch `nn.Flatten` layer that achieves the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.flatten = ...\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = ...\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using layers to do the calculations of the activation functions and the max-pooling, you can also use them as operators in the forward function, which may be more satisfactory from a coding point of view, as these \"layers\" don't actually have a lot of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fun with CNNs is to determine how to calculate the output of the stacked layers of convolutions and max-pooling operations. \n",
    "\n",
    "Here's a handy function that does this automatically for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(w, k=3, s=1, p=0, m=True):\n",
    "    \"\"\"\n",
    "    Returns the right size of the flattened tensor after\n",
    "        convolutional transformation\n",
    "    :param w: width of image\n",
    "    :param k: kernel size\n",
    "    :param s: stride\n",
    "    :param p: padding\n",
    "    :param m: max pooling (bool)\n",
    "    :return: proper shape and params: use x * x * previous_out_channels\n",
    "\n",
    "    Example:\n",
    "    r = flatten(*flatten(*flatten(w=100, k=3, s=1, p=0, m=True)))[0]\n",
    "    self.fc1 = nn.Linear(r*r*128, 1024)\n",
    "    \"\"\"\n",
    "    return int((np.floor((w - k + 2 * p) / s) + 1) / 2 if m else 1), k, s, p, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-class problem with convolutions\n",
    "\n",
    "Let's finally go to an implementation using our newly-minted CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            # put a batch through the model\n",
    "            outputs = model(imgs)\n",
    "            # determine the loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # zero the gradients and determine backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # do one step of optimization\n",
    "            optimizer.step()\n",
    "            # keep track of the loss\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train this (small) convolutional neural network with this training_loop class for 100 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# get the model\n",
    "model = Net()\n",
    "# standard optimizer \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
    "# classification loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# go for 100 epochs\n",
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That takes a little time, but we can see that the error is going down. Loss is, however, not very intuitive to parse for humans, so let's convert to accuracies on training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, our tiny model with only 18,000 parameters is actually doing quite well. The power of the filters!!\n",
    "\n",
    "Let's save this model using Pytorch like so (this will require the full model class for instantiation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..., data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Net()\n",
    "...(data_path\n",
    "                                        + 'birds_vs_airplanes.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the filters\n",
    "\n",
    "We have now trained a bunch of filters. What do they look like?\n",
    "\n",
    "First, a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "import torchvision.transforms.functional as F1\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F1.to_pil_image(img[0,:,:])\n",
    "        axs[0, i].imshow(np.asarray(img),cmap='gray')\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the filters from a first convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the second layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to come up with a \"better\" intuition, let's turn to a bigger training exercise and take a look at a breakout link: \n",
    "\n",
    "### Breakout\n",
    "\n",
    "https://cs.stanford.edu/people/karpathy/convnetjs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
